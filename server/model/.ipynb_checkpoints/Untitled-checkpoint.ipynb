{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3db4ffff-475c-488f-88d5-d207de9e6e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            age     sex        bmi smoker     region  children       charges\n",
      "0     21.000000    male  25.745000     no  northeast         2   3279.868550\n",
      "1     36.976978  female  25.744165    yes  southeast         3  21454.494239\n",
      "2     18.000000    male  30.030000     no  southeast         1   1720.353700\n",
      "3     37.000000    male  30.676891     no  northeast         3   6801.437542\n",
      "4     58.000000    male  32.010000     no  southeast         1  11946.625900\n",
      "...         ...     ...        ...    ...        ...       ...           ...\n",
      "3625  48.820767  female  41.426984     no  northwest         4  10987.324964\n",
      "3626  38.661977  female  26.202557     no  southeast         2  11735.844352\n",
      "3627  56.000000    male  40.300000     no  southwest         0  10602.385000\n",
      "3628  48.061207  female  34.930624     no  southeast         1   8976.140452\n",
      "3629  37.598865  female  25.219233     no  northeast         3   7027.698968\n",
      "\n",
      "[3630 rows x 7 columns]            age     sex        bmi smoker     region  children\n",
      "0    40.000000    male  29.900000     no  southwest         2\n",
      "1    47.000000    male  32.300000     no  southwest         1\n",
      "2    54.000000  female  28.880000     no  northeast         2\n",
      "3    37.000000    male  30.568094     no  northeast         3\n",
      "4    59.130049    male  33.132854    yes  northeast         4\n",
      "..         ...     ...        ...    ...        ...       ...\n",
      "487  51.000000    male  27.740000     no  northeast         1\n",
      "488  33.000000    male  42.400000     no  southwest         5\n",
      "489  47.769999    male  29.064615     no  northeast         4\n",
      "490  41.530738  female  24.260852     no  southeast         5\n",
      "491  36.000000    male  33.400000    yes  southwest         2\n",
      "\n",
      "[492 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"./Train_Data.csv\")\n",
    "test_data = pd.read_csv(\"./Test_Data.csv\")\n",
    "\n",
    "print(train_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e03485bc-d314-41ff-9ad3-c115f568b10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoker</th>\n",
       "      <th>children</th>\n",
       "      <th>charges</th>\n",
       "      <th>is_male</th>\n",
       "      <th>is_female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>25.745000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3279.868550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.976978</td>\n",
       "      <td>25.744165</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21454.494239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>30.030000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1720.353700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>30.676891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6801.437542</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>32.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11946.625900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>48.820767</td>\n",
       "      <td>41.426984</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10987.324964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>38.661977</td>\n",
       "      <td>26.202557</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11735.844352</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3627</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>40.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10602.385000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>48.061207</td>\n",
       "      <td>34.930624</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8976.140452</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>37.598865</td>\n",
       "      <td>25.219233</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7027.698968</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3630 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age        bmi  smoker  children       charges  is_male  is_female\n",
       "0     21.000000  25.745000       0         2   3279.868550        1          0\n",
       "1     36.976978  25.744165       1         3  21454.494239        0          1\n",
       "2     18.000000  30.030000       0         1   1720.353700        1          0\n",
       "3     37.000000  30.676891       0         3   6801.437542        1          0\n",
       "4     58.000000  32.010000       0         1  11946.625900        1          0\n",
       "...         ...        ...     ...       ...           ...      ...        ...\n",
       "3625  48.820767  41.426984       0         4  10987.324964        0          1\n",
       "3626  38.661977  26.202557       0         2  11735.844352        0          1\n",
       "3627  56.000000  40.300000       0         0  10602.385000        1          0\n",
       "3628  48.061207  34.930624       0         1   8976.140452        0          1\n",
       "3629  37.598865  25.219233       0         3   7027.698968        0          1\n",
       "\n",
       "[3630 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dataset in [train_data, test_data]:\n",
    "    dataset[\"is_male\"] = [1 if i == \"male\" else 0 for i in dataset[\"sex\"]]\n",
    "    dataset[\"is_female\"] = [1 if i == \"female\" else 0 for i in dataset[\"sex\"]]\n",
    "    dataset[\"smoker\"] = [1 if i == \"yes\" else 0 for i in dataset[\"smoker\"]]\n",
    "\n",
    "    del dataset[\"region\"]\n",
    "    del dataset[\"sex\"]\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1fd2c9e-e13e-4323-b55a-7ec693c81105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InsuranceDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): Prepared dataframe.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Select features (X) and target (y)\n",
    "        X = self.data.loc[idx, ['age', 'bmi', 'smoker', 'children', 'is_male', 'is_female']].values.astype(float)\n",
    "        y = self.data.loc[idx, 'charges'].astype(float)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        sample = {'X': X, 'y': y}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d83a2ace-051e-4c4a-8351-40f87d46ef1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([21.0000, 25.7450,  0.0000,  2.0000,  1.0000,  0.0000]),\n",
       " 'y': tensor(3279.8687)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = InsuranceDataset(train_data)\n",
    "test_dataset = InsuranceDataset(test_data)\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ea7df05-a937-4cb4-b650-2fd22f33c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 6])\n",
      "Shape of y: torch.Size([64]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for i in train_dataloader:\n",
    "    X, y = i.values()\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "542fdd4c-0275-40fb-851f-de2dcf267510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=36, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=36, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(6, 36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42968702-a850-4a25-a87d-d05d1cd5afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "tensor([ 3279.8687, 21454.4941,  1720.3538,  6801.4375, 11946.6260,  7742.1099,\n",
      "        21736.3281,  4916.9531,  5515.8096, 17009.3359, 38433.5234,  8549.1367,\n",
      "        16099.3672,  7492.9854, 18091.3730, 41501.6562,  2661.1912, 42211.1367,\n",
      "        16455.7070,  4433.9160,  6571.5439, 26236.5801,  6318.7979,  5795.9058,\n",
      "        47305.3047, 11488.3174, 29101.7520, 19305.1934, 12479.5273,  7730.7632,\n",
      "        10118.0635,  9579.1553, 38126.2461,  7168.9038,  5482.2915, 47462.8945,\n",
      "         9051.9004,  5383.5361, 17352.4258, 10480.8955, 11945.1328,  8083.1782,\n",
      "         8347.1641, 40018.2305,  6686.4312,  2219.4451,  6662.1387,  7076.8926,\n",
      "         4790.3115,  9434.7314,  7102.4077, 10043.2490, 27533.9121,  5240.7651,\n",
      "         3533.2556,  4673.3921, 48173.3594,  4179.4453,  6133.8828,  9716.7441,\n",
      "         4461.8037,  5148.5527, 13974.4551,  7050.0215])\n",
      "loss: 9133714571264.000000  [   64/ 3630]\n",
      "tensor([18061.0137,  4877.9810,  7726.8540, 19121.4629,  8519.2305,  1769.5316,\n",
      "        18629.5156, 36124.5742, 10601.4121,  5824.7817,  9094.7920, 11743.2988,\n",
      "        25737.9805,  5014.1821,  4914.3413,  3777.2117, 15526.5752,  5241.3555,\n",
      "         4833.8750, 33131.8438,  6797.2192, 18157.8770,  6025.5762,  1121.8739,\n",
      "        10658.5049, 21982.4746, 12485.3594,  6659.9111,  5263.4888, 11611.7080,\n",
      "         7079.3530, 12265.9785,  7147.4727,  8415.5195,  8877.5127,  8303.0703,\n",
      "         2257.4753, 11741.7256,  5093.7124,  7978.5215, 10143.2617,  7672.9897,\n",
      "         5523.9819,  8594.0195,  9254.6621, 12818.6055,  9378.4590, 13099.8857,\n",
      "        19120.2246,  2130.6758, 10019.4902,  4266.1660,  7168.0327,  3500.6123,\n",
      "         2136.8823, 14319.0312,  4921.8262,  6123.5688,  8649.2002, 11107.3457,\n",
      "        14829.3516, 11273.8643, 15413.5967,  7144.8628])\n",
      "tensor([ 6505.3262, 19539.2422,  7325.0483,  7625.1558,  7257.4282, 12154.0332,\n",
      "         9654.1816,  4721.4297, 24890.1055,  1242.2600,  6425.8906,  6837.3687,\n",
      "         3981.9768,  3261.9417,  3471.4097, 19043.1621,  1646.4297, 17693.6172,\n",
      "         1621.3402, 10607.0059, 38792.6875,  9440.7178,  9387.7686, 39783.3828,\n",
      "        23503.1953, 13352.0996,  6309.6631, 46599.1094,  2710.7117, 21190.7207,\n",
      "        22028.7109, 13744.0557,  9360.9238,  4189.1133,  4795.6567,  8534.6719,\n",
      "        45826.9258, 22395.7441,  7027.6987,  2416.9551, 10799.3418, 14984.9824,\n",
      "        10776.0654, 28205.9336, 10226.2842,  4894.7534, 11253.4209, 10422.9170,\n",
      "         8116.6802, 15170.0693,  9632.6973,  4818.7358,  7263.6362,  9890.5205,\n",
      "        12254.8135, 12927.1318, 11370.8428, 13635.6377, 11307.3711,  1737.3760,\n",
      "        13125.3525, 35064.6719,  9091.8574,  3591.4800])\n",
      "tensor([13143.3428,  6009.9824,  3293.2917, 21348.7051, 44213.4219, 16150.7910,\n",
      "         8199.8369, 11881.3584, 26506.6816,  4022.5654, 21223.6758,  5621.9448,\n",
      "         6500.2358,  4977.2495,  5334.7085,  7209.4917, 10107.2207,  5560.0654,\n",
      "         9962.6123,  8280.6230,  9137.7803,  9861.0254,  7371.7720,  4912.5459,\n",
      "         4431.6924,  5699.8374, 10104.5605, 11754.4150, 17448.0000,  9046.0400,\n",
      "        23445.8926,  2867.1196, 16884.9238,  5427.3716, 36808.3750,  3070.8086,\n",
      "         4737.9204,  4889.0366, 13596.5107,  2727.3950, 11439.6240, 13915.5215,\n",
      "         8059.4062,  8835.2646,  9235.0645,  9462.1533, 11335.2959,  3410.3240,\n",
      "        27322.7344,  7987.8325,  7537.1641, 10278.1416,  3866.8552, 38321.4492,\n",
      "        15973.6201,  9878.7461,  8068.1851, 10741.0586,  5080.0962,  4350.5142,\n",
      "         8585.8467, 39774.2773, 12026.0957,  8794.9893])\n",
      "tensor([ 4533.9902,  4835.7505, 12877.6006, 28403.3711,  2473.3340, 17178.6816,\n",
      "         7765.3745,  7631.4805,  5851.0708,  3176.2876, 14478.3301, 16114.3955,\n",
      "        13844.7969, 14002.8252,  6750.4321,  4670.9858, 24393.6230, 13145.5684,\n",
      "         6412.4795,  5318.4067,  5466.6616, 14374.9912, 46200.9844,  9432.9258,\n",
      "         7583.2061,  5033.0952,  4559.5342,  5138.2568,  1627.2825,  6361.5400,\n",
      "         2974.1260,  5484.5405,  1967.0227,  9670.2822, 11674.1299,  4337.7354,\n",
      "        24321.3496, 11015.1846, 20462.9980,  6747.9126, 14007.2217,  8931.1758,\n",
      "        11100.1875, 37079.3711, 11018.9854,  2196.4731,  6579.8521, 30166.6191,\n",
      "        11048.9639,  6660.8281,  5727.1279,  6551.7500,  6414.1782, 16176.0850,\n",
      "        48549.1797,  4685.1455, 44627.9375, 30364.5371,  6143.1284,  4515.1943,\n",
      "        44501.3984,  4265.1128, 17879.6328, 10711.7891])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11842.4424, 28101.3340,  7148.2329,  9520.0020, 10801.6104,  9272.5332,\n",
      "        11906.1250, 38709.1758,  4040.5583,  7624.6299, 11291.3857, 12244.5312,\n",
      "         6496.8862, 10977.2061, 34617.8398, 13019.5244, 10461.9795, 14436.8926,\n",
      "        17765.9785, 24915.2207, 18638.6133, 12846.3525, 18765.8750,  4928.3643,\n",
      "         7160.3081,  7060.2222, 21002.7578,  6358.1851,  4990.2998, 16420.4941,\n",
      "        19120.6562, 10942.1318,  2720.7209,  1515.3448,  9634.1523,  6923.6221,\n",
      "         5632.9487, 17108.6309, 17315.6914,  6951.1992,  6941.6724, 40419.0195,\n",
      "         8233.0977, 33732.6875, 12002.3662, 13937.6660,  4852.9478, 11261.8174,\n",
      "        10577.0869,  3756.6216, 38822.0898, 27082.6641,  8252.2842,  6020.6094,\n",
      "        13616.3584, 12269.6885,  5605.0166,  8342.9092, 10920.3223, 11497.8213,\n",
      "        20641.5938,  9198.3574, 36632.5430,  4561.1885])\n",
      "tensor([ 7789.6348, 19435.9277,  4501.2344,  5452.0752, 11085.5869,  9239.9736,\n",
      "         7152.6714, 18317.6250,  5354.0747, 46220.9961, 25421.9336,  9916.5957,\n",
      "        22606.0645, 11931.1250, 11280.1006,  6415.2651,  6680.3022,  6061.5435,\n",
      "         3925.7583,  5257.5078, 14113.4727, 11044.6406, 20411.7734,  9553.4902,\n",
      "         2155.6814, 14119.6201,  2154.3611, 12495.2910, 11643.1836, 42303.6914,\n",
      "        15412.6855,  4433.2295, 14426.0742,  6358.7764, 21880.8203, 18099.5254,\n",
      "        10967.5176,  3385.3992,  6437.9717,  4909.7246, 28923.1367,  4719.7363,\n",
      "         9880.0684,  5375.0381,  7371.1211,  6325.6953,  9875.6807, 19798.0547,\n",
      "         1986.9333,  5940.5850, 11365.9521,  5186.6426,  1639.5631, 24948.2012,\n",
      "         2719.2798,  8728.9082,  7552.9785, 15456.2949,  7442.3052,  3597.5959,\n",
      "         4326.2134, 13143.3369,  9143.6279, 17270.3926])\n",
      "tensor([ 8569.8613, 14003.7881,  6353.1885,  9285.9922,  3500.7427, 12533.9229,\n",
      "         9304.7021, 11318.2275,  9301.8936,  8516.8291,  9290.3174,  2102.2646,\n",
      "         6059.1729,  3591.4707,  8920.6895,  4527.1831,  4783.2427,  5267.8184,\n",
      "         8704.1816,  5572.8662, 13430.2646,  3292.5298, 17716.7422,  8603.8232,\n",
      "         6743.1680, 35956.5898, 11316.2148, 47055.5312, 35069.3750, 10550.3281,\n",
      "         4827.9048, 29010.3184, 12347.1719,  1815.8759, 11881.6777,  4053.6936,\n",
      "         3832.0100,  8520.2656,  6393.6035, 26018.9512, 17929.3027,  9138.3857,\n",
      "         6970.0791, 10769.5791,  4740.7705,  9964.0566, 41676.0820,  6330.5752,\n",
      "        11300.3760,  7740.3369,  5631.7529,  4463.2051,  6764.7871,  4200.9316,\n",
      "         9172.7686, 36085.2188, 44445.3477,  6377.8208,  9456.3994, 17352.6797,\n",
      "        11010.6445, 10289.4707, 46182.8320, 16692.1406])\n",
      "tensor([ 4832.6074,  7650.7739, 46151.1250,  6673.0005, 10791.9600,  4134.0825,\n",
      "         5540.3325, 14539.2803, 38800.3867, 10086.8330, 43813.8672,  7331.8779,\n",
      "         8623.3125, 11371.9863,  8930.9346, 10959.3301, 12094.7510, 11455.2803,\n",
      "        48824.4492, 14235.0723, 18861.9668,  1136.3994,  4523.3594,  6088.1855,\n",
      "         1704.7002, 11881.9697, 18903.4922, 42112.2344, 11454.0215,  6891.6792,\n",
      "        30438.9199,  2913.5691,  6164.6860,  5976.8311,  4891.4346, 10652.5762,\n",
      "         4910.7397,  7243.8135,  2566.4707, 19853.7910,  4873.8315,  6182.4146,\n",
      "         3878.7839,  7151.0918,  2801.9724,  9283.5615,  5510.3447,  7378.3042,\n",
      "        18451.5840,  6605.6602, 34672.1484,  5319.6094, 11187.6562, 11196.8926,\n",
      "        24667.4199,  4559.3188,  6586.8999,  1917.3184, 11179.6074,  4372.8345,\n",
      "         3544.2539, 14142.7627, 10381.4785, 20664.3613])\n",
      "tensor([ 6347.0557,  4837.5825, 20099.7363, 19663.3809,  4765.3315,  3352.8745,\n",
      "        11987.7871, 17069.8730,  6289.7549,  4296.2710,  4388.5278, 15022.6826,\n",
      "        36308.0508, 15820.6992, 11218.9795,  8752.8184,  5379.5894,  7640.3091,\n",
      "         3481.8679,  9018.6416, 11268.0127, 47928.0312,  6616.0239, 15408.4268,\n",
      "         3940.8850, 21082.1602,  8596.8281, 10508.9199,  5227.9888, 36197.6992,\n",
      "        14711.7344, 17742.1055,  3531.7224,  6406.4106, 14410.9316, 43871.3320,\n",
      "         5209.5786,  6948.7007, 21472.4785, 34779.6133, 21797.0000,  4719.5239,\n",
      "        10744.9287, 11270.3340, 23781.3574,  5245.2271,  4992.3765,  6572.0107,\n",
      "         6738.9458,  4119.3862,  5581.6836,  7441.5010, 12163.5576,  1635.7336,\n",
      "        15828.8213,  9446.0977, 25729.1855,  7235.6587, 34166.2734,  7890.6313,\n",
      "         8568.6943, 11013.7119,  4076.4971,  3895.4170])\n",
      "tensor([ 7448.4038, 16495.7285,  7049.5171,  1711.0269,  5347.5425, 14254.6084,\n",
      "        48381.3633,  3577.9990,  8563.3232,  3611.9717,  4702.1309, 15010.1758,\n",
      "        11285.7783,  5120.5830,  8815.0703, 13112.6045, 18306.3945, 13952.1602,\n",
      "         6705.0220, 10237.2578, 19493.5508, 21774.3223, 19811.0117, 16173.2686,\n",
      "        39611.7578,  7985.6104,  9409.5928, 11082.5771, 10923.9336,  4960.0352,\n",
      "        41332.5312, 28950.4688, 22462.0430,  7639.4175,  4915.0601, 10602.7920,\n",
      "         4731.0303, 37742.5742, 24180.9336,  1664.9996,  8605.3613, 21978.6777,\n",
      "         9500.5732, 41180.8711, 13717.1807,  7201.7007, 12491.4844, 14239.9189,\n",
      "        25656.5762,  6035.6963, 12574.0488,  7518.0254, 14474.6748, 11023.4434,\n",
      "         1241.5649, 51194.5586,  5790.1455,  5267.5312,  7730.8525,  5551.1509,\n",
      "         9863.4717, 22192.4375, 46130.5273,  2690.1138])\n",
      "tensor([11397.8027, 39725.5195, 23326.9160,  6402.2915,  9390.1387,  5762.3604,\n",
      "        10214.6357,  2534.3938,  5167.4790,  7046.7222, 36021.0117, 11350.5879,\n",
      "         2220.0222,  6697.6084,  6766.8198, 32787.4570,  7730.8125,  1534.3044,\n",
      "        10544.1963, 29964.4688, 12680.1914,  6713.5645, 27043.2266, 36133.6328,\n",
      "         8283.6807,  8595.7656, 47496.4961,  4508.3540, 19252.2188, 15183.2344,\n",
      "         6079.6714,  1704.5681, 24901.6484,  2850.6838, 18648.4219, 19777.6309,\n",
      "        39793.6562,  4544.2349,  6657.9819,  4462.7217, 10823.6777, 18137.9727,\n",
      "        12198.5254, 11986.8477, 34241.5391,  5426.0776,  6196.4482,  6765.6182,\n",
      "        10631.4541,  7475.3062,  9182.0586,  9549.5654, 18218.1621,  6673.6426,\n",
      "        12839.8486, 12479.7090, 24106.9121,  4317.3154, 11856.4111, 11032.7969,\n",
      "        10713.6436, 30942.1914,  5247.9961, 12913.9922])\n",
      "tensor([ 4790.2144,  2782.4663,  4386.8716, 17740.2930, 20773.6270, 13470.8604,\n",
      "        21344.8477, 11212.6543, 14383.3535, 11305.9346, 10826.0000, 22153.5918,\n",
      "        40337.0664, 20009.6328, 12271.1777,  5466.0791, 12079.0439, 47291.0547,\n",
      "        23288.9277,  9298.7139,  7612.2227,  8798.5928, 20633.8691,  8396.6006,\n",
      "        11214.5322,  7210.7290, 17716.6367, 40920.5703, 13289.4209, 32248.7012,\n",
      "         4846.9199,  8027.9678,  3861.2097, 17758.2520, 20088.1133,  9361.3271,\n",
      "        43578.9375, 11072.3447,  6784.1021,  6250.4351, 10680.0986, 15359.1045,\n",
      "        14210.5361,  6849.0259,  4345.8423, 16790.6562, 19144.5762, 11058.1572,\n",
      "         4532.1323, 38168.0039,  4987.0674,  6877.9800,  6660.0488,  6758.4141,\n",
      "        25311.5215, 20177.6719, 11299.3428,  4151.0288, 25332.6934,  5884.6030,\n",
      "         9898.4219,  3882.6570,  3392.9768,  4364.6133])\n",
      "tensor([ 3268.8467, 24881.2305,  9884.6934, 11218.5703,  6785.8169,  5012.6846,\n",
      "         8891.1396,  8978.1855,  5891.5181,  5225.6348,  7729.6455, 12755.5361,\n",
      "         8824.9102,  8152.0234, 10129.2852, 11286.5391,  8718.8525, 44585.4570,\n",
      "         7517.5283, 16570.5996,  1163.4626,  8556.9072,  2480.9790, 18971.7168,\n",
      "         2775.1921, 21318.5898,  5472.4492, 11276.6777, 62592.8750, 11197.0967,\n",
      "        17012.1875,  5209.1787, 11280.4385,  3659.3459,  3227.1211,  8609.9229,\n",
      "         8699.4893, 16297.8457, 10579.7109, 11244.3770,  7193.5654,  9874.5771,\n",
      "         2241.7705, 48673.5586,  8527.5322,  8941.0898, 40273.6445,  5743.7627,\n",
      "         4433.5488,  7345.0840, 11500.3193, 12425.2158,  8743.7490,  8424.0781,\n",
      "         4529.8950,  4676.9346,  5128.0859,  1137.4697,  4508.5103,  4537.2729,\n",
      "        39556.4961, 12346.6543, 24873.3848,  5514.5098])\n",
      "tensor([41003.0234, 30322.2207,  6426.7280,  3906.1270,  9914.9834, 10450.5518,\n",
      "         8765.2490,  8807.7295,  8739.6240,  2104.1133,  1632.0363, 17468.9844,\n",
      "        11460.4990,  4646.7588,  4402.2329, 32351.1719, 21760.0762, 47400.9414,\n",
      "         4753.6367, 18264.6367,  1705.6245,  7316.6323, 10206.4766, 18524.0332,\n",
      "        12235.8389, 33307.5508,  1731.6770, 35585.5742, 12219.4395,  9034.2324,\n",
      "         2643.2686, 37747.6836, 19023.2598,  4900.3213,  7731.4272,  2264.7219,\n",
      "         7935.2910,  1146.7966,  8825.0859, 11657.7188,  9541.6953, 18033.9688,\n",
      "         3350.4404, 36910.6094,  5584.3057, 11680.1318,  4762.3291,  3005.1008,\n",
      "        23746.4551,  1261.8590,  5594.8457,  8442.6670,  4911.9731, 26412.2129,\n",
      "        14481.9785,  2138.0708, 39586.8633,  2457.2112,  9985.4580,  4346.6533,\n",
      "         9788.8662,  9914.8770,  4784.8198,  6875.9609])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9595.4395,  4832.7827, 36869.9336,  1824.2854,  7256.7231,  9620.3311,\n",
      "         8713.6016, 24869.8359, 20709.0195, 10338.9316,  4618.5430, 16977.8887,\n",
      "         5969.7231,  6069.8608, 39508.8711,  6986.6968, 36103.1719,  2203.7358,\n",
      "        11015.1748,  1137.0110,  8419.5869,  8964.0605, 10928.8486, 27302.6289,\n",
      "         5514.9214, 36898.7344,  3201.2451, 21677.2832,  4976.8608, 10792.8076,\n",
      "        39871.7031,  1135.9407,  7160.3301, 13489.0400,  5511.5137, 11326.7148,\n",
      "         9490.4531,  9896.7666,  9563.0293,  8844.6348, 11358.5850, 13415.0381,\n",
      "         8211.1006,  4952.9585,  6182.0483, 27724.2891,  4347.0234,  5729.0054,\n",
      "         4949.7588, 18767.7383,  8825.4463, 10698.1123, 47069.8359, 28468.9199,\n",
      "         6268.3389, 18804.7520,  9335.0225,  5518.2974,  5469.8594,  8334.5898,\n",
      "         8938.4619, 18618.7344,  6527.7969,  6338.7256])\n",
      "tensor([40824.5898,  4375.7065,  7629.2109, 16500.1699, 44145.9219, 19124.3691,\n",
      "         4746.3442,  1615.7667,  2927.0647, 18596.5156, 12494.1885, 11436.7383,\n",
      "        11801.3779, 17904.5273, 20781.4883,  2902.9065,  4267.7329, 39000.4609,\n",
      "         8052.0400,  9877.6074, 60021.3984,  9564.2031,  9785.7725, 28954.2754,\n",
      "        10790.3672,  8976.1406,  2585.2690,  9763.2705,  4958.0732, 13123.6904,\n",
      "        12333.8281, 18838.7031,  4008.6965,  4341.8853,  8932.0840, 18435.6348,\n",
      "         8151.5220, 43097.4141,  9841.7959,  6686.3301, 42760.5039,  4466.6216,\n",
      "         4482.7412,  8428.0693,  6878.5137,  9421.6201,  4983.4512, 35391.7617,\n",
      "        15011.1729, 27346.0430,  8975.1846, 10982.5010, 11065.9971,  4765.5669,\n",
      "        12815.4453,  8762.7617, 12148.6709, 12636.3799,  5348.0586,  6658.6318,\n",
      "         1631.6683,  5840.4814,  4816.4673, 17731.7051])\n",
      "tensor([ 4977.9277, 10079.9287,  2217.6013,  3375.3062, 39109.7539,  6239.0723,\n",
      "        15322.8506, 11538.4209, 11848.1406, 36325.0352, 21259.3789,  7630.3252,\n",
      "         7153.4839,  9385.6670,  5972.3779, 25517.1133, 11658.1152, 25354.4531,\n",
      "        24508.0273,  6344.1738, 14692.6689, 44423.8047, 14224.7549,  5093.8828,\n",
      "        14200.8975, 11048.3555,  4234.9268,  6985.5068,  4705.4697,  8269.0439,\n",
      "         6117.4946,  5345.6807,  5438.7490,  7887.6675, 21103.6543, 18473.0508,\n",
      "        19071.2871, 12830.8213,  2801.2588, 16886.6387, 18972.4941,  9694.4453,\n",
      "        46412.5352,  9566.9912,  5708.8672,  9282.6914, 13483.6182,  4930.2988,\n",
      "         3533.4658, 15403.1816,  6341.8433,  4766.0220, 17179.5215,  4487.3071,\n",
      "        28724.9434, 17515.5254, 26109.3281,  5669.1655,  3693.4280, 24707.1543,\n",
      "         5123.0977, 17129.8613, 11987.1680,  6073.2026])\n",
      "tensor([ 9869.8105,  7968.3511,  8410.0469,  5857.2939, 12655.3779,  5662.2251,\n",
      "        11578.8184,  7576.7598, 18259.2168, 40242.7969,  8797.1270, 27963.1992,\n",
      "         6389.9458, 23757.2539, 42656.6914, 19533.8320,  9602.3350,  5958.8755,\n",
      "        12493.1104, 15554.3066, 13217.0947, 35595.5898, 11265.8232, 13224.0566,\n",
      "         2494.0220,  7165.7222, 28624.6621,  4434.9209, 11200.5312,  4746.4692,\n",
      "         8767.5732,  5395.9326,  7080.1421,  3056.3882,  6375.3955, 14349.8906,\n",
      "         4988.3501,  2498.4143,  4435.0942,  9676.6895,  4456.7559, 25333.3320,\n",
      "         3857.7593, 13019.1611, 29141.3594, 10818.9297, 10336.7979, 13981.8506,\n",
      "        11258.9199, 20745.9883,  2731.8477, 21984.4707, 13126.6777,  7499.3477,\n",
      "        11004.0498, 11481.3750,  4747.0527, 63770.4297,  6360.9937,  8944.1152,\n",
      "        10118.4238, 16577.7793, 12105.3203, 23162.3965])\n",
      "tensor([ 2716.0391, 10766.3447, 17914.1328,  5458.0464,  9473.7930,  7786.8691,\n",
      "         4239.8926, 16420.6777,  3985.0098,  8956.8672, 14006.8447, 30203.5801,\n",
      "         6128.7974, 29158.3398, 18246.4961,  5927.6006,  5320.1294, 10869.6318,\n",
      "        16862.8984, 12020.1104, 48885.1367,  9086.0244, 11289.1094, 27808.7246,\n",
      "        10431.9648,  2967.0117, 38117.9766, 39597.4062, 20630.7500, 15230.3584,\n",
      "        29330.9824,  2689.4954, 11184.4941,  8520.0264,  2362.2290,  4438.2632,\n",
      "         6821.3232, 40103.8906,  9630.5283,  8174.4531,  9101.7979,  2709.1118,\n",
      "        11390.6504, 11723.9111,  6311.9712,  9783.7812, 25309.4883,  4734.0195,\n",
      "         4751.0698,  4536.2588, 11509.6084, 16739.4160,  6662.1602, 13822.8027,\n",
      "         9262.5146,  4853.6143, 41949.2422, 13047.3320, 25891.7871, 14003.8545,\n",
      "        10906.6855,  6704.9863,  5615.3691, 10348.4600])\n",
      "tensor([11576.1299,  5347.0938,  4012.1570,  4832.9170, 13129.6035,  3493.3052,\n",
      "        10987.7705, 10269.4600, 16173.1904, 12153.3301, 10755.5332,  8816.9209,\n",
      "        12478.4727, 12638.1953,  7954.5171, 10838.7627,  8026.6665, 12475.3516,\n",
      "         5483.0806,  4784.0469, 34428.3711,  4709.6787, 14193.6133,  4809.7949,\n",
      "         2709.2439,  8865.4697,  5856.4531, 18820.2930, 11247.5459,  9058.7305,\n",
      "        11408.6787,  4828.5142,  6805.5850, 13887.2041,  8596.4873, 10649.4697,\n",
      "         8615.2998,  1263.2490, 20309.9355,  8968.3301,  1837.2371, 24227.3379,\n",
      "        13473.8389,  6475.8110, 13712.7646,  8598.2676,  6748.5913, 21595.3828,\n",
      "         5261.7490,  1622.1885,  6650.1948,  6453.5278,  8796.3018, 10012.6191,\n",
      "         7533.3374,  7392.6172, 12251.2207, 32548.3398,  5649.7148, 38499.0742,\n",
      "         5654.8184, 11566.3008,  4805.2856, 18237.9766])\n",
      "tensor([20172.1641, 25111.4336,  2400.4021,  8588.3887,  2020.1770, 10702.6426,\n",
      "        24971.7656, 11003.6924, 38239.4102,  3935.1799,  4927.7622,  6572.6685,\n",
      "         6571.0244,  5857.0200,  6733.6309, 16851.3691,  5325.6509, 21098.5547,\n",
      "         5410.2686, 12268.6318, 10560.4922, 10576.4160,  1391.5287,  6603.6201,\n",
      "        10992.1768,  6665.0405,  6123.5933, 16232.8467,  6108.3462,  2585.8506,\n",
      "         9097.2939,  5385.3379,  4058.7124,  9182.1699,  3161.4541, 13429.0352,\n",
      "        16118.8936,  9270.5479, 12646.2070, 11232.4961, 38514.6719,  3206.4915,\n",
      "        14455.6445,  4281.1631,  4645.6904, 43896.3750, 12142.5781, 14988.4316,\n",
      "        11155.0889, 12360.8281,  9239.3896, 10138.4346,  6449.4521, 43254.4180,\n",
      "        28152.8223,  6284.9707, 14004.3096,  8823.2793,  4953.1499,  4714.8999,\n",
      "        40160.7891,  5017.6616,  4340.0557, 27292.3242])\n",
      "tensor([20813.8711,  5368.5508,  6457.8433, 44069.0625, 34203.3672, 58571.0742,\n",
      "        13275.3184,  4472.5229, 47273.1094,  1969.6140,  4670.6118,  5377.8545,\n",
      "         3062.5083,  3904.8750,  5472.7295,  4689.0356, 44790.4922, 11938.2559,\n",
      "        36219.4062, 11115.5176, 10250.4209,  5811.5186,  4767.5073, 14358.3643,\n",
      "         7738.2622, 11346.8027,  4349.4619, 13343.0127, 12916.5557, 41034.2227,\n",
      "         6378.0747,  4957.9004, 11388.3428,  9581.1436, 11049.1953,  2250.8352,\n",
      "         9910.3594,  4707.6753,  4391.6519,  8548.1328,  6414.2407,  5288.9102,\n",
      "         9978.4072, 18836.8555, 31620.0020,  7098.9106, 19530.0527, 47974.2344,\n",
      "         1977.8149,  7494.2979, 11443.9365, 10882.8584, 22494.5723, 11313.6768,\n",
      "         3392.3652,  1131.5066,  9290.1396,  7117.9800,  1628.4709,  6503.9780,\n",
      "        12231.6133,  5000.0771,  8731.4805,  9491.4062])\n",
      "tensor([14064.4277, 11196.6904,  5507.6475, 13390.5586, 11129.3320, 14418.2803,\n",
      "         6999.2749,  6335.7681, 15169.1836, 30184.9375, 18150.9121, 46255.1133,\n",
      "        44641.1992, 14310.4971,  7237.4185,  1909.5275,  9991.0381, 15161.5342,\n",
      "        10370.9121, 10824.4844,  7112.5806,  1725.5869,  8886.0176,  3532.9143,\n",
      "        11512.4053, 23258.9980,  3994.1777, 37662.7617, 10435.0654,  4948.6729,\n",
      "        18707.2246,  6009.3750,  6265.1655,  5930.1489,  6227.6714, 26467.0977,\n",
      "         6728.2705,  8547.6914, 14001.2871, 10879.1729,  4661.2861,  1634.5734,\n",
      "        11013.7930,  4686.3887,  5124.1885, 10812.6650,  3394.4917, 34838.8711,\n",
      "         6013.1274, 32734.1855,  6770.1924,  4527.1177,  5855.9023, 38054.4922,\n",
      "         4441.2134, 13143.8652, 16378.9141,  9590.4580,  2459.7202, 18191.7344,\n",
      "        11520.0996,  8583.2227, 11262.1416, 10436.0957])\n",
      "tensor([ 3943.5955, 28105.2344, 10805.9551, 15648.6582, 11482.6348,  5127.6079,\n",
      "         6648.9106,  9012.5898, 40232.9336, 24904.3164,  4772.9556, 10056.7275,\n",
      "         6238.2979, 12645.1660,  5709.1646,  4158.2539, 11120.1904, 10676.0977,\n",
      "        23300.7910,  5246.0469,  5910.9438,  1826.8430, 10373.2334,  1252.4070,\n",
      "         3353.2839, 42969.8516, 20878.7852,  4822.7959, 38746.3555, 12387.7539,\n",
      "         9441.5166, 14925.9062, 41999.5195, 27107.3418, 24685.0215, 13673.9072,\n",
      "        19453.6855, 23065.4199, 12643.3779,  6032.5845, 10611.9785, 13462.5195,\n",
      "         2156.7517, 37464.0859,  8671.1914, 38511.6289,  5364.4507, 11259.7061,\n",
      "         9644.2529,  4888.4927,  9669.7842,  7960.8682,  5086.6528, 33475.8164,\n",
      "         4674.7974,  6312.1934,  3594.1709,  1625.4337, 11166.6670,  8709.8965,\n",
      "        10148.4678, 13660.8369,  5495.3916, 38245.5938])\n",
      "tensor([ 5793.8735,  8869.2197,  8827.7451, 12523.6045, 11030.4414, 52590.8281,\n",
      "         7949.7476,  4005.4226,  8878.8574, 20341.3789,  8334.4580,  3736.9246,\n",
      "         5150.3169,  8703.4561, 18963.1719,  8486.9014, 14164.9453,  4243.5898,\n",
      "        18772.8262, 42560.4297, 10932.0791, 18444.5488, 10896.6133,  9946.9463,\n",
      "         4571.4131, 13880.9492,  2730.1079, 11820.7783,  5177.4429, 11737.8486,\n",
      "         6611.5747,  5535.8774, 36536.8750, 11555.6875, 11093.9844, 13640.6631,\n",
      "        46460.5859,  1880.0699,  7623.5181, 16405.6543,  9583.7246,  5459.4141,\n",
      "         5836.8037, 39241.4414, 14833.3945,  6521.4097,  4773.0742, 47289.7266,\n",
      "         4518.2300, 10159.4766,  6059.3662, 19257.8496, 12741.1670,  8797.9082,\n",
      "        13352.4766,  9779.9639, 13224.6934, 37284.8203,  3579.8286,  1629.8335,\n",
      "        16069.0850, 19235.8223,  4032.2407,  6799.4580])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20420.6055, 11008.3906, 39802.3516, 11094.9131,  3919.3872, 20135.9785,\n",
      "        11658.3789,  5240.1348,  1708.9258, 13030.3164,  5327.4004,  8782.4688,\n",
      "         6682.4009,  5587.0991, 17526.3105, 13887.9688, 11165.4180, 19749.3828,\n",
      "         7682.6699,  9080.3447, 26392.2598, 11883.8730,  7173.3599,  3309.7925,\n",
      "         4779.6025,  5426.9219, 14451.8350,  6746.7427, 26081.9590, 12112.2822,\n",
      "         5920.1040, 38998.5469, 35907.0117,  6341.7935,  6725.8091, 19496.7188,\n",
      "        12925.8857, 11399.6484,  4826.3560,  8612.3125,  5979.7310,  6272.4771,\n",
      "        26322.8652, 18322.1152,  9132.1406, 47896.7930, 44246.3320, 11961.2793,\n",
      "         2946.0974,  2731.9121,  8584.4238, 11443.1621,  3877.3042, 20624.8047,\n",
      "         2842.7607,  8023.1353,  7348.1421, 14407.8955,  3277.1609, 10231.5000,\n",
      "         5187.8267,  2322.6218, 15795.4727,  6255.7744])\n",
      "tensor([ 4687.7969, 24329.3379, 11394.0654,  8583.3838,  9535.4424, 10024.0820,\n",
      "         5191.5757, 11015.3203,  7467.0039,  4645.9453, 47417.1797,  4590.5381,\n",
      "         7281.5054,  7155.7979, 48316.0078,  7633.7207,  9179.2607, 48517.5625,\n",
      "         5211.0073,  4766.5093,  6338.0757,  4725.9038,  9861.1865,  9903.7754,\n",
      "        40720.5508,  1980.0699,  6933.2422,  8773.6094, 23887.6621,  7626.9932,\n",
      "        25718.6445, 13063.8828,  4989.5703,  7620.8408, 32108.6621,  3213.6221,\n",
      "         6185.3208, 14901.5166, 24096.1113,  3919.9067,  8859.7686,  8278.1475,\n",
      "         3366.6697,  9583.8936, 19040.8770,  5466.0967, 11312.4746,  2254.7966,\n",
      "         8902.8799,  3479.0305, 28443.2227, 10965.4463, 11027.3135,  4133.6416,\n",
      "        11068.2744, 17043.3418,  6282.2349, 18381.5977, 12649.6045, 12829.4551,\n",
      "         5265.0400,  9909.5566,  5377.0161, 12592.5342])\n",
      "tensor([ 5003.8530,  1712.4691,  9236.1387,  5301.3931, 10788.7412,  2352.9685,\n",
      "        26140.3594,  4809.4521,  6216.3872, 13929.6611,  2279.8958,  4113.1367,\n",
      "        16140.9229,  9249.4951, 28340.1895,  9625.9199,  5846.9175,  7742.2729,\n",
      "         8795.0664, 11411.6846,  9337.3311,  2714.4729,  4710.8916,  3234.1348,\n",
      "        36041.9883,  6621.8008,  6455.8628,  6658.1187, 11743.9346,  5390.6113,\n",
      "         2699.5684, 11033.6621,  5970.1597,  7409.4360, 11833.7822,  8162.7163,\n",
      "        10114.5957,  4518.8262, 19370.4336,  8515.7588, 12146.9707,  4452.5493,\n",
      "         5146.2993, 29186.4824, 15230.3242,  5397.6167, 17883.0195,  2117.3389,\n",
      "        40935.6445,  8059.6792,  3077.0955, 10795.9375, 12878.1699, 36189.1016,\n",
      "         4906.4097, 12905.0811,  8704.5488, 12404.8789,  1682.5970,  9202.4727,\n",
      "         8078.5361,  4436.6260,  7418.5220, 17776.6387])\n",
      "tensor([ 5241.4849,  7228.2158,  9471.1426,  4137.5225, 24513.0918,  4449.2339,\n",
      "         4185.0977, 45710.2070, 15179.2051,  2788.0889, 23241.4746,  5125.2158,\n",
      "        17063.4258,  8993.5400,  6488.8950,  5549.3247,  6712.0068,  6772.6304,\n",
      "        12612.1074, 39969.4414, 40142.9062, 23843.5332, 10499.2393,  4294.2544,\n",
      "        21220.4004, 11345.5186,  6640.5449, 23033.8848,  5152.1338,  5031.2695,\n",
      "         6201.7407,  5999.7490, 20296.8633, 24535.6992,  4884.5830,  4761.0718,\n",
      "         6046.6641, 11035.8584, 11554.2236, 26619.4922,  8219.2041,  7175.9937,\n",
      "        11107.4102, 11335.8467, 10096.9697, 14409.9561,  5258.5146,  7323.7349,\n",
      "        10407.0859,  9308.5625,  3502.8157,  8240.5898, 19361.9980, 37701.8750,\n",
      "        10141.1357, 13968.1250, 11212.9551,  4518.1045, 12979.3584,  2396.0959,\n",
      "        11735.8447,  4022.6128, 44241.9023,  5028.1250])\n",
      "tensor([ 9370.4639,  8987.6270, 12430.9531, 12251.3223,  1149.3959, 19988.1250,\n",
      "         5712.3555, 10806.8389,  6516.0615, 11363.2832,  2597.7791,  2134.9016,\n",
      "         9619.6699, 24520.2637, 10602.3848,  5272.1758, 13846.9619, 19521.9688,\n",
      "         9722.9551,  6515.7598,  6088.7759,  3378.9099, 11034.1230,  5136.1411,\n",
      "        37086.6680, 25026.4238, 24635.1055,  4356.1611,  6474.0132, 15006.5791,\n",
      "        17511.0918, 12363.5469,  2904.0879,  3615.3975,  2203.4719,  9222.4023,\n",
      "         3875.7341,  6379.7090, 18775.4609,  1631.8212,  9439.2363, 17150.6855,\n",
      "        11615.2969, 23261.0215,  7222.4863, 49577.6641, 33750.2930,  8827.2100,\n",
      "         5044.3613,  7746.2632,  8716.2393, 12103.8350, 10802.8574,  4237.1265,\n",
      "        18745.6289,  5256.2808, 23104.1172,  9273.9131, 11552.9043, 10841.7090,\n",
      "         2755.0210,  5630.4580,  4510.5015,  7965.5845])\n",
      "tensor([ 7296.7134, 23510.6797,  1261.4420,  6981.8662, 13256.1748, 10072.0547,\n",
      "        46113.5117, 12267.4463, 11232.9414,  4821.6626, 27375.9043, 23352.3184,\n",
      "         7337.7480, 13508.6172, 43943.8750, 15305.1963, 23082.9551, 12629.1660,\n",
      "         5425.0234,  6027.3472,  1633.9618,  9321.8613, 19933.4570,  1906.3583,\n",
      "        10256.5186, 12013.5322, 39047.2852, 12094.4775, 11342.8555,  8871.1514,\n",
      "         5194.6929, 18242.1328,  9723.1592, 36149.4844,  5454.9780,  1702.4553,\n",
      "         6417.0142,  9866.3047, 25678.7793, 23401.3066,  6292.8462, 10950.0391,\n",
      "        11386.5137, 11362.7549, 11908.1914,  6101.8604, 41920.0781, 11975.6758,\n",
      "         7443.6431,  8423.9121, 11356.6611, 42124.5156, 14115.0889,  9447.2500,\n",
      "        32471.2324,  3180.5100, 11967.3867, 38431.8398, 23862.7793, 24876.9414,\n",
      "        11020.7070, 18303.1406, 12474.2529, 11264.5410])\n",
      "tensor([ 4040.4421,  5038.1357, 10978.5791,  9411.0049,  2302.3000, 10992.3867,\n",
      "         8936.3086, 10909.8730,  6764.9863,  2855.4375, 10115.0088,  4944.6729,\n",
      "        21195.8184, 10601.6318, 13405.3906, 36580.2812,  4697.8984, 19528.0117,\n",
      "        47937.2695, 12957.1182, 14313.8467,  8716.3652,  9821.0898, 10887.2500,\n",
      "        47916.9648, 13747.8721,  5550.9214, 11798.5205, 12044.3418, 39983.4258,\n",
      "         1141.4451, 13502.7852, 12100.5918, 14194.9961,  7262.3726, 25274.0840,\n",
      "         5748.8198,  8591.2148,  3987.9260, 10987.3252,  2020.5522, 11250.8926,\n",
      "        10989.1992, 21533.6562, 16218.6719,  4402.4639, 10232.0596, 13451.1221,\n",
      "        20144.6738, 10992.3857,  9858.0254, 10325.2061,  6405.0771,  7358.1758,\n",
      "         6956.8833,  6099.9766,  6753.0381, 19080.2051, 32584.9629, 11879.1045,\n",
      "         8937.6973, 10991.3584,  6666.0552,  5313.6807])\n",
      "tensor([12558.4961,  3847.6741,  5829.9429, 27709.7969,  9453.6953, 41817.2773,\n",
      "         9634.5381, 43921.1836,  2281.3782,  8457.8184, 24956.6738,  9704.6680,\n",
      "        10451.2744, 20155.6504,  2680.9492, 17879.2539, 37270.1523,  4836.1221,\n",
      "        14201.5137,  6961.4736,  4260.7441,  6781.3540,  9172.3047,  5207.8936,\n",
      "        10961.7842,  5114.4463, 45008.9570,  9668.9141,  7445.9180, 21112.5098,\n",
      "         2789.0574, 34969.1055, 17878.9004,  9377.9043,  4805.6055,  2201.0972,\n",
      "        10961.4395,  8727.1738,  5142.2842, 11475.5439, 40974.1641,  6653.7886,\n",
      "         8444.4736,  8310.8389,  3208.7871,  6526.5928, 21232.1816,  6313.7588,\n",
      "         7147.1050,  4922.9160, 34393.4492, 16115.3047, 29918.5391,  9046.3711,\n",
      "         5488.2622, 18129.7715, 45610.9219, 10763.5752,  7526.7065,  5204.4038,\n",
      "        18944.2148,  9800.8887,  3914.3958,  6593.5083])\n",
      "tensor([ 5400.9805,  2217.4692, 14643.6221,  7228.7710, 17550.7637, 11534.8730,\n",
      "         6429.9795,  6235.9092,  2137.6536,  5313.0581,  5210.3770,  9166.4688,\n",
      "        19658.3535,  1877.9294,  6787.5308,  4618.7397,  8971.3008, 23632.3750,\n",
      "        11168.2471,  1728.8970,  8017.0610, 19107.7793,  9230.8125,  2128.4312,\n",
      "        11145.7910, 12982.8750,  8635.2988,  6604.7139,  2055.3250,  8591.3857,\n",
      "         6751.0796,  3536.5894,  6607.4302,  8753.9961,  5002.7827, 11433.1113,\n",
      "         5244.6787, 11016.3340,  7150.3506,  9778.3477, 39245.1641,  1639.5631,\n",
      "        10355.6406, 10680.1963,  7727.2534,  9781.5039,  1744.4650, 44397.3906,\n",
      "         4931.6470, 19515.5410, 10008.9678, 17081.0801, 46247.9648, 10922.4590,\n",
      "         5257.6685, 18806.1445,  5119.3516,  8733.2295, 11281.5967,  8526.8809,\n",
      "        11842.6240, 11298.1533,  3537.7029, 10740.0205])\n",
      "tensor([ 4768.7769, 17984.5977,  5028.1465, 30063.5801, 20335.8613,  2899.4893,\n",
      "        11008.7568, 42000.0352,  4708.9570,  9386.1611, 44400.4062,  1711.3367,\n",
      "         8697.2246, 12233.8281,  6750.3789,  6652.5288, 22493.6602,  2457.5020,\n",
      "         2198.1899, 22198.8750, 10976.2461, 10491.6934,  4667.6074, 48885.2422,\n",
      "        10806.4873,  9193.8389, 10537.9121,  8953.5049, 41097.1602,  6270.9673,\n",
      "         8047.2007,  9036.5869, 11117.1660, 10797.3359,  8988.1592,  6738.0117,\n",
      "        13073.8184, 12904.9053,  2750.1782,  5495.3623, 12635.6611, 14923.2422,\n",
      "         5525.4204,  7148.5503,  7634.4023, 46544.0820, 23945.7148, 14004.6660,\n",
      "        10959.6943,  3761.2920,  1674.6323,  6123.9985, 10621.7578, 12476.7812,\n",
      "         3526.3516, 19964.7461, 17663.1445,  6585.8774, 38169.4375, 14289.9834,\n",
      "         8583.9736, 54108.4180,  4892.4019,  9391.3457])\n",
      "tensor([15003.8145, 11922.0859,  3310.4224, 46718.1641,  1972.9500,  4883.8662,\n",
      "         2353.8313, 17560.3789,  1712.2271,  2527.8186,  3535.1780, 28039.5391,\n",
      "         5390.6162,  2207.6975,  9636.7324, 12557.6055,  4415.1587, 10594.5020,\n",
      "        19444.2656,  6677.5752,  4830.6299, 14043.4766,  8653.0039,  1621.8827,\n",
      "         6990.8960,  2150.4690,  4472.8804,  2261.5688,  8549.4805, 11837.1602,\n",
      "         5406.6313,  6403.4287,  5620.7578, 14051.1152,  7265.7026,  5379.2900,\n",
      "         8186.8477, 27218.4375,  1694.7964, 22218.1152,  7443.8477,  4801.4756,\n",
      "        26823.7402,  8766.9248,  9313.5957, 13457.9609,  3956.0715, 13936.8818,\n",
      "        47437.5430,  6658.3477, 16753.3984, 36397.5742,  6186.1270,  9820.2334,\n",
      "        17496.3066,  4274.2217, 18817.6211,  8712.5498,  9288.0264, 11272.3311,\n",
      "         4149.7358, 18415.4375,  6364.9355,  8606.1201])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20277.8066,  4320.4106, 11228.0234,  7609.9360, 26993.5117, 18823.5957,\n",
      "        16796.4121,  5116.5005, 19129.8770,  5062.8535, 10986.4014, 12222.8984,\n",
      "        10782.4883, 19726.1484,  4921.7368, 12648.7031, 11314.8672, 23514.5645,\n",
      "         6071.1353,  4756.5337, 11661.3965,  2639.0430,  3757.8447,  9523.9785,\n",
      "        12644.5889,  7724.7695, 10715.7383, 14571.8906, 10334.8125, 10991.8525,\n",
      "        11372.7080, 14453.7627, 10313.1445, 42856.8398, 10796.3506, 10090.1299,\n",
      "        19431.9727,  9365.2812,  8124.4082, 38392.7461, 21491.9805, 13844.5059,\n",
      "         8699.7979,  5271.1968, 26926.5137, 13555.0049,  9597.2881, 21828.0293,\n",
      "        15181.1689,  8601.3291,  3281.9583, 14154.8193,  5836.5205,  9163.1084,\n",
      "        24058.4453,  7161.5146, 11353.2275, 38092.4102,  3393.5928, 11163.5684,\n",
      "         9813.9365,  4917.9258,  4611.5186,  8587.1279])\n",
      "tensor([ 8678.6367,  4669.0410,  2407.0042, 12890.0576, 42705.1836,  4889.9995,\n",
      "        11339.5977, 18223.4512, 35491.6406, 13770.0977,  1708.0013,  9086.5273,\n",
      "        18232.0332, 18955.2207,  3484.3311,  6748.7524, 14283.4590, 17593.3750,\n",
      "         7688.3145, 20167.3359, 29762.3105, 12323.9355,  9210.7949, 36036.2812,\n",
      "         5451.4233,  8989.9814, 10882.7529, 13342.3301,  7372.1353,  6684.9663,\n",
      "        26125.6738,  3736.4646,  4752.7188,  8302.5361, 10952.2207, 13041.9209,\n",
      "         8688.8584, 10446.0498, 19434.5449,  7441.0532,  9074.2441,  5702.2310,\n",
      "        13675.2305,  5871.2061,  9450.8184, 13204.2861, 12265.8857,  3369.4167,\n",
      "        45841.4688,  5398.8911,  3522.6023, 12142.8438, 12629.8965,  9095.0684,\n",
      "        23306.5469,  7153.5537, 24499.0840, 17578.2148, 46217.6641, 14799.0312,\n",
      "         2721.3208,  4853.6582, 11247.4453,  6628.2686])\n",
      "tensor([ 5889.1040,  6788.5342, 12401.7461, 10376.2949,  6658.2373,  7705.1616,\n",
      "         9141.2588,  6674.1318, 15019.7598, 26875.1562,  5227.2632,  9655.7354,\n",
      "         8734.1172,  9143.5732,  1632.5645,  9755.0039,  3404.4211, 10092.3564,\n",
      "         8107.7363, 10264.4424,  5312.5171,  7121.3081,  4869.0752,  3021.8091,\n",
      "        38310.2305,  4360.3711, 10451.9365, 38258.9219, 40941.2852,  8624.3711,\n",
      "         5214.5659, 16710.0039,  2523.1694,  2200.8308, 23807.2402,  5050.9492,\n",
      "         4708.5298,  4745.0073,  9931.6240, 37484.4492,  6286.6333, 16776.3047,\n",
      "         8116.2690,  9921.9004, 47403.8789, 27000.9844, 11176.2168, 18328.2383,\n",
      "        39722.7461,  5312.1699,  5757.4136,  6258.0811, 11038.9854, 14001.1338,\n",
      "        17844.1328, 11070.5352,  3238.4358,  9748.9102, 10929.7871,  8217.4004,\n",
      "        37465.3438, 14744.0938, 12030.7793,  5221.3887])\n",
      "tensor([11537.5742,  4297.9951, 43223.4648, 42661.0117, 34593.8672, 10015.4395,\n",
      "         8583.3223,  6716.7861,  4788.8638,  6417.5674, 40690.5430, 11891.9922,\n",
      "         6655.1758,  1748.7740, 10825.2539, 10751.9248, 10726.4121, 23552.6621,\n",
      "        14109.8145, 37607.5273,  9724.5303,  4343.5864, 46199.6367, 46889.2617,\n",
      "         3353.4702,  5465.4326, 19593.2031, 26730.3984,  5934.3799,  8070.9092,\n",
      "        10594.2256,  5412.7876, 28287.8984, 10584.0400, 10493.9453,  4791.5850,\n",
      "         4863.7627, 18866.6875,  6018.9741,  3443.0640, 10897.6035,  6753.0000,\n",
      "         2210.1880,  7731.8579,  7162.0122, 36837.4688, 20334.6172, 12797.2100,\n",
      "         2221.5645, 16450.8945, 15817.9854, 11193.2080, 19480.2383,  9630.3975,\n",
      "         4934.0996,  6203.9019, 17101.9160,  6014.0322,  6416.2925, 48675.5195,\n",
      "        17500.2051,  9104.3770,  6457.3037, 11540.8369])\n",
      "tensor([19350.3691,  5012.4712,  7196.8672,  9957.4824, 13140.4922, 12757.7354,\n",
      "         6940.9097,  6971.5854, 12656.7852, 28476.7344,  5548.1787, 12835.6465,\n",
      "         6718.9780, 42983.4570,  5377.4580,  5966.8872, 30284.6426, 17942.1055,\n",
      "         7400.1914, 11366.3506,  6726.0503, 38711.0000, 12032.3262,  5898.1665,\n",
      "        25382.2969, 13648.6016, 25147.0195,  9850.4316,  3554.2029,  9485.2559,\n",
      "         6755.6372, 38349.7500, 40861.9336,  9411.1689,  7441.2729,  3591.6248,\n",
      "         5272.6763, 47412.0312,  9610.9561,  9979.2393, 11200.0918, 24671.6641,\n",
      "         1839.4099, 25081.7676,  7077.1895,  8592.1816, 10761.4844,  8564.4863,\n",
      "        18650.8203, 15731.3271, 11073.1758, 14256.1924,  4307.1519, 43753.3359,\n",
      "         5983.9121,  2483.7361,  5824.1021,  9174.1357,  6211.9331,  8083.9199,\n",
      "         6688.9746, 12838.9932,  5809.5537,  9584.8838])\n",
      "tensor([16417.6914,  1981.5819,  6330.9521, 19085.0898,  6350.9326, 12609.8867,\n",
      "         8218.7500, 11170.6738,  6756.5708, 21308.4199,  2331.5190,  4986.4912,\n",
      "         5238.0103, 17434.5801,  7421.1943,  5599.7192, 37133.8984, 10278.5635,\n",
      "        12112.9785, 19892.5918,  6475.1812,  5243.5674,  4500.3394, 19069.4746,\n",
      "        10274.5830,  6761.2153, 24476.4785,  4428.8877, 19594.8105,  5282.9453,\n",
      "         5150.5874,  4738.2681,  5455.4463,  4853.2388, 17626.2402,  5602.9517,\n",
      "        16657.7168, 45863.2031, 29523.1660,  6356.2705,  5151.9502, 33900.6523,\n",
      "        11226.9932, 11830.6074,  5003.3179,  7512.2671, 27037.9141,  2211.1309,\n",
      "         1727.7850, 15439.3281,  3850.3679, 22478.5996,  8551.3467,  4793.2402,\n",
      "         9872.7012,  6184.2993, 20630.2832, 46475.2383, 43287.3750, 13725.4717,\n",
      "         8732.0859,  8792.4785,  9254.5244,  8662.5176])\n",
      "tensor([ 3766.8838,  7804.1606,  3972.9248,  9516.3633, 27117.9941,  6777.4302,\n",
      "        20149.3223,  6548.1948, 41279.2422, 14768.0205, 11568.4766, 11300.4541,\n",
      "        12265.5068,  5527.3135, 12387.4072, 34472.8398,  6664.6860, 45702.0234,\n",
      "         9393.3584, 22412.6484,  9948.9697, 10720.6953,  4708.3374,  4522.0054,\n",
      "        10736.8711, 26489.5762,  6093.4038,  5822.7246,  9964.0596,  5415.6611,\n",
      "         4433.7495,  8769.6445, 32905.8164, 15238.4053,  8965.7959,  5185.2397,\n",
      "        10065.4131,  6763.1831, 24919.3574,  4846.9849,  8517.6445, 12129.6143,\n",
      "        10807.3350, 11664.4541,  6604.6367, 12731.0000,  6657.5737, 22060.1719,\n",
      "         7085.8442,  6832.8516,  6677.3218,  7671.1797, 36023.1484, 46661.4414,\n",
      "         9642.2871,  7810.5654,  5266.3657,  4929.9517,  8559.9883, 30269.4277,\n",
      "        10390.9131, 11504.0811,  3398.1965,  9873.8760])\n",
      "tensor([ 1242.8160, 27626.1797, 18608.2617,  9950.3486,  3980.1387, 11854.6738,\n",
      "         3704.3545, 13670.9150, 13393.7559, 22331.5664, 10693.0615, 36225.4023,\n",
      "        10600.5479,  6710.1919,  2395.1716, 19524.6582, 46145.6523, 17748.5059,\n",
      "         2404.7339,  6663.0903, 15746.4619, 28058.4707, 46654.7266, 21659.9297,\n",
      "         6543.0454, 13919.8232,  8823.9854,  7985.8149, 41919.0977, 13994.2930,\n",
      "         8491.9072,  3176.8159, 27941.2871, 23244.7910, 13065.5439,  7156.5244,\n",
      "        39532.2461,  9414.9199, 33907.5469,  7206.2949, 14133.0381,  2045.6853,\n",
      "         6389.8535, 11950.1514, 10106.1338,  4895.6797, 41661.6016,  4318.3760,\n",
      "         7034.8438,  3556.9224, 17174.6816, 10823.2266,  6211.3403, 14249.3955,\n",
      "        11674.8730,  6105.5820, 18310.7422,  4292.8584,  8277.5234,  4743.8374,\n",
      "         6112.3530,  6149.6147, 10751.5498,  9047.6494])\n",
      "tensor([21925.6836,  4357.0435,  8595.2559, 11884.0488, 12874.1396, 55135.4023,\n",
      "         3393.3564, 10746.0781,  3855.9429,  5859.6704, 10240.5527,  5929.4653,\n",
      "        24915.0469,  7222.7861,  4772.8760,  6775.9609, 46988.3516, 12950.0713,\n",
      "        16085.1279,  4698.2017,  4340.4409,  2741.9480,  1880.4871,  4934.7051,\n",
      "         7045.4990, 14049.8330,  3044.2134, 10563.2197, 23582.4688,  5253.5239,\n",
      "        37829.7227,  1964.7800,  5926.8462, 12491.8818, 44202.6523,  8553.9541,\n",
      "         8712.3389, 44061.4805,  6322.3701,  5008.2500, 16586.4980,  8827.5264,\n",
      "         4662.8926, 13228.8467,  9095.5312, 11396.9004,  4285.9956, 25154.1719,\n",
      "        39385.6328,  6198.7520, 10787.6904, 10832.7705, 11130.3535, 13557.7188,\n",
      "         3947.4131,  1526.3120, 33353.5000, 39836.5195,  5210.1001, 11111.9248,\n",
      "        19214.7051,  5257.9595, 23563.0156, 40932.4297])\n",
      "tensor([42284.9180, 28868.6641,  2026.9741, 18989.3652, 21806.1816, 11355.8174,\n",
      "        11185.9141, 39727.6133,  4894.7534,  4779.6025,  6216.3872,  8844.6348,\n",
      "        16776.3047,  5425.0234, 13846.9619, 17593.3750,  2689.4954, 10976.2461,\n",
      "         9884.6934, 41949.2422,  9138.3857,  8807.7295, 16796.4121,  8559.9883,\n",
      "        39586.8633,  3526.3516,  8716.2393, 12905.0811,  6664.6860, 19071.2871,\n",
      "        10431.9648,  6775.9609,  9948.9697, 41180.8711, 13393.7559,  8598.2676,\n",
      "         5327.4004,  4833.8750, 20277.8066,  1137.0110, 22218.1152, 13145.5684,\n",
      "        10250.4209,  6697.6084,  5257.5078, 48173.3594,  7625.1558, 11837.1602,\n",
      "         3234.1348, 11833.7822,  5427.3716,  2585.8506, 12838.9932, 11018.9854,\n",
      "        18817.6211, 36189.1016,  6674.1318,  8792.4785, 20630.2832,  4709.6787,\n",
      "         9046.3711,  6272.4771, 15554.3066, 11068.2744])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43753.3359,  2480.9790,  6684.9663,  4909.7246, 12533.9229,  8709.8965,\n",
      "         5000.0771,  6728.2705, 12491.4844,  3268.8467, 28868.6641,  8591.2148,\n",
      "         9182.0586, 12235.8389,  6112.3530, 11048.3555,  6322.3701,  4297.9951,\n",
      "         8116.2690,  4234.9268,  2352.9685,  6648.9106, 15439.3281, 17844.1328,\n",
      "         4752.7188, 18232.0332,  4350.5142,  4449.2339,  5654.8184,  2362.2290,\n",
      "         5347.5425, 18804.7520,  5257.9595,  5050.9492, 10824.4844,  8588.3887,\n",
      "        10991.8525,  2731.8477, 16739.4160,  6837.3687, 17740.2930,  8595.2559,\n",
      "         4766.5093,  4267.7329,  8816.9209, 11353.2275,  4747.0527, 14410.9316,\n",
      "        25729.1855,  6961.4736, 40941.2852, 14064.4277,  4058.7124, 39109.7539,\n",
      "         2680.9492,  9946.9463, 21677.2832,  5345.6807, 13508.6172,  7633.7207,\n",
      "         4294.2544, 19539.2422,  2045.6853,  4766.0220])\n",
      "tensor([ 6196.4482,  3005.1008,  5605.0166,  9878.7461,  9462.1533,  4784.8198,\n",
      "         4266.1660,  9783.7812, 17352.6797, 62592.8750,  6009.9824, 34166.2734,\n",
      "         9172.7686,  6616.0239, 10090.1299, 17716.6367, 34393.4492, 29523.1660,\n",
      "         2775.1921,  8347.1641,  5240.7651,  7050.0215,  6571.0244, 12638.1953,\n",
      "        11013.7119,  9222.4023, 10373.2334, 11460.4990,  3847.6741, 17009.3359,\n",
      "        14249.3955,  5288.9102, 16455.7070,  9360.9238,  4977.2495, 15169.1836,\n",
      "        38239.4102,  4360.3711,  6764.7871,  6117.4946, 12485.3594, 11555.6875,\n",
      "        44246.3320,  9962.6123,  6093.4038,  6415.2651, 17434.5801, 20099.7363,\n",
      "         9748.9102,  7201.7007,  2302.3000, 12360.8281,  4827.9048, 18091.3730,\n",
      "         5093.8828, 24956.6738,  5415.6611,  5857.2939, 42983.4570, 10787.6904,\n",
      "        18823.5957,  7475.3062,  3985.0098, 12493.1104])\n",
      "tensor([ 6680.3022, 11335.8467, 13352.4766, 19663.3809, 10118.4238, 14235.0723,\n",
      "        18444.5488,  8553.9541, 10148.4678,  6361.5400,  6738.9458, 37747.6836,\n",
      "        12491.8818, 46151.1250,  4040.5583,  5400.9805, 18303.1406,  5080.0962,\n",
      "        14224.7549,  8938.4619, 11168.2471, 11170.6738,  9321.8613, 10942.1318,\n",
      "        11228.0234, 39611.7578, 12644.5889, 18317.6250,  8732.0859, 13483.6182,\n",
      "         7441.5010,  4805.6055, 13770.0977,  8798.5928, 18629.5156, 25333.3320,\n",
      "        13143.8652,  4884.5830,  9875.6807, 19533.8320,  3398.1965,  8825.0859,\n",
      "        10959.3301,  2967.0117, 10019.4902, 18861.9668,  9716.7441,  4433.2295,\n",
      "        19530.0527,  4133.6416, 30166.6191,  5253.5239,  9046.0400, 11316.2148,\n",
      "         6088.7759,  5927.6006, 18310.7422, 23162.3965, 14692.6689,  9174.1357,\n",
      "        10806.8389, 11386.5137, 10159.4766, 11615.2969])\n",
      "tensor([11100.1875, 40242.7969,  7730.8525, 11891.9922,  4435.0942,  9861.0254,\n",
      "         3227.1211,  8116.6802,  8162.7163,  3404.4211,  9290.3174,  8797.9082,\n",
      "        13844.7969,  9236.1387,  6360.9937, 12254.8135, 39245.1641,  6705.0220,\n",
      "        11554.2236, 23757.2539,  6999.2749, 18157.8770,  7742.2729, 38310.2305,\n",
      "        15413.5967,  5702.2310, 10812.6650,  2102.2646,  6356.2705,  5466.0967,\n",
      "        13140.4922, 29010.3184,  1137.4697,  2026.9741,  4888.4927,  7935.2910,\n",
      "         4832.9170,  5271.1968, 13880.9492,  7098.9106,  4809.7949,  3385.3992,\n",
      "         4243.5898,  9453.6953,  3471.4097,  7147.4727, 19107.7793, 13073.8184,\n",
      "         8869.2197,  6338.0757, 16297.8457,  9921.9004, 45863.2031,  3293.2917,\n",
      "         6088.1855, 10711.7891,  3261.9417,  4687.7969,  6933.2422, 12635.6611,\n",
      "         6128.7974,  8107.7363,  4510.5015,  9634.5381])\n",
      "tensor([ 1149.3959, 44213.4219,  4461.8037,  7345.0840, 44641.1992,  1837.2371,\n",
      "        17174.6816, 10422.9170,  6403.4287, 24321.3496,  5191.5757, 10658.5049,\n",
      "        10232.0596,  4921.8262,  6079.6714, 37464.0859, 23258.9980,  1880.4871,\n",
      "         7168.9038,  5214.5659,  8877.5127,  2137.6536,  4345.8423, 22494.5723,\n",
      "         8186.8477,  8520.2656, 14004.3096,  4518.2300, 48673.5586,  2899.4893,\n",
      "         4340.0557,  8902.8799, 19496.7188,  6406.4106,  9370.4639, 40920.5703,\n",
      "         5282.9453,  4826.3560, 11961.2793,  7209.4917, 10370.9121, 15230.3242,\n",
      "         1252.4070,  5028.1465, 16710.0039,  2198.1899, 38392.7461,  4441.2134,\n",
      "        11922.0859,  9095.0684,  8486.9014, 44397.3906, 10621.7578,  4846.9199,\n",
      "         6255.7744, 17748.5059,  6101.8604,  4032.2407, 14254.6084, 16862.8984,\n",
      "         8569.8613,  7620.8408,  1146.7966, 21880.8203])\n",
      "tensor([10796.3506, 11388.3428, 42124.5156, 11661.3965,  9898.4219, 16420.6777,\n",
      "         8410.0469,  7726.8540, 21223.6758, 10825.2539,  6149.6147,  5748.8198,\n",
      "         7281.5054, 14833.3945, 41501.6562, 11197.0967,  7046.7222,  9365.2812,\n",
      "        19023.2598, 10823.2266, 35907.0117,  1534.3044,  6849.0259, 40861.9336,\n",
      "        42969.8516, 32548.3398,  5452.0752,  8563.3232,  6014.0322, 23033.8848,\n",
      "         9874.5771, 17765.9785, 33907.5469, 18306.3945, 11048.9639,  6046.6641,\n",
      "        12265.5068, 47400.9414,  6551.7500, 12592.5342, 42760.5039, 47412.0312,\n",
      "         6426.7280,  7147.1050, 11276.6777, 14988.4316,  5028.1250,  9755.0039,\n",
      "         6405.0771, 41920.0781,  4158.2539, 10792.8076,  9301.8936,  5729.0054,\n",
      "         9166.4688,  8731.4805,  9432.9258,  5150.3169, 13256.1748,  5972.3779,\n",
      "         5301.3931,  6628.2686,  5811.5186, 35069.3750])\n",
      "tensor([ 1391.5287, 16420.4941,  4721.4297,  1632.0363,  4317.3154,  3375.3062,\n",
      "         9137.7803,  6986.6968,  9490.4531, 11218.9795,  9877.6074, 33475.8164,\n",
      "         5934.3799,  5312.1699,  9304.7021,  9235.0645,  4662.8926, 18972.4941,\n",
      "         9630.3975, 18061.0137, 47403.8789, 11512.4053,  1632.5645,  2264.7219,\n",
      "         4040.4421, 14349.8906, 10763.5752,  4669.0410,  8334.5898,  6746.7427,\n",
      "         2407.0042, 25154.1719, 28403.3711,  2155.6814, 10584.0400,  5241.3555,\n",
      "         6099.9766, 20773.6270,  8026.6665, 30063.5801,  4452.5493,  6948.7007,\n",
      "         9270.5479,  3502.8157, 11268.0127, 11155.0889,  4869.0752,  9535.4424,\n",
      "        19964.7461, 11443.9365, 17929.3027, 46182.8320,  2527.8186,  7729.6455,\n",
      "         9595.4395, 39802.3516,  3947.4131,  4559.5342,  4590.5381,  2020.1770,\n",
      "        41279.2422, 41332.5312, 14643.6221,  5548.1787])\n",
      "tensor([17914.1328,  6712.0068,  5482.2915, 19453.6855,  5920.1040, 23300.7910,\n",
      "         1635.7336, 12479.7090,  6061.5435,  4889.0366, 12646.2070,  4930.2988,\n",
      "         2457.2112, 44501.3984,  8794.9893,  5851.0708,  6770.1924,  4912.5459,\n",
      "         3484.3311,  3875.7341, 28101.3340, 19480.2383,  6891.6792,  9694.4453,\n",
      "         7378.3042, 23632.3750,  5397.6167,  6358.1851,  8891.1396, 20462.9980,\n",
      "        37484.4492, 23261.0215, 10436.0957,  5379.2900, 21774.3223, 16232.8467,\n",
      "         8549.4805, 11674.1299,  7235.6587, 13041.9209, 47273.1094,  6527.7969,\n",
      "         9850.4316, 12649.6045, 11030.4414, 40160.7891, 21982.4746, 11801.3779,\n",
      "         7730.8125, 18765.8750,  6425.8906,  4343.5864,  9553.4902,  6579.8521,\n",
      "         3980.1387, 39722.7461, 17270.3926, 12478.4727, 41999.5195, 12233.8281,\n",
      "        38511.6289, 19305.1934,  5822.7246, 17879.6328])\n",
      "tensor([ 5211.0073, 39871.7031,  9387.7686,  6182.4146,  7731.8579,  8886.0176,\n",
      "         7085.8442,  6186.1270, 14829.3516, 13887.9688, 10950.0391,  5377.0161,\n",
      "         5210.3770, 27107.3418, 13405.3906, 14133.0381,  5515.8096,  6971.5854,\n",
      "        23945.7148,  3213.6221, 11184.4941,  8936.3086, 39000.4609,  2156.7517,\n",
      "         4791.5850,  4559.3188, 11033.6621,  7727.2534, 12129.6143,  1631.8212,\n",
      "         4661.2861,  4012.1570,  5390.6113, 10715.7383,  6765.6182, 29964.4688,\n",
      "         8424.0781, 11015.1748,  4853.6143, 12815.4453,  1629.8335,  3479.0305,\n",
      "         4508.5103,  5620.7578, 12645.1660, 10920.3223,  8653.0039,  9669.7842,\n",
      "        10348.4600,  5003.8530,  2721.3208,  8218.7500, 36910.6094, 37270.1523,\n",
      "         5207.8936,  5966.8872,  5167.4790,  7448.4038, 40103.8906, 18137.9727,\n",
      "         5540.3325, 37829.7227,  7196.8672,  8920.6895])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7337.7480, 10991.3584, 17942.1055, 11312.4746, 13224.6934, 12574.0488,\n",
      "         8023.1353,  8609.9229,  5560.0654, 10336.7979, 39983.4258,  9193.8389,\n",
      "        10008.9678, 46412.5352,  8342.9092,  9583.8936, 27941.2871,  6108.3462,\n",
      "        19121.4629,  5014.1821,  8027.9678, 10491.6934, 12146.9707, 27709.7969,\n",
      "         5550.9214,  3615.3975,  9724.5303, 21925.6836,  9283.5615,  2396.0959,\n",
      "         2639.0430,  6673.0005, 11093.9844, 19361.9980,  4260.7441,  6777.4302,\n",
      "         6653.7886,  9610.9561, 21806.1816,  2755.0210, 10381.4785, 10987.3252,\n",
      "        11735.8447, 10602.3848,  8976.1406,  7027.6987])\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "tensor([ 3279.8687, 21454.4941,  1720.3538,  6801.4375, 11946.6260,  7742.1099,\n",
      "        21736.3281,  4916.9531,  5515.8096, 17009.3359, 38433.5234,  8549.1367,\n",
      "        16099.3672,  7492.9854, 18091.3730, 41501.6562,  2661.1912, 42211.1367,\n",
      "        16455.7070,  4433.9160,  6571.5439, 26236.5801,  6318.7979,  5795.9058,\n",
      "        47305.3047, 11488.3174, 29101.7520, 19305.1934, 12479.5273,  7730.7632,\n",
      "        10118.0635,  9579.1553, 38126.2461,  7168.9038,  5482.2915, 47462.8945,\n",
      "         9051.9004,  5383.5361, 17352.4258, 10480.8955, 11945.1328,  8083.1782,\n",
      "         8347.1641, 40018.2305,  6686.4312,  2219.4451,  6662.1387,  7076.8926,\n",
      "         4790.3115,  9434.7314,  7102.4077, 10043.2490, 27533.9121,  5240.7651,\n",
      "         3533.2556,  4673.3921, 48173.3594,  4179.4453,  6133.8828,  9716.7441,\n",
      "         4461.8037,  5148.5527, 13974.4551,  7050.0215])\n",
      "loss: 9133714571264.000000  [   64/ 3630]\n",
      "tensor([18061.0137,  4877.9810,  7726.8540, 19121.4629,  8519.2305,  1769.5316,\n",
      "        18629.5156, 36124.5742, 10601.4121,  5824.7817,  9094.7920, 11743.2988,\n",
      "        25737.9805,  5014.1821,  4914.3413,  3777.2117, 15526.5752,  5241.3555,\n",
      "         4833.8750, 33131.8438,  6797.2192, 18157.8770,  6025.5762,  1121.8739,\n",
      "        10658.5049, 21982.4746, 12485.3594,  6659.9111,  5263.4888, 11611.7080,\n",
      "         7079.3530, 12265.9785,  7147.4727,  8415.5195,  8877.5127,  8303.0703,\n",
      "         2257.4753, 11741.7256,  5093.7124,  7978.5215, 10143.2617,  7672.9897,\n",
      "         5523.9819,  8594.0195,  9254.6621, 12818.6055,  9378.4590, 13099.8857,\n",
      "        19120.2246,  2130.6758, 10019.4902,  4266.1660,  7168.0327,  3500.6123,\n",
      "         2136.8823, 14319.0312,  4921.8262,  6123.5688,  8649.2002, 11107.3457,\n",
      "        14829.3516, 11273.8643, 15413.5967,  7144.8628])\n",
      "tensor([ 6505.3262, 19539.2422,  7325.0483,  7625.1558,  7257.4282, 12154.0332,\n",
      "         9654.1816,  4721.4297, 24890.1055,  1242.2600,  6425.8906,  6837.3687,\n",
      "         3981.9768,  3261.9417,  3471.4097, 19043.1621,  1646.4297, 17693.6172,\n",
      "         1621.3402, 10607.0059, 38792.6875,  9440.7178,  9387.7686, 39783.3828,\n",
      "        23503.1953, 13352.0996,  6309.6631, 46599.1094,  2710.7117, 21190.7207,\n",
      "        22028.7109, 13744.0557,  9360.9238,  4189.1133,  4795.6567,  8534.6719,\n",
      "        45826.9258, 22395.7441,  7027.6987,  2416.9551, 10799.3418, 14984.9824,\n",
      "        10776.0654, 28205.9336, 10226.2842,  4894.7534, 11253.4209, 10422.9170,\n",
      "         8116.6802, 15170.0693,  9632.6973,  4818.7358,  7263.6362,  9890.5205,\n",
      "        12254.8135, 12927.1318, 11370.8428, 13635.6377, 11307.3711,  1737.3760,\n",
      "        13125.3525, 35064.6719,  9091.8574,  3591.4800])\n",
      "tensor([13143.3428,  6009.9824,  3293.2917, 21348.7051, 44213.4219, 16150.7910,\n",
      "         8199.8369, 11881.3584, 26506.6816,  4022.5654, 21223.6758,  5621.9448,\n",
      "         6500.2358,  4977.2495,  5334.7085,  7209.4917, 10107.2207,  5560.0654,\n",
      "         9962.6123,  8280.6230,  9137.7803,  9861.0254,  7371.7720,  4912.5459,\n",
      "         4431.6924,  5699.8374, 10104.5605, 11754.4150, 17448.0000,  9046.0400,\n",
      "        23445.8926,  2867.1196, 16884.9238,  5427.3716, 36808.3750,  3070.8086,\n",
      "         4737.9204,  4889.0366, 13596.5107,  2727.3950, 11439.6240, 13915.5215,\n",
      "         8059.4062,  8835.2646,  9235.0645,  9462.1533, 11335.2959,  3410.3240,\n",
      "        27322.7344,  7987.8325,  7537.1641, 10278.1416,  3866.8552, 38321.4492,\n",
      "        15973.6201,  9878.7461,  8068.1851, 10741.0586,  5080.0962,  4350.5142,\n",
      "         8585.8467, 39774.2773, 12026.0957,  8794.9893])\n",
      "tensor([ 4533.9902,  4835.7505, 12877.6006, 28403.3711,  2473.3340, 17178.6816,\n",
      "         7765.3745,  7631.4805,  5851.0708,  3176.2876, 14478.3301, 16114.3955,\n",
      "        13844.7969, 14002.8252,  6750.4321,  4670.9858, 24393.6230, 13145.5684,\n",
      "         6412.4795,  5318.4067,  5466.6616, 14374.9912, 46200.9844,  9432.9258,\n",
      "         7583.2061,  5033.0952,  4559.5342,  5138.2568,  1627.2825,  6361.5400,\n",
      "         2974.1260,  5484.5405,  1967.0227,  9670.2822, 11674.1299,  4337.7354,\n",
      "        24321.3496, 11015.1846, 20462.9980,  6747.9126, 14007.2217,  8931.1758,\n",
      "        11100.1875, 37079.3711, 11018.9854,  2196.4731,  6579.8521, 30166.6191,\n",
      "        11048.9639,  6660.8281,  5727.1279,  6551.7500,  6414.1782, 16176.0850,\n",
      "        48549.1797,  4685.1455, 44627.9375, 30364.5371,  6143.1284,  4515.1943,\n",
      "        44501.3984,  4265.1128, 17879.6328, 10711.7891])\n",
      "tensor([11842.4424, 28101.3340,  7148.2329,  9520.0020, 10801.6104,  9272.5332,\n",
      "        11906.1250, 38709.1758,  4040.5583,  7624.6299, 11291.3857, 12244.5312,\n",
      "         6496.8862, 10977.2061, 34617.8398, 13019.5244, 10461.9795, 14436.8926,\n",
      "        17765.9785, 24915.2207, 18638.6133, 12846.3525, 18765.8750,  4928.3643,\n",
      "         7160.3081,  7060.2222, 21002.7578,  6358.1851,  4990.2998, 16420.4941,\n",
      "        19120.6562, 10942.1318,  2720.7209,  1515.3448,  9634.1523,  6923.6221,\n",
      "         5632.9487, 17108.6309, 17315.6914,  6951.1992,  6941.6724, 40419.0195,\n",
      "         8233.0977, 33732.6875, 12002.3662, 13937.6660,  4852.9478, 11261.8174,\n",
      "        10577.0869,  3756.6216, 38822.0898, 27082.6641,  8252.2842,  6020.6094,\n",
      "        13616.3584, 12269.6885,  5605.0166,  8342.9092, 10920.3223, 11497.8213,\n",
      "        20641.5938,  9198.3574, 36632.5430,  4561.1885])\n",
      "tensor([ 7789.6348, 19435.9277,  4501.2344,  5452.0752, 11085.5869,  9239.9736,\n",
      "         7152.6714, 18317.6250,  5354.0747, 46220.9961, 25421.9336,  9916.5957,\n",
      "        22606.0645, 11931.1250, 11280.1006,  6415.2651,  6680.3022,  6061.5435,\n",
      "         3925.7583,  5257.5078, 14113.4727, 11044.6406, 20411.7734,  9553.4902,\n",
      "         2155.6814, 14119.6201,  2154.3611, 12495.2910, 11643.1836, 42303.6914,\n",
      "        15412.6855,  4433.2295, 14426.0742,  6358.7764, 21880.8203, 18099.5254,\n",
      "        10967.5176,  3385.3992,  6437.9717,  4909.7246, 28923.1367,  4719.7363,\n",
      "         9880.0684,  5375.0381,  7371.1211,  6325.6953,  9875.6807, 19798.0547,\n",
      "         1986.9333,  5940.5850, 11365.9521,  5186.6426,  1639.5631, 24948.2012,\n",
      "         2719.2798,  8728.9082,  7552.9785, 15456.2949,  7442.3052,  3597.5959,\n",
      "         4326.2134, 13143.3369,  9143.6279, 17270.3926])\n",
      "tensor([ 8569.8613, 14003.7881,  6353.1885,  9285.9922,  3500.7427, 12533.9229,\n",
      "         9304.7021, 11318.2275,  9301.8936,  8516.8291,  9290.3174,  2102.2646,\n",
      "         6059.1729,  3591.4707,  8920.6895,  4527.1831,  4783.2427,  5267.8184,\n",
      "         8704.1816,  5572.8662, 13430.2646,  3292.5298, 17716.7422,  8603.8232,\n",
      "         6743.1680, 35956.5898, 11316.2148, 47055.5312, 35069.3750, 10550.3281,\n",
      "         4827.9048, 29010.3184, 12347.1719,  1815.8759, 11881.6777,  4053.6936,\n",
      "         3832.0100,  8520.2656,  6393.6035, 26018.9512, 17929.3027,  9138.3857,\n",
      "         6970.0791, 10769.5791,  4740.7705,  9964.0566, 41676.0820,  6330.5752,\n",
      "        11300.3760,  7740.3369,  5631.7529,  4463.2051,  6764.7871,  4200.9316,\n",
      "         9172.7686, 36085.2188, 44445.3477,  6377.8208,  9456.3994, 17352.6797,\n",
      "        11010.6445, 10289.4707, 46182.8320, 16692.1406])\n",
      "tensor([ 4832.6074,  7650.7739, 46151.1250,  6673.0005, 10791.9600,  4134.0825,\n",
      "         5540.3325, 14539.2803, 38800.3867, 10086.8330, 43813.8672,  7331.8779,\n",
      "         8623.3125, 11371.9863,  8930.9346, 10959.3301, 12094.7510, 11455.2803,\n",
      "        48824.4492, 14235.0723, 18861.9668,  1136.3994,  4523.3594,  6088.1855,\n",
      "         1704.7002, 11881.9697, 18903.4922, 42112.2344, 11454.0215,  6891.6792,\n",
      "        30438.9199,  2913.5691,  6164.6860,  5976.8311,  4891.4346, 10652.5762,\n",
      "         4910.7397,  7243.8135,  2566.4707, 19853.7910,  4873.8315,  6182.4146,\n",
      "         3878.7839,  7151.0918,  2801.9724,  9283.5615,  5510.3447,  7378.3042,\n",
      "        18451.5840,  6605.6602, 34672.1484,  5319.6094, 11187.6562, 11196.8926,\n",
      "        24667.4199,  4559.3188,  6586.8999,  1917.3184, 11179.6074,  4372.8345,\n",
      "         3544.2539, 14142.7627, 10381.4785, 20664.3613])\n",
      "tensor([ 6347.0557,  4837.5825, 20099.7363, 19663.3809,  4765.3315,  3352.8745,\n",
      "        11987.7871, 17069.8730,  6289.7549,  4296.2710,  4388.5278, 15022.6826,\n",
      "        36308.0508, 15820.6992, 11218.9795,  8752.8184,  5379.5894,  7640.3091,\n",
      "         3481.8679,  9018.6416, 11268.0127, 47928.0312,  6616.0239, 15408.4268,\n",
      "         3940.8850, 21082.1602,  8596.8281, 10508.9199,  5227.9888, 36197.6992,\n",
      "        14711.7344, 17742.1055,  3531.7224,  6406.4106, 14410.9316, 43871.3320,\n",
      "         5209.5786,  6948.7007, 21472.4785, 34779.6133, 21797.0000,  4719.5239,\n",
      "        10744.9287, 11270.3340, 23781.3574,  5245.2271,  4992.3765,  6572.0107,\n",
      "         6738.9458,  4119.3862,  5581.6836,  7441.5010, 12163.5576,  1635.7336,\n",
      "        15828.8213,  9446.0977, 25729.1855,  7235.6587, 34166.2734,  7890.6313,\n",
      "         8568.6943, 11013.7119,  4076.4971,  3895.4170])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7448.4038, 16495.7285,  7049.5171,  1711.0269,  5347.5425, 14254.6084,\n",
      "        48381.3633,  3577.9990,  8563.3232,  3611.9717,  4702.1309, 15010.1758,\n",
      "        11285.7783,  5120.5830,  8815.0703, 13112.6045, 18306.3945, 13952.1602,\n",
      "         6705.0220, 10237.2578, 19493.5508, 21774.3223, 19811.0117, 16173.2686,\n",
      "        39611.7578,  7985.6104,  9409.5928, 11082.5771, 10923.9336,  4960.0352,\n",
      "        41332.5312, 28950.4688, 22462.0430,  7639.4175,  4915.0601, 10602.7920,\n",
      "         4731.0303, 37742.5742, 24180.9336,  1664.9996,  8605.3613, 21978.6777,\n",
      "         9500.5732, 41180.8711, 13717.1807,  7201.7007, 12491.4844, 14239.9189,\n",
      "        25656.5762,  6035.6963, 12574.0488,  7518.0254, 14474.6748, 11023.4434,\n",
      "         1241.5649, 51194.5586,  5790.1455,  5267.5312,  7730.8525,  5551.1509,\n",
      "         9863.4717, 22192.4375, 46130.5273,  2690.1138])\n",
      "tensor([11397.8027, 39725.5195, 23326.9160,  6402.2915,  9390.1387,  5762.3604,\n",
      "        10214.6357,  2534.3938,  5167.4790,  7046.7222, 36021.0117, 11350.5879,\n",
      "         2220.0222,  6697.6084,  6766.8198, 32787.4570,  7730.8125,  1534.3044,\n",
      "        10544.1963, 29964.4688, 12680.1914,  6713.5645, 27043.2266, 36133.6328,\n",
      "         8283.6807,  8595.7656, 47496.4961,  4508.3540, 19252.2188, 15183.2344,\n",
      "         6079.6714,  1704.5681, 24901.6484,  2850.6838, 18648.4219, 19777.6309,\n",
      "        39793.6562,  4544.2349,  6657.9819,  4462.7217, 10823.6777, 18137.9727,\n",
      "        12198.5254, 11986.8477, 34241.5391,  5426.0776,  6196.4482,  6765.6182,\n",
      "        10631.4541,  7475.3062,  9182.0586,  9549.5654, 18218.1621,  6673.6426,\n",
      "        12839.8486, 12479.7090, 24106.9121,  4317.3154, 11856.4111, 11032.7969,\n",
      "        10713.6436, 30942.1914,  5247.9961, 12913.9922])\n",
      "tensor([ 4790.2144,  2782.4663,  4386.8716, 17740.2930, 20773.6270, 13470.8604,\n",
      "        21344.8477, 11212.6543, 14383.3535, 11305.9346, 10826.0000, 22153.5918,\n",
      "        40337.0664, 20009.6328, 12271.1777,  5466.0791, 12079.0439, 47291.0547,\n",
      "        23288.9277,  9298.7139,  7612.2227,  8798.5928, 20633.8691,  8396.6006,\n",
      "        11214.5322,  7210.7290, 17716.6367, 40920.5703, 13289.4209, 32248.7012,\n",
      "         4846.9199,  8027.9678,  3861.2097, 17758.2520, 20088.1133,  9361.3271,\n",
      "        43578.9375, 11072.3447,  6784.1021,  6250.4351, 10680.0986, 15359.1045,\n",
      "        14210.5361,  6849.0259,  4345.8423, 16790.6562, 19144.5762, 11058.1572,\n",
      "         4532.1323, 38168.0039,  4987.0674,  6877.9800,  6660.0488,  6758.4141,\n",
      "        25311.5215, 20177.6719, 11299.3428,  4151.0288, 25332.6934,  5884.6030,\n",
      "         9898.4219,  3882.6570,  3392.9768,  4364.6133])\n",
      "tensor([ 3268.8467, 24881.2305,  9884.6934, 11218.5703,  6785.8169,  5012.6846,\n",
      "         8891.1396,  8978.1855,  5891.5181,  5225.6348,  7729.6455, 12755.5361,\n",
      "         8824.9102,  8152.0234, 10129.2852, 11286.5391,  8718.8525, 44585.4570,\n",
      "         7517.5283, 16570.5996,  1163.4626,  8556.9072,  2480.9790, 18971.7168,\n",
      "         2775.1921, 21318.5898,  5472.4492, 11276.6777, 62592.8750, 11197.0967,\n",
      "        17012.1875,  5209.1787, 11280.4385,  3659.3459,  3227.1211,  8609.9229,\n",
      "         8699.4893, 16297.8457, 10579.7109, 11244.3770,  7193.5654,  9874.5771,\n",
      "         2241.7705, 48673.5586,  8527.5322,  8941.0898, 40273.6445,  5743.7627,\n",
      "         4433.5488,  7345.0840, 11500.3193, 12425.2158,  8743.7490,  8424.0781,\n",
      "         4529.8950,  4676.9346,  5128.0859,  1137.4697,  4508.5103,  4537.2729,\n",
      "        39556.4961, 12346.6543, 24873.3848,  5514.5098])\n",
      "tensor([41003.0234, 30322.2207,  6426.7280,  3906.1270,  9914.9834, 10450.5518,\n",
      "         8765.2490,  8807.7295,  8739.6240,  2104.1133,  1632.0363, 17468.9844,\n",
      "        11460.4990,  4646.7588,  4402.2329, 32351.1719, 21760.0762, 47400.9414,\n",
      "         4753.6367, 18264.6367,  1705.6245,  7316.6323, 10206.4766, 18524.0332,\n",
      "        12235.8389, 33307.5508,  1731.6770, 35585.5742, 12219.4395,  9034.2324,\n",
      "         2643.2686, 37747.6836, 19023.2598,  4900.3213,  7731.4272,  2264.7219,\n",
      "         7935.2910,  1146.7966,  8825.0859, 11657.7188,  9541.6953, 18033.9688,\n",
      "         3350.4404, 36910.6094,  5584.3057, 11680.1318,  4762.3291,  3005.1008,\n",
      "        23746.4551,  1261.8590,  5594.8457,  8442.6670,  4911.9731, 26412.2129,\n",
      "        14481.9785,  2138.0708, 39586.8633,  2457.2112,  9985.4580,  4346.6533,\n",
      "         9788.8662,  9914.8770,  4784.8198,  6875.9609])\n",
      "tensor([ 9595.4395,  4832.7827, 36869.9336,  1824.2854,  7256.7231,  9620.3311,\n",
      "         8713.6016, 24869.8359, 20709.0195, 10338.9316,  4618.5430, 16977.8887,\n",
      "         5969.7231,  6069.8608, 39508.8711,  6986.6968, 36103.1719,  2203.7358,\n",
      "        11015.1748,  1137.0110,  8419.5869,  8964.0605, 10928.8486, 27302.6289,\n",
      "         5514.9214, 36898.7344,  3201.2451, 21677.2832,  4976.8608, 10792.8076,\n",
      "        39871.7031,  1135.9407,  7160.3301, 13489.0400,  5511.5137, 11326.7148,\n",
      "         9490.4531,  9896.7666,  9563.0293,  8844.6348, 11358.5850, 13415.0381,\n",
      "         8211.1006,  4952.9585,  6182.0483, 27724.2891,  4347.0234,  5729.0054,\n",
      "         4949.7588, 18767.7383,  8825.4463, 10698.1123, 47069.8359, 28468.9199,\n",
      "         6268.3389, 18804.7520,  9335.0225,  5518.2974,  5469.8594,  8334.5898,\n",
      "         8938.4619, 18618.7344,  6527.7969,  6338.7256])\n",
      "tensor([40824.5898,  4375.7065,  7629.2109, 16500.1699, 44145.9219, 19124.3691,\n",
      "         4746.3442,  1615.7667,  2927.0647, 18596.5156, 12494.1885, 11436.7383,\n",
      "        11801.3779, 17904.5273, 20781.4883,  2902.9065,  4267.7329, 39000.4609,\n",
      "         8052.0400,  9877.6074, 60021.3984,  9564.2031,  9785.7725, 28954.2754,\n",
      "        10790.3672,  8976.1406,  2585.2690,  9763.2705,  4958.0732, 13123.6904,\n",
      "        12333.8281, 18838.7031,  4008.6965,  4341.8853,  8932.0840, 18435.6348,\n",
      "         8151.5220, 43097.4141,  9841.7959,  6686.3301, 42760.5039,  4466.6216,\n",
      "         4482.7412,  8428.0693,  6878.5137,  9421.6201,  4983.4512, 35391.7617,\n",
      "        15011.1729, 27346.0430,  8975.1846, 10982.5010, 11065.9971,  4765.5669,\n",
      "        12815.4453,  8762.7617, 12148.6709, 12636.3799,  5348.0586,  6658.6318,\n",
      "         1631.6683,  5840.4814,  4816.4673, 17731.7051])\n",
      "tensor([ 4977.9277, 10079.9287,  2217.6013,  3375.3062, 39109.7539,  6239.0723,\n",
      "        15322.8506, 11538.4209, 11848.1406, 36325.0352, 21259.3789,  7630.3252,\n",
      "         7153.4839,  9385.6670,  5972.3779, 25517.1133, 11658.1152, 25354.4531,\n",
      "        24508.0273,  6344.1738, 14692.6689, 44423.8047, 14224.7549,  5093.8828,\n",
      "        14200.8975, 11048.3555,  4234.9268,  6985.5068,  4705.4697,  8269.0439,\n",
      "         6117.4946,  5345.6807,  5438.7490,  7887.6675, 21103.6543, 18473.0508,\n",
      "        19071.2871, 12830.8213,  2801.2588, 16886.6387, 18972.4941,  9694.4453,\n",
      "        46412.5352,  9566.9912,  5708.8672,  9282.6914, 13483.6182,  4930.2988,\n",
      "         3533.4658, 15403.1816,  6341.8433,  4766.0220, 17179.5215,  4487.3071,\n",
      "        28724.9434, 17515.5254, 26109.3281,  5669.1655,  3693.4280, 24707.1543,\n",
      "         5123.0977, 17129.8613, 11987.1680,  6073.2026])\n",
      "tensor([ 9869.8105,  7968.3511,  8410.0469,  5857.2939, 12655.3779,  5662.2251,\n",
      "        11578.8184,  7576.7598, 18259.2168, 40242.7969,  8797.1270, 27963.1992,\n",
      "         6389.9458, 23757.2539, 42656.6914, 19533.8320,  9602.3350,  5958.8755,\n",
      "        12493.1104, 15554.3066, 13217.0947, 35595.5898, 11265.8232, 13224.0566,\n",
      "         2494.0220,  7165.7222, 28624.6621,  4434.9209, 11200.5312,  4746.4692,\n",
      "         8767.5732,  5395.9326,  7080.1421,  3056.3882,  6375.3955, 14349.8906,\n",
      "         4988.3501,  2498.4143,  4435.0942,  9676.6895,  4456.7559, 25333.3320,\n",
      "         3857.7593, 13019.1611, 29141.3594, 10818.9297, 10336.7979, 13981.8506,\n",
      "        11258.9199, 20745.9883,  2731.8477, 21984.4707, 13126.6777,  7499.3477,\n",
      "        11004.0498, 11481.3750,  4747.0527, 63770.4297,  6360.9937,  8944.1152,\n",
      "        10118.4238, 16577.7793, 12105.3203, 23162.3965])\n",
      "tensor([ 2716.0391, 10766.3447, 17914.1328,  5458.0464,  9473.7930,  7786.8691,\n",
      "         4239.8926, 16420.6777,  3985.0098,  8956.8672, 14006.8447, 30203.5801,\n",
      "         6128.7974, 29158.3398, 18246.4961,  5927.6006,  5320.1294, 10869.6318,\n",
      "        16862.8984, 12020.1104, 48885.1367,  9086.0244, 11289.1094, 27808.7246,\n",
      "        10431.9648,  2967.0117, 38117.9766, 39597.4062, 20630.7500, 15230.3584,\n",
      "        29330.9824,  2689.4954, 11184.4941,  8520.0264,  2362.2290,  4438.2632,\n",
      "         6821.3232, 40103.8906,  9630.5283,  8174.4531,  9101.7979,  2709.1118,\n",
      "        11390.6504, 11723.9111,  6311.9712,  9783.7812, 25309.4883,  4734.0195,\n",
      "         4751.0698,  4536.2588, 11509.6084, 16739.4160,  6662.1602, 13822.8027,\n",
      "         9262.5146,  4853.6143, 41949.2422, 13047.3320, 25891.7871, 14003.8545,\n",
      "        10906.6855,  6704.9863,  5615.3691, 10348.4600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11576.1299,  5347.0938,  4012.1570,  4832.9170, 13129.6035,  3493.3052,\n",
      "        10987.7705, 10269.4600, 16173.1904, 12153.3301, 10755.5332,  8816.9209,\n",
      "        12478.4727, 12638.1953,  7954.5171, 10838.7627,  8026.6665, 12475.3516,\n",
      "         5483.0806,  4784.0469, 34428.3711,  4709.6787, 14193.6133,  4809.7949,\n",
      "         2709.2439,  8865.4697,  5856.4531, 18820.2930, 11247.5459,  9058.7305,\n",
      "        11408.6787,  4828.5142,  6805.5850, 13887.2041,  8596.4873, 10649.4697,\n",
      "         8615.2998,  1263.2490, 20309.9355,  8968.3301,  1837.2371, 24227.3379,\n",
      "        13473.8389,  6475.8110, 13712.7646,  8598.2676,  6748.5913, 21595.3828,\n",
      "         5261.7490,  1622.1885,  6650.1948,  6453.5278,  8796.3018, 10012.6191,\n",
      "         7533.3374,  7392.6172, 12251.2207, 32548.3398,  5649.7148, 38499.0742,\n",
      "         5654.8184, 11566.3008,  4805.2856, 18237.9766])\n",
      "tensor([20172.1641, 25111.4336,  2400.4021,  8588.3887,  2020.1770, 10702.6426,\n",
      "        24971.7656, 11003.6924, 38239.4102,  3935.1799,  4927.7622,  6572.6685,\n",
      "         6571.0244,  5857.0200,  6733.6309, 16851.3691,  5325.6509, 21098.5547,\n",
      "         5410.2686, 12268.6318, 10560.4922, 10576.4160,  1391.5287,  6603.6201,\n",
      "        10992.1768,  6665.0405,  6123.5933, 16232.8467,  6108.3462,  2585.8506,\n",
      "         9097.2939,  5385.3379,  4058.7124,  9182.1699,  3161.4541, 13429.0352,\n",
      "        16118.8936,  9270.5479, 12646.2070, 11232.4961, 38514.6719,  3206.4915,\n",
      "        14455.6445,  4281.1631,  4645.6904, 43896.3750, 12142.5781, 14988.4316,\n",
      "        11155.0889, 12360.8281,  9239.3896, 10138.4346,  6449.4521, 43254.4180,\n",
      "        28152.8223,  6284.9707, 14004.3096,  8823.2793,  4953.1499,  4714.8999,\n",
      "        40160.7891,  5017.6616,  4340.0557, 27292.3242])\n",
      "tensor([20813.8711,  5368.5508,  6457.8433, 44069.0625, 34203.3672, 58571.0742,\n",
      "        13275.3184,  4472.5229, 47273.1094,  1969.6140,  4670.6118,  5377.8545,\n",
      "         3062.5083,  3904.8750,  5472.7295,  4689.0356, 44790.4922, 11938.2559,\n",
      "        36219.4062, 11115.5176, 10250.4209,  5811.5186,  4767.5073, 14358.3643,\n",
      "         7738.2622, 11346.8027,  4349.4619, 13343.0127, 12916.5557, 41034.2227,\n",
      "         6378.0747,  4957.9004, 11388.3428,  9581.1436, 11049.1953,  2250.8352,\n",
      "         9910.3594,  4707.6753,  4391.6519,  8548.1328,  6414.2407,  5288.9102,\n",
      "         9978.4072, 18836.8555, 31620.0020,  7098.9106, 19530.0527, 47974.2344,\n",
      "         1977.8149,  7494.2979, 11443.9365, 10882.8584, 22494.5723, 11313.6768,\n",
      "         3392.3652,  1131.5066,  9290.1396,  7117.9800,  1628.4709,  6503.9780,\n",
      "        12231.6133,  5000.0771,  8731.4805,  9491.4062])\n",
      "tensor([14064.4277, 11196.6904,  5507.6475, 13390.5586, 11129.3320, 14418.2803,\n",
      "         6999.2749,  6335.7681, 15169.1836, 30184.9375, 18150.9121, 46255.1133,\n",
      "        44641.1992, 14310.4971,  7237.4185,  1909.5275,  9991.0381, 15161.5342,\n",
      "        10370.9121, 10824.4844,  7112.5806,  1725.5869,  8886.0176,  3532.9143,\n",
      "        11512.4053, 23258.9980,  3994.1777, 37662.7617, 10435.0654,  4948.6729,\n",
      "        18707.2246,  6009.3750,  6265.1655,  5930.1489,  6227.6714, 26467.0977,\n",
      "         6728.2705,  8547.6914, 14001.2871, 10879.1729,  4661.2861,  1634.5734,\n",
      "        11013.7930,  4686.3887,  5124.1885, 10812.6650,  3394.4917, 34838.8711,\n",
      "         6013.1274, 32734.1855,  6770.1924,  4527.1177,  5855.9023, 38054.4922,\n",
      "         4441.2134, 13143.8652, 16378.9141,  9590.4580,  2459.7202, 18191.7344,\n",
      "        11520.0996,  8583.2227, 11262.1416, 10436.0957])\n",
      "tensor([ 3943.5955, 28105.2344, 10805.9551, 15648.6582, 11482.6348,  5127.6079,\n",
      "         6648.9106,  9012.5898, 40232.9336, 24904.3164,  4772.9556, 10056.7275,\n",
      "         6238.2979, 12645.1660,  5709.1646,  4158.2539, 11120.1904, 10676.0977,\n",
      "        23300.7910,  5246.0469,  5910.9438,  1826.8430, 10373.2334,  1252.4070,\n",
      "         3353.2839, 42969.8516, 20878.7852,  4822.7959, 38746.3555, 12387.7539,\n",
      "         9441.5166, 14925.9062, 41999.5195, 27107.3418, 24685.0215, 13673.9072,\n",
      "        19453.6855, 23065.4199, 12643.3779,  6032.5845, 10611.9785, 13462.5195,\n",
      "         2156.7517, 37464.0859,  8671.1914, 38511.6289,  5364.4507, 11259.7061,\n",
      "         9644.2529,  4888.4927,  9669.7842,  7960.8682,  5086.6528, 33475.8164,\n",
      "         4674.7974,  6312.1934,  3594.1709,  1625.4337, 11166.6670,  8709.8965,\n",
      "        10148.4678, 13660.8369,  5495.3916, 38245.5938])\n",
      "tensor([ 5793.8735,  8869.2197,  8827.7451, 12523.6045, 11030.4414, 52590.8281,\n",
      "         7949.7476,  4005.4226,  8878.8574, 20341.3789,  8334.4580,  3736.9246,\n",
      "         5150.3169,  8703.4561, 18963.1719,  8486.9014, 14164.9453,  4243.5898,\n",
      "        18772.8262, 42560.4297, 10932.0791, 18444.5488, 10896.6133,  9946.9463,\n",
      "         4571.4131, 13880.9492,  2730.1079, 11820.7783,  5177.4429, 11737.8486,\n",
      "         6611.5747,  5535.8774, 36536.8750, 11555.6875, 11093.9844, 13640.6631,\n",
      "        46460.5859,  1880.0699,  7623.5181, 16405.6543,  9583.7246,  5459.4141,\n",
      "         5836.8037, 39241.4414, 14833.3945,  6521.4097,  4773.0742, 47289.7266,\n",
      "         4518.2300, 10159.4766,  6059.3662, 19257.8496, 12741.1670,  8797.9082,\n",
      "        13352.4766,  9779.9639, 13224.6934, 37284.8203,  3579.8286,  1629.8335,\n",
      "        16069.0850, 19235.8223,  4032.2407,  6799.4580])\n",
      "tensor([20420.6055, 11008.3906, 39802.3516, 11094.9131,  3919.3872, 20135.9785,\n",
      "        11658.3789,  5240.1348,  1708.9258, 13030.3164,  5327.4004,  8782.4688,\n",
      "         6682.4009,  5587.0991, 17526.3105, 13887.9688, 11165.4180, 19749.3828,\n",
      "         7682.6699,  9080.3447, 26392.2598, 11883.8730,  7173.3599,  3309.7925,\n",
      "         4779.6025,  5426.9219, 14451.8350,  6746.7427, 26081.9590, 12112.2822,\n",
      "         5920.1040, 38998.5469, 35907.0117,  6341.7935,  6725.8091, 19496.7188,\n",
      "        12925.8857, 11399.6484,  4826.3560,  8612.3125,  5979.7310,  6272.4771,\n",
      "        26322.8652, 18322.1152,  9132.1406, 47896.7930, 44246.3320, 11961.2793,\n",
      "         2946.0974,  2731.9121,  8584.4238, 11443.1621,  3877.3042, 20624.8047,\n",
      "         2842.7607,  8023.1353,  7348.1421, 14407.8955,  3277.1609, 10231.5000,\n",
      "         5187.8267,  2322.6218, 15795.4727,  6255.7744])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4687.7969, 24329.3379, 11394.0654,  8583.3838,  9535.4424, 10024.0820,\n",
      "         5191.5757, 11015.3203,  7467.0039,  4645.9453, 47417.1797,  4590.5381,\n",
      "         7281.5054,  7155.7979, 48316.0078,  7633.7207,  9179.2607, 48517.5625,\n",
      "         5211.0073,  4766.5093,  6338.0757,  4725.9038,  9861.1865,  9903.7754,\n",
      "        40720.5508,  1980.0699,  6933.2422,  8773.6094, 23887.6621,  7626.9932,\n",
      "        25718.6445, 13063.8828,  4989.5703,  7620.8408, 32108.6621,  3213.6221,\n",
      "         6185.3208, 14901.5166, 24096.1113,  3919.9067,  8859.7686,  8278.1475,\n",
      "         3366.6697,  9583.8936, 19040.8770,  5466.0967, 11312.4746,  2254.7966,\n",
      "         8902.8799,  3479.0305, 28443.2227, 10965.4463, 11027.3135,  4133.6416,\n",
      "        11068.2744, 17043.3418,  6282.2349, 18381.5977, 12649.6045, 12829.4551,\n",
      "         5265.0400,  9909.5566,  5377.0161, 12592.5342])\n",
      "tensor([ 5003.8530,  1712.4691,  9236.1387,  5301.3931, 10788.7412,  2352.9685,\n",
      "        26140.3594,  4809.4521,  6216.3872, 13929.6611,  2279.8958,  4113.1367,\n",
      "        16140.9229,  9249.4951, 28340.1895,  9625.9199,  5846.9175,  7742.2729,\n",
      "         8795.0664, 11411.6846,  9337.3311,  2714.4729,  4710.8916,  3234.1348,\n",
      "        36041.9883,  6621.8008,  6455.8628,  6658.1187, 11743.9346,  5390.6113,\n",
      "         2699.5684, 11033.6621,  5970.1597,  7409.4360, 11833.7822,  8162.7163,\n",
      "        10114.5957,  4518.8262, 19370.4336,  8515.7588, 12146.9707,  4452.5493,\n",
      "         5146.2993, 29186.4824, 15230.3242,  5397.6167, 17883.0195,  2117.3389,\n",
      "        40935.6445,  8059.6792,  3077.0955, 10795.9375, 12878.1699, 36189.1016,\n",
      "         4906.4097, 12905.0811,  8704.5488, 12404.8789,  1682.5970,  9202.4727,\n",
      "         8078.5361,  4436.6260,  7418.5220, 17776.6387])\n",
      "tensor([ 5241.4849,  7228.2158,  9471.1426,  4137.5225, 24513.0918,  4449.2339,\n",
      "         4185.0977, 45710.2070, 15179.2051,  2788.0889, 23241.4746,  5125.2158,\n",
      "        17063.4258,  8993.5400,  6488.8950,  5549.3247,  6712.0068,  6772.6304,\n",
      "        12612.1074, 39969.4414, 40142.9062, 23843.5332, 10499.2393,  4294.2544,\n",
      "        21220.4004, 11345.5186,  6640.5449, 23033.8848,  5152.1338,  5031.2695,\n",
      "         6201.7407,  5999.7490, 20296.8633, 24535.6992,  4884.5830,  4761.0718,\n",
      "         6046.6641, 11035.8584, 11554.2236, 26619.4922,  8219.2041,  7175.9937,\n",
      "        11107.4102, 11335.8467, 10096.9697, 14409.9561,  5258.5146,  7323.7349,\n",
      "        10407.0859,  9308.5625,  3502.8157,  8240.5898, 19361.9980, 37701.8750,\n",
      "        10141.1357, 13968.1250, 11212.9551,  4518.1045, 12979.3584,  2396.0959,\n",
      "        11735.8447,  4022.6128, 44241.9023,  5028.1250])\n",
      "tensor([ 9370.4639,  8987.6270, 12430.9531, 12251.3223,  1149.3959, 19988.1250,\n",
      "         5712.3555, 10806.8389,  6516.0615, 11363.2832,  2597.7791,  2134.9016,\n",
      "         9619.6699, 24520.2637, 10602.3848,  5272.1758, 13846.9619, 19521.9688,\n",
      "         9722.9551,  6515.7598,  6088.7759,  3378.9099, 11034.1230,  5136.1411,\n",
      "        37086.6680, 25026.4238, 24635.1055,  4356.1611,  6474.0132, 15006.5791,\n",
      "        17511.0918, 12363.5469,  2904.0879,  3615.3975,  2203.4719,  9222.4023,\n",
      "         3875.7341,  6379.7090, 18775.4609,  1631.8212,  9439.2363, 17150.6855,\n",
      "        11615.2969, 23261.0215,  7222.4863, 49577.6641, 33750.2930,  8827.2100,\n",
      "         5044.3613,  7746.2632,  8716.2393, 12103.8350, 10802.8574,  4237.1265,\n",
      "        18745.6289,  5256.2808, 23104.1172,  9273.9131, 11552.9043, 10841.7090,\n",
      "         2755.0210,  5630.4580,  4510.5015,  7965.5845])\n",
      "tensor([ 7296.7134, 23510.6797,  1261.4420,  6981.8662, 13256.1748, 10072.0547,\n",
      "        46113.5117, 12267.4463, 11232.9414,  4821.6626, 27375.9043, 23352.3184,\n",
      "         7337.7480, 13508.6172, 43943.8750, 15305.1963, 23082.9551, 12629.1660,\n",
      "         5425.0234,  6027.3472,  1633.9618,  9321.8613, 19933.4570,  1906.3583,\n",
      "        10256.5186, 12013.5322, 39047.2852, 12094.4775, 11342.8555,  8871.1514,\n",
      "         5194.6929, 18242.1328,  9723.1592, 36149.4844,  5454.9780,  1702.4553,\n",
      "         6417.0142,  9866.3047, 25678.7793, 23401.3066,  6292.8462, 10950.0391,\n",
      "        11386.5137, 11362.7549, 11908.1914,  6101.8604, 41920.0781, 11975.6758,\n",
      "         7443.6431,  8423.9121, 11356.6611, 42124.5156, 14115.0889,  9447.2500,\n",
      "        32471.2324,  3180.5100, 11967.3867, 38431.8398, 23862.7793, 24876.9414,\n",
      "        11020.7070, 18303.1406, 12474.2529, 11264.5410])\n",
      "tensor([ 4040.4421,  5038.1357, 10978.5791,  9411.0049,  2302.3000, 10992.3867,\n",
      "         8936.3086, 10909.8730,  6764.9863,  2855.4375, 10115.0088,  4944.6729,\n",
      "        21195.8184, 10601.6318, 13405.3906, 36580.2812,  4697.8984, 19528.0117,\n",
      "        47937.2695, 12957.1182, 14313.8467,  8716.3652,  9821.0898, 10887.2500,\n",
      "        47916.9648, 13747.8721,  5550.9214, 11798.5205, 12044.3418, 39983.4258,\n",
      "         1141.4451, 13502.7852, 12100.5918, 14194.9961,  7262.3726, 25274.0840,\n",
      "         5748.8198,  8591.2148,  3987.9260, 10987.3252,  2020.5522, 11250.8926,\n",
      "        10989.1992, 21533.6562, 16218.6719,  4402.4639, 10232.0596, 13451.1221,\n",
      "        20144.6738, 10992.3857,  9858.0254, 10325.2061,  6405.0771,  7358.1758,\n",
      "         6956.8833,  6099.9766,  6753.0381, 19080.2051, 32584.9629, 11879.1045,\n",
      "         8937.6973, 10991.3584,  6666.0552,  5313.6807])\n",
      "tensor([12558.4961,  3847.6741,  5829.9429, 27709.7969,  9453.6953, 41817.2773,\n",
      "         9634.5381, 43921.1836,  2281.3782,  8457.8184, 24956.6738,  9704.6680,\n",
      "        10451.2744, 20155.6504,  2680.9492, 17879.2539, 37270.1523,  4836.1221,\n",
      "        14201.5137,  6961.4736,  4260.7441,  6781.3540,  9172.3047,  5207.8936,\n",
      "        10961.7842,  5114.4463, 45008.9570,  9668.9141,  7445.9180, 21112.5098,\n",
      "         2789.0574, 34969.1055, 17878.9004,  9377.9043,  4805.6055,  2201.0972,\n",
      "        10961.4395,  8727.1738,  5142.2842, 11475.5439, 40974.1641,  6653.7886,\n",
      "         8444.4736,  8310.8389,  3208.7871,  6526.5928, 21232.1816,  6313.7588,\n",
      "         7147.1050,  4922.9160, 34393.4492, 16115.3047, 29918.5391,  9046.3711,\n",
      "         5488.2622, 18129.7715, 45610.9219, 10763.5752,  7526.7065,  5204.4038,\n",
      "        18944.2148,  9800.8887,  3914.3958,  6593.5083])\n",
      "tensor([ 5400.9805,  2217.4692, 14643.6221,  7228.7710, 17550.7637, 11534.8730,\n",
      "         6429.9795,  6235.9092,  2137.6536,  5313.0581,  5210.3770,  9166.4688,\n",
      "        19658.3535,  1877.9294,  6787.5308,  4618.7397,  8971.3008, 23632.3750,\n",
      "        11168.2471,  1728.8970,  8017.0610, 19107.7793,  9230.8125,  2128.4312,\n",
      "        11145.7910, 12982.8750,  8635.2988,  6604.7139,  2055.3250,  8591.3857,\n",
      "         6751.0796,  3536.5894,  6607.4302,  8753.9961,  5002.7827, 11433.1113,\n",
      "         5244.6787, 11016.3340,  7150.3506,  9778.3477, 39245.1641,  1639.5631,\n",
      "        10355.6406, 10680.1963,  7727.2534,  9781.5039,  1744.4650, 44397.3906,\n",
      "         4931.6470, 19515.5410, 10008.9678, 17081.0801, 46247.9648, 10922.4590,\n",
      "         5257.6685, 18806.1445,  5119.3516,  8733.2295, 11281.5967,  8526.8809,\n",
      "        11842.6240, 11298.1533,  3537.7029, 10740.0205])\n",
      "tensor([ 4768.7769, 17984.5977,  5028.1465, 30063.5801, 20335.8613,  2899.4893,\n",
      "        11008.7568, 42000.0352,  4708.9570,  9386.1611, 44400.4062,  1711.3367,\n",
      "         8697.2246, 12233.8281,  6750.3789,  6652.5288, 22493.6602,  2457.5020,\n",
      "         2198.1899, 22198.8750, 10976.2461, 10491.6934,  4667.6074, 48885.2422,\n",
      "        10806.4873,  9193.8389, 10537.9121,  8953.5049, 41097.1602,  6270.9673,\n",
      "         8047.2007,  9036.5869, 11117.1660, 10797.3359,  8988.1592,  6738.0117,\n",
      "        13073.8184, 12904.9053,  2750.1782,  5495.3623, 12635.6611, 14923.2422,\n",
      "         5525.4204,  7148.5503,  7634.4023, 46544.0820, 23945.7148, 14004.6660,\n",
      "        10959.6943,  3761.2920,  1674.6323,  6123.9985, 10621.7578, 12476.7812,\n",
      "         3526.3516, 19964.7461, 17663.1445,  6585.8774, 38169.4375, 14289.9834,\n",
      "         8583.9736, 54108.4180,  4892.4019,  9391.3457])\n",
      "tensor([15003.8145, 11922.0859,  3310.4224, 46718.1641,  1972.9500,  4883.8662,\n",
      "         2353.8313, 17560.3789,  1712.2271,  2527.8186,  3535.1780, 28039.5391,\n",
      "         5390.6162,  2207.6975,  9636.7324, 12557.6055,  4415.1587, 10594.5020,\n",
      "        19444.2656,  6677.5752,  4830.6299, 14043.4766,  8653.0039,  1621.8827,\n",
      "         6990.8960,  2150.4690,  4472.8804,  2261.5688,  8549.4805, 11837.1602,\n",
      "         5406.6313,  6403.4287,  5620.7578, 14051.1152,  7265.7026,  5379.2900,\n",
      "         8186.8477, 27218.4375,  1694.7964, 22218.1152,  7443.8477,  4801.4756,\n",
      "        26823.7402,  8766.9248,  9313.5957, 13457.9609,  3956.0715, 13936.8818,\n",
      "        47437.5430,  6658.3477, 16753.3984, 36397.5742,  6186.1270,  9820.2334,\n",
      "        17496.3066,  4274.2217, 18817.6211,  8712.5498,  9288.0264, 11272.3311,\n",
      "         4149.7358, 18415.4375,  6364.9355,  8606.1201])\n",
      "tensor([20277.8066,  4320.4106, 11228.0234,  7609.9360, 26993.5117, 18823.5957,\n",
      "        16796.4121,  5116.5005, 19129.8770,  5062.8535, 10986.4014, 12222.8984,\n",
      "        10782.4883, 19726.1484,  4921.7368, 12648.7031, 11314.8672, 23514.5645,\n",
      "         6071.1353,  4756.5337, 11661.3965,  2639.0430,  3757.8447,  9523.9785,\n",
      "        12644.5889,  7724.7695, 10715.7383, 14571.8906, 10334.8125, 10991.8525,\n",
      "        11372.7080, 14453.7627, 10313.1445, 42856.8398, 10796.3506, 10090.1299,\n",
      "        19431.9727,  9365.2812,  8124.4082, 38392.7461, 21491.9805, 13844.5059,\n",
      "         8699.7979,  5271.1968, 26926.5137, 13555.0049,  9597.2881, 21828.0293,\n",
      "        15181.1689,  8601.3291,  3281.9583, 14154.8193,  5836.5205,  9163.1084,\n",
      "        24058.4453,  7161.5146, 11353.2275, 38092.4102,  3393.5928, 11163.5684,\n",
      "         9813.9365,  4917.9258,  4611.5186,  8587.1279])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8678.6367,  4669.0410,  2407.0042, 12890.0576, 42705.1836,  4889.9995,\n",
      "        11339.5977, 18223.4512, 35491.6406, 13770.0977,  1708.0013,  9086.5273,\n",
      "        18232.0332, 18955.2207,  3484.3311,  6748.7524, 14283.4590, 17593.3750,\n",
      "         7688.3145, 20167.3359, 29762.3105, 12323.9355,  9210.7949, 36036.2812,\n",
      "         5451.4233,  8989.9814, 10882.7529, 13342.3301,  7372.1353,  6684.9663,\n",
      "        26125.6738,  3736.4646,  4752.7188,  8302.5361, 10952.2207, 13041.9209,\n",
      "         8688.8584, 10446.0498, 19434.5449,  7441.0532,  9074.2441,  5702.2310,\n",
      "        13675.2305,  5871.2061,  9450.8184, 13204.2861, 12265.8857,  3369.4167,\n",
      "        45841.4688,  5398.8911,  3522.6023, 12142.8438, 12629.8965,  9095.0684,\n",
      "        23306.5469,  7153.5537, 24499.0840, 17578.2148, 46217.6641, 14799.0312,\n",
      "         2721.3208,  4853.6582, 11247.4453,  6628.2686])\n",
      "tensor([ 5889.1040,  6788.5342, 12401.7461, 10376.2949,  6658.2373,  7705.1616,\n",
      "         9141.2588,  6674.1318, 15019.7598, 26875.1562,  5227.2632,  9655.7354,\n",
      "         8734.1172,  9143.5732,  1632.5645,  9755.0039,  3404.4211, 10092.3564,\n",
      "         8107.7363, 10264.4424,  5312.5171,  7121.3081,  4869.0752,  3021.8091,\n",
      "        38310.2305,  4360.3711, 10451.9365, 38258.9219, 40941.2852,  8624.3711,\n",
      "         5214.5659, 16710.0039,  2523.1694,  2200.8308, 23807.2402,  5050.9492,\n",
      "         4708.5298,  4745.0073,  9931.6240, 37484.4492,  6286.6333, 16776.3047,\n",
      "         8116.2690,  9921.9004, 47403.8789, 27000.9844, 11176.2168, 18328.2383,\n",
      "        39722.7461,  5312.1699,  5757.4136,  6258.0811, 11038.9854, 14001.1338,\n",
      "        17844.1328, 11070.5352,  3238.4358,  9748.9102, 10929.7871,  8217.4004,\n",
      "        37465.3438, 14744.0938, 12030.7793,  5221.3887])\n",
      "tensor([11537.5742,  4297.9951, 43223.4648, 42661.0117, 34593.8672, 10015.4395,\n",
      "         8583.3223,  6716.7861,  4788.8638,  6417.5674, 40690.5430, 11891.9922,\n",
      "         6655.1758,  1748.7740, 10825.2539, 10751.9248, 10726.4121, 23552.6621,\n",
      "        14109.8145, 37607.5273,  9724.5303,  4343.5864, 46199.6367, 46889.2617,\n",
      "         3353.4702,  5465.4326, 19593.2031, 26730.3984,  5934.3799,  8070.9092,\n",
      "        10594.2256,  5412.7876, 28287.8984, 10584.0400, 10493.9453,  4791.5850,\n",
      "         4863.7627, 18866.6875,  6018.9741,  3443.0640, 10897.6035,  6753.0000,\n",
      "         2210.1880,  7731.8579,  7162.0122, 36837.4688, 20334.6172, 12797.2100,\n",
      "         2221.5645, 16450.8945, 15817.9854, 11193.2080, 19480.2383,  9630.3975,\n",
      "         4934.0996,  6203.9019, 17101.9160,  6014.0322,  6416.2925, 48675.5195,\n",
      "        17500.2051,  9104.3770,  6457.3037, 11540.8369])\n",
      "tensor([19350.3691,  5012.4712,  7196.8672,  9957.4824, 13140.4922, 12757.7354,\n",
      "         6940.9097,  6971.5854, 12656.7852, 28476.7344,  5548.1787, 12835.6465,\n",
      "         6718.9780, 42983.4570,  5377.4580,  5966.8872, 30284.6426, 17942.1055,\n",
      "         7400.1914, 11366.3506,  6726.0503, 38711.0000, 12032.3262,  5898.1665,\n",
      "        25382.2969, 13648.6016, 25147.0195,  9850.4316,  3554.2029,  9485.2559,\n",
      "         6755.6372, 38349.7500, 40861.9336,  9411.1689,  7441.2729,  3591.6248,\n",
      "         5272.6763, 47412.0312,  9610.9561,  9979.2393, 11200.0918, 24671.6641,\n",
      "         1839.4099, 25081.7676,  7077.1895,  8592.1816, 10761.4844,  8564.4863,\n",
      "        18650.8203, 15731.3271, 11073.1758, 14256.1924,  4307.1519, 43753.3359,\n",
      "         5983.9121,  2483.7361,  5824.1021,  9174.1357,  6211.9331,  8083.9199,\n",
      "         6688.9746, 12838.9932,  5809.5537,  9584.8838])\n",
      "tensor([16417.6914,  1981.5819,  6330.9521, 19085.0898,  6350.9326, 12609.8867,\n",
      "         8218.7500, 11170.6738,  6756.5708, 21308.4199,  2331.5190,  4986.4912,\n",
      "         5238.0103, 17434.5801,  7421.1943,  5599.7192, 37133.8984, 10278.5635,\n",
      "        12112.9785, 19892.5918,  6475.1812,  5243.5674,  4500.3394, 19069.4746,\n",
      "        10274.5830,  6761.2153, 24476.4785,  4428.8877, 19594.8105,  5282.9453,\n",
      "         5150.5874,  4738.2681,  5455.4463,  4853.2388, 17626.2402,  5602.9517,\n",
      "        16657.7168, 45863.2031, 29523.1660,  6356.2705,  5151.9502, 33900.6523,\n",
      "        11226.9932, 11830.6074,  5003.3179,  7512.2671, 27037.9141,  2211.1309,\n",
      "         1727.7850, 15439.3281,  3850.3679, 22478.5996,  8551.3467,  4793.2402,\n",
      "         9872.7012,  6184.2993, 20630.2832, 46475.2383, 43287.3750, 13725.4717,\n",
      "         8732.0859,  8792.4785,  9254.5244,  8662.5176])\n",
      "tensor([ 3766.8838,  7804.1606,  3972.9248,  9516.3633, 27117.9941,  6777.4302,\n",
      "        20149.3223,  6548.1948, 41279.2422, 14768.0205, 11568.4766, 11300.4541,\n",
      "        12265.5068,  5527.3135, 12387.4072, 34472.8398,  6664.6860, 45702.0234,\n",
      "         9393.3584, 22412.6484,  9948.9697, 10720.6953,  4708.3374,  4522.0054,\n",
      "        10736.8711, 26489.5762,  6093.4038,  5822.7246,  9964.0596,  5415.6611,\n",
      "         4433.7495,  8769.6445, 32905.8164, 15238.4053,  8965.7959,  5185.2397,\n",
      "        10065.4131,  6763.1831, 24919.3574,  4846.9849,  8517.6445, 12129.6143,\n",
      "        10807.3350, 11664.4541,  6604.6367, 12731.0000,  6657.5737, 22060.1719,\n",
      "         7085.8442,  6832.8516,  6677.3218,  7671.1797, 36023.1484, 46661.4414,\n",
      "         9642.2871,  7810.5654,  5266.3657,  4929.9517,  8559.9883, 30269.4277,\n",
      "        10390.9131, 11504.0811,  3398.1965,  9873.8760])\n",
      "tensor([ 1242.8160, 27626.1797, 18608.2617,  9950.3486,  3980.1387, 11854.6738,\n",
      "         3704.3545, 13670.9150, 13393.7559, 22331.5664, 10693.0615, 36225.4023,\n",
      "        10600.5479,  6710.1919,  2395.1716, 19524.6582, 46145.6523, 17748.5059,\n",
      "         2404.7339,  6663.0903, 15746.4619, 28058.4707, 46654.7266, 21659.9297,\n",
      "         6543.0454, 13919.8232,  8823.9854,  7985.8149, 41919.0977, 13994.2930,\n",
      "         8491.9072,  3176.8159, 27941.2871, 23244.7910, 13065.5439,  7156.5244,\n",
      "        39532.2461,  9414.9199, 33907.5469,  7206.2949, 14133.0381,  2045.6853,\n",
      "         6389.8535, 11950.1514, 10106.1338,  4895.6797, 41661.6016,  4318.3760,\n",
      "         7034.8438,  3556.9224, 17174.6816, 10823.2266,  6211.3403, 14249.3955,\n",
      "        11674.8730,  6105.5820, 18310.7422,  4292.8584,  8277.5234,  4743.8374,\n",
      "         6112.3530,  6149.6147, 10751.5498,  9047.6494])\n",
      "tensor([21925.6836,  4357.0435,  8595.2559, 11884.0488, 12874.1396, 55135.4023,\n",
      "         3393.3564, 10746.0781,  3855.9429,  5859.6704, 10240.5527,  5929.4653,\n",
      "        24915.0469,  7222.7861,  4772.8760,  6775.9609, 46988.3516, 12950.0713,\n",
      "        16085.1279,  4698.2017,  4340.4409,  2741.9480,  1880.4871,  4934.7051,\n",
      "         7045.4990, 14049.8330,  3044.2134, 10563.2197, 23582.4688,  5253.5239,\n",
      "        37829.7227,  1964.7800,  5926.8462, 12491.8818, 44202.6523,  8553.9541,\n",
      "         8712.3389, 44061.4805,  6322.3701,  5008.2500, 16586.4980,  8827.5264,\n",
      "         4662.8926, 13228.8467,  9095.5312, 11396.9004,  4285.9956, 25154.1719,\n",
      "        39385.6328,  6198.7520, 10787.6904, 10832.7705, 11130.3535, 13557.7188,\n",
      "         3947.4131,  1526.3120, 33353.5000, 39836.5195,  5210.1001, 11111.9248,\n",
      "        19214.7051,  5257.9595, 23563.0156, 40932.4297])\n",
      "tensor([42284.9180, 28868.6641,  2026.9741, 18989.3652, 21806.1816, 11355.8174,\n",
      "        11185.9141, 39727.6133,  4894.7534,  4779.6025,  6216.3872,  8844.6348,\n",
      "        16776.3047,  5425.0234, 13846.9619, 17593.3750,  2689.4954, 10976.2461,\n",
      "         9884.6934, 41949.2422,  9138.3857,  8807.7295, 16796.4121,  8559.9883,\n",
      "        39586.8633,  3526.3516,  8716.2393, 12905.0811,  6664.6860, 19071.2871,\n",
      "        10431.9648,  6775.9609,  9948.9697, 41180.8711, 13393.7559,  8598.2676,\n",
      "         5327.4004,  4833.8750, 20277.8066,  1137.0110, 22218.1152, 13145.5684,\n",
      "        10250.4209,  6697.6084,  5257.5078, 48173.3594,  7625.1558, 11837.1602,\n",
      "         3234.1348, 11833.7822,  5427.3716,  2585.8506, 12838.9932, 11018.9854,\n",
      "        18817.6211, 36189.1016,  6674.1318,  8792.4785, 20630.2832,  4709.6787,\n",
      "         9046.3711,  6272.4771, 15554.3066, 11068.2744])\n",
      "tensor([43753.3359,  2480.9790,  6684.9663,  4909.7246, 12533.9229,  8709.8965,\n",
      "         5000.0771,  6728.2705, 12491.4844,  3268.8467, 28868.6641,  8591.2148,\n",
      "         9182.0586, 12235.8389,  6112.3530, 11048.3555,  6322.3701,  4297.9951,\n",
      "         8116.2690,  4234.9268,  2352.9685,  6648.9106, 15439.3281, 17844.1328,\n",
      "         4752.7188, 18232.0332,  4350.5142,  4449.2339,  5654.8184,  2362.2290,\n",
      "         5347.5425, 18804.7520,  5257.9595,  5050.9492, 10824.4844,  8588.3887,\n",
      "        10991.8525,  2731.8477, 16739.4160,  6837.3687, 17740.2930,  8595.2559,\n",
      "         4766.5093,  4267.7329,  8816.9209, 11353.2275,  4747.0527, 14410.9316,\n",
      "        25729.1855,  6961.4736, 40941.2852, 14064.4277,  4058.7124, 39109.7539,\n",
      "         2680.9492,  9946.9463, 21677.2832,  5345.6807, 13508.6172,  7633.7207,\n",
      "         4294.2544, 19539.2422,  2045.6853,  4766.0220])\n",
      "tensor([ 6196.4482,  3005.1008,  5605.0166,  9878.7461,  9462.1533,  4784.8198,\n",
      "         4266.1660,  9783.7812, 17352.6797, 62592.8750,  6009.9824, 34166.2734,\n",
      "         9172.7686,  6616.0239, 10090.1299, 17716.6367, 34393.4492, 29523.1660,\n",
      "         2775.1921,  8347.1641,  5240.7651,  7050.0215,  6571.0244, 12638.1953,\n",
      "        11013.7119,  9222.4023, 10373.2334, 11460.4990,  3847.6741, 17009.3359,\n",
      "        14249.3955,  5288.9102, 16455.7070,  9360.9238,  4977.2495, 15169.1836,\n",
      "        38239.4102,  4360.3711,  6764.7871,  6117.4946, 12485.3594, 11555.6875,\n",
      "        44246.3320,  9962.6123,  6093.4038,  6415.2651, 17434.5801, 20099.7363,\n",
      "         9748.9102,  7201.7007,  2302.3000, 12360.8281,  4827.9048, 18091.3730,\n",
      "         5093.8828, 24956.6738,  5415.6611,  5857.2939, 42983.4570, 10787.6904,\n",
      "        18823.5957,  7475.3062,  3985.0098, 12493.1104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6680.3022, 11335.8467, 13352.4766, 19663.3809, 10118.4238, 14235.0723,\n",
      "        18444.5488,  8553.9541, 10148.4678,  6361.5400,  6738.9458, 37747.6836,\n",
      "        12491.8818, 46151.1250,  4040.5583,  5400.9805, 18303.1406,  5080.0962,\n",
      "        14224.7549,  8938.4619, 11168.2471, 11170.6738,  9321.8613, 10942.1318,\n",
      "        11228.0234, 39611.7578, 12644.5889, 18317.6250,  8732.0859, 13483.6182,\n",
      "         7441.5010,  4805.6055, 13770.0977,  8798.5928, 18629.5156, 25333.3320,\n",
      "        13143.8652,  4884.5830,  9875.6807, 19533.8320,  3398.1965,  8825.0859,\n",
      "        10959.3301,  2967.0117, 10019.4902, 18861.9668,  9716.7441,  4433.2295,\n",
      "        19530.0527,  4133.6416, 30166.6191,  5253.5239,  9046.0400, 11316.2148,\n",
      "         6088.7759,  5927.6006, 18310.7422, 23162.3965, 14692.6689,  9174.1357,\n",
      "        10806.8389, 11386.5137, 10159.4766, 11615.2969])\n",
      "tensor([11100.1875, 40242.7969,  7730.8525, 11891.9922,  4435.0942,  9861.0254,\n",
      "         3227.1211,  8116.6802,  8162.7163,  3404.4211,  9290.3174,  8797.9082,\n",
      "        13844.7969,  9236.1387,  6360.9937, 12254.8135, 39245.1641,  6705.0220,\n",
      "        11554.2236, 23757.2539,  6999.2749, 18157.8770,  7742.2729, 38310.2305,\n",
      "        15413.5967,  5702.2310, 10812.6650,  2102.2646,  6356.2705,  5466.0967,\n",
      "        13140.4922, 29010.3184,  1137.4697,  2026.9741,  4888.4927,  7935.2910,\n",
      "         4832.9170,  5271.1968, 13880.9492,  7098.9106,  4809.7949,  3385.3992,\n",
      "         4243.5898,  9453.6953,  3471.4097,  7147.4727, 19107.7793, 13073.8184,\n",
      "         8869.2197,  6338.0757, 16297.8457,  9921.9004, 45863.2031,  3293.2917,\n",
      "         6088.1855, 10711.7891,  3261.9417,  4687.7969,  6933.2422, 12635.6611,\n",
      "         6128.7974,  8107.7363,  4510.5015,  9634.5381])\n",
      "tensor([ 1149.3959, 44213.4219,  4461.8037,  7345.0840, 44641.1992,  1837.2371,\n",
      "        17174.6816, 10422.9170,  6403.4287, 24321.3496,  5191.5757, 10658.5049,\n",
      "        10232.0596,  4921.8262,  6079.6714, 37464.0859, 23258.9980,  1880.4871,\n",
      "         7168.9038,  5214.5659,  8877.5127,  2137.6536,  4345.8423, 22494.5723,\n",
      "         8186.8477,  8520.2656, 14004.3096,  4518.2300, 48673.5586,  2899.4893,\n",
      "         4340.0557,  8902.8799, 19496.7188,  6406.4106,  9370.4639, 40920.5703,\n",
      "         5282.9453,  4826.3560, 11961.2793,  7209.4917, 10370.9121, 15230.3242,\n",
      "         1252.4070,  5028.1465, 16710.0039,  2198.1899, 38392.7461,  4441.2134,\n",
      "        11922.0859,  9095.0684,  8486.9014, 44397.3906, 10621.7578,  4846.9199,\n",
      "         6255.7744, 17748.5059,  6101.8604,  4032.2407, 14254.6084, 16862.8984,\n",
      "         8569.8613,  7620.8408,  1146.7966, 21880.8203])\n",
      "tensor([10796.3506, 11388.3428, 42124.5156, 11661.3965,  9898.4219, 16420.6777,\n",
      "         8410.0469,  7726.8540, 21223.6758, 10825.2539,  6149.6147,  5748.8198,\n",
      "         7281.5054, 14833.3945, 41501.6562, 11197.0967,  7046.7222,  9365.2812,\n",
      "        19023.2598, 10823.2266, 35907.0117,  1534.3044,  6849.0259, 40861.9336,\n",
      "        42969.8516, 32548.3398,  5452.0752,  8563.3232,  6014.0322, 23033.8848,\n",
      "         9874.5771, 17765.9785, 33907.5469, 18306.3945, 11048.9639,  6046.6641,\n",
      "        12265.5068, 47400.9414,  6551.7500, 12592.5342, 42760.5039, 47412.0312,\n",
      "         6426.7280,  7147.1050, 11276.6777, 14988.4316,  5028.1250,  9755.0039,\n",
      "         6405.0771, 41920.0781,  4158.2539, 10792.8076,  9301.8936,  5729.0054,\n",
      "         9166.4688,  8731.4805,  9432.9258,  5150.3169, 13256.1748,  5972.3779,\n",
      "         5301.3931,  6628.2686,  5811.5186, 35069.3750])\n",
      "tensor([ 1391.5287, 16420.4941,  4721.4297,  1632.0363,  4317.3154,  3375.3062,\n",
      "         9137.7803,  6986.6968,  9490.4531, 11218.9795,  9877.6074, 33475.8164,\n",
      "         5934.3799,  5312.1699,  9304.7021,  9235.0645,  4662.8926, 18972.4941,\n",
      "         9630.3975, 18061.0137, 47403.8789, 11512.4053,  1632.5645,  2264.7219,\n",
      "         4040.4421, 14349.8906, 10763.5752,  4669.0410,  8334.5898,  6746.7427,\n",
      "         2407.0042, 25154.1719, 28403.3711,  2155.6814, 10584.0400,  5241.3555,\n",
      "         6099.9766, 20773.6270,  8026.6665, 30063.5801,  4452.5493,  6948.7007,\n",
      "         9270.5479,  3502.8157, 11268.0127, 11155.0889,  4869.0752,  9535.4424,\n",
      "        19964.7461, 11443.9365, 17929.3027, 46182.8320,  2527.8186,  7729.6455,\n",
      "         9595.4395, 39802.3516,  3947.4131,  4559.5342,  4590.5381,  2020.1770,\n",
      "        41279.2422, 41332.5312, 14643.6221,  5548.1787])\n",
      "tensor([17914.1328,  6712.0068,  5482.2915, 19453.6855,  5920.1040, 23300.7910,\n",
      "         1635.7336, 12479.7090,  6061.5435,  4889.0366, 12646.2070,  4930.2988,\n",
      "         2457.2112, 44501.3984,  8794.9893,  5851.0708,  6770.1924,  4912.5459,\n",
      "         3484.3311,  3875.7341, 28101.3340, 19480.2383,  6891.6792,  9694.4453,\n",
      "         7378.3042, 23632.3750,  5397.6167,  6358.1851,  8891.1396, 20462.9980,\n",
      "        37484.4492, 23261.0215, 10436.0957,  5379.2900, 21774.3223, 16232.8467,\n",
      "         8549.4805, 11674.1299,  7235.6587, 13041.9209, 47273.1094,  6527.7969,\n",
      "         9850.4316, 12649.6045, 11030.4414, 40160.7891, 21982.4746, 11801.3779,\n",
      "         7730.8125, 18765.8750,  6425.8906,  4343.5864,  9553.4902,  6579.8521,\n",
      "         3980.1387, 39722.7461, 17270.3926, 12478.4727, 41999.5195, 12233.8281,\n",
      "        38511.6289, 19305.1934,  5822.7246, 17879.6328])\n",
      "tensor([ 5211.0073, 39871.7031,  9387.7686,  6182.4146,  7731.8579,  8886.0176,\n",
      "         7085.8442,  6186.1270, 14829.3516, 13887.9688, 10950.0391,  5377.0161,\n",
      "         5210.3770, 27107.3418, 13405.3906, 14133.0381,  5515.8096,  6971.5854,\n",
      "        23945.7148,  3213.6221, 11184.4941,  8936.3086, 39000.4609,  2156.7517,\n",
      "         4791.5850,  4559.3188, 11033.6621,  7727.2534, 12129.6143,  1631.8212,\n",
      "         4661.2861,  4012.1570,  5390.6113, 10715.7383,  6765.6182, 29964.4688,\n",
      "         8424.0781, 11015.1748,  4853.6143, 12815.4453,  1629.8335,  3479.0305,\n",
      "         4508.5103,  5620.7578, 12645.1660, 10920.3223,  8653.0039,  9669.7842,\n",
      "        10348.4600,  5003.8530,  2721.3208,  8218.7500, 36910.6094, 37270.1523,\n",
      "         5207.8936,  5966.8872,  5167.4790,  7448.4038, 40103.8906, 18137.9727,\n",
      "         5540.3325, 37829.7227,  7196.8672,  8920.6895])\n",
      "tensor([ 7337.7480, 10991.3584, 17942.1055, 11312.4746, 13224.6934, 12574.0488,\n",
      "         8023.1353,  8609.9229,  5560.0654, 10336.7979, 39983.4258,  9193.8389,\n",
      "        10008.9678, 46412.5352,  8342.9092,  9583.8936, 27941.2871,  6108.3462,\n",
      "        19121.4629,  5014.1821,  8027.9678, 10491.6934, 12146.9707, 27709.7969,\n",
      "         5550.9214,  3615.3975,  9724.5303, 21925.6836,  9283.5615,  2396.0959,\n",
      "         2639.0430,  6673.0005, 11093.9844, 19361.9980,  4260.7441,  6777.4302,\n",
      "         6653.7886,  9610.9561, 21806.1816,  2755.0210, 10381.4785, 10987.3252,\n",
      "        11735.8447, 10602.3848,  8976.1406,  7027.6987])\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "tensor([ 3279.8687, 21454.4941,  1720.3538,  6801.4375, 11946.6260,  7742.1099,\n",
      "        21736.3281,  4916.9531,  5515.8096, 17009.3359, 38433.5234,  8549.1367,\n",
      "        16099.3672,  7492.9854, 18091.3730, 41501.6562,  2661.1912, 42211.1367,\n",
      "        16455.7070,  4433.9160,  6571.5439, 26236.5801,  6318.7979,  5795.9058,\n",
      "        47305.3047, 11488.3174, 29101.7520, 19305.1934, 12479.5273,  7730.7632,\n",
      "        10118.0635,  9579.1553, 38126.2461,  7168.9038,  5482.2915, 47462.8945,\n",
      "         9051.9004,  5383.5361, 17352.4258, 10480.8955, 11945.1328,  8083.1782,\n",
      "         8347.1641, 40018.2305,  6686.4312,  2219.4451,  6662.1387,  7076.8926,\n",
      "         4790.3115,  9434.7314,  7102.4077, 10043.2490, 27533.9121,  5240.7651,\n",
      "         3533.2556,  4673.3921, 48173.3594,  4179.4453,  6133.8828,  9716.7441,\n",
      "         4461.8037,  5148.5527, 13974.4551,  7050.0215])\n",
      "loss: 9133714571264.000000  [   64/ 3630]\n",
      "tensor([18061.0137,  4877.9810,  7726.8540, 19121.4629,  8519.2305,  1769.5316,\n",
      "        18629.5156, 36124.5742, 10601.4121,  5824.7817,  9094.7920, 11743.2988,\n",
      "        25737.9805,  5014.1821,  4914.3413,  3777.2117, 15526.5752,  5241.3555,\n",
      "         4833.8750, 33131.8438,  6797.2192, 18157.8770,  6025.5762,  1121.8739,\n",
      "        10658.5049, 21982.4746, 12485.3594,  6659.9111,  5263.4888, 11611.7080,\n",
      "         7079.3530, 12265.9785,  7147.4727,  8415.5195,  8877.5127,  8303.0703,\n",
      "         2257.4753, 11741.7256,  5093.7124,  7978.5215, 10143.2617,  7672.9897,\n",
      "         5523.9819,  8594.0195,  9254.6621, 12818.6055,  9378.4590, 13099.8857,\n",
      "        19120.2246,  2130.6758, 10019.4902,  4266.1660,  7168.0327,  3500.6123,\n",
      "         2136.8823, 14319.0312,  4921.8262,  6123.5688,  8649.2002, 11107.3457,\n",
      "        14829.3516, 11273.8643, 15413.5967,  7144.8628])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6505.3262, 19539.2422,  7325.0483,  7625.1558,  7257.4282, 12154.0332,\n",
      "         9654.1816,  4721.4297, 24890.1055,  1242.2600,  6425.8906,  6837.3687,\n",
      "         3981.9768,  3261.9417,  3471.4097, 19043.1621,  1646.4297, 17693.6172,\n",
      "         1621.3402, 10607.0059, 38792.6875,  9440.7178,  9387.7686, 39783.3828,\n",
      "        23503.1953, 13352.0996,  6309.6631, 46599.1094,  2710.7117, 21190.7207,\n",
      "        22028.7109, 13744.0557,  9360.9238,  4189.1133,  4795.6567,  8534.6719,\n",
      "        45826.9258, 22395.7441,  7027.6987,  2416.9551, 10799.3418, 14984.9824,\n",
      "        10776.0654, 28205.9336, 10226.2842,  4894.7534, 11253.4209, 10422.9170,\n",
      "         8116.6802, 15170.0693,  9632.6973,  4818.7358,  7263.6362,  9890.5205,\n",
      "        12254.8135, 12927.1318, 11370.8428, 13635.6377, 11307.3711,  1737.3760,\n",
      "        13125.3525, 35064.6719,  9091.8574,  3591.4800])\n",
      "tensor([13143.3428,  6009.9824,  3293.2917, 21348.7051, 44213.4219, 16150.7910,\n",
      "         8199.8369, 11881.3584, 26506.6816,  4022.5654, 21223.6758,  5621.9448,\n",
      "         6500.2358,  4977.2495,  5334.7085,  7209.4917, 10107.2207,  5560.0654,\n",
      "         9962.6123,  8280.6230,  9137.7803,  9861.0254,  7371.7720,  4912.5459,\n",
      "         4431.6924,  5699.8374, 10104.5605, 11754.4150, 17448.0000,  9046.0400,\n",
      "        23445.8926,  2867.1196, 16884.9238,  5427.3716, 36808.3750,  3070.8086,\n",
      "         4737.9204,  4889.0366, 13596.5107,  2727.3950, 11439.6240, 13915.5215,\n",
      "         8059.4062,  8835.2646,  9235.0645,  9462.1533, 11335.2959,  3410.3240,\n",
      "        27322.7344,  7987.8325,  7537.1641, 10278.1416,  3866.8552, 38321.4492,\n",
      "        15973.6201,  9878.7461,  8068.1851, 10741.0586,  5080.0962,  4350.5142,\n",
      "         8585.8467, 39774.2773, 12026.0957,  8794.9893])\n",
      "tensor([ 4533.9902,  4835.7505, 12877.6006, 28403.3711,  2473.3340, 17178.6816,\n",
      "         7765.3745,  7631.4805,  5851.0708,  3176.2876, 14478.3301, 16114.3955,\n",
      "        13844.7969, 14002.8252,  6750.4321,  4670.9858, 24393.6230, 13145.5684,\n",
      "         6412.4795,  5318.4067,  5466.6616, 14374.9912, 46200.9844,  9432.9258,\n",
      "         7583.2061,  5033.0952,  4559.5342,  5138.2568,  1627.2825,  6361.5400,\n",
      "         2974.1260,  5484.5405,  1967.0227,  9670.2822, 11674.1299,  4337.7354,\n",
      "        24321.3496, 11015.1846, 20462.9980,  6747.9126, 14007.2217,  8931.1758,\n",
      "        11100.1875, 37079.3711, 11018.9854,  2196.4731,  6579.8521, 30166.6191,\n",
      "        11048.9639,  6660.8281,  5727.1279,  6551.7500,  6414.1782, 16176.0850,\n",
      "        48549.1797,  4685.1455, 44627.9375, 30364.5371,  6143.1284,  4515.1943,\n",
      "        44501.3984,  4265.1128, 17879.6328, 10711.7891])\n",
      "tensor([11842.4424, 28101.3340,  7148.2329,  9520.0020, 10801.6104,  9272.5332,\n",
      "        11906.1250, 38709.1758,  4040.5583,  7624.6299, 11291.3857, 12244.5312,\n",
      "         6496.8862, 10977.2061, 34617.8398, 13019.5244, 10461.9795, 14436.8926,\n",
      "        17765.9785, 24915.2207, 18638.6133, 12846.3525, 18765.8750,  4928.3643,\n",
      "         7160.3081,  7060.2222, 21002.7578,  6358.1851,  4990.2998, 16420.4941,\n",
      "        19120.6562, 10942.1318,  2720.7209,  1515.3448,  9634.1523,  6923.6221,\n",
      "         5632.9487, 17108.6309, 17315.6914,  6951.1992,  6941.6724, 40419.0195,\n",
      "         8233.0977, 33732.6875, 12002.3662, 13937.6660,  4852.9478, 11261.8174,\n",
      "        10577.0869,  3756.6216, 38822.0898, 27082.6641,  8252.2842,  6020.6094,\n",
      "        13616.3584, 12269.6885,  5605.0166,  8342.9092, 10920.3223, 11497.8213,\n",
      "        20641.5938,  9198.3574, 36632.5430,  4561.1885])\n",
      "tensor([ 7789.6348, 19435.9277,  4501.2344,  5452.0752, 11085.5869,  9239.9736,\n",
      "         7152.6714, 18317.6250,  5354.0747, 46220.9961, 25421.9336,  9916.5957,\n",
      "        22606.0645, 11931.1250, 11280.1006,  6415.2651,  6680.3022,  6061.5435,\n",
      "         3925.7583,  5257.5078, 14113.4727, 11044.6406, 20411.7734,  9553.4902,\n",
      "         2155.6814, 14119.6201,  2154.3611, 12495.2910, 11643.1836, 42303.6914,\n",
      "        15412.6855,  4433.2295, 14426.0742,  6358.7764, 21880.8203, 18099.5254,\n",
      "        10967.5176,  3385.3992,  6437.9717,  4909.7246, 28923.1367,  4719.7363,\n",
      "         9880.0684,  5375.0381,  7371.1211,  6325.6953,  9875.6807, 19798.0547,\n",
      "         1986.9333,  5940.5850, 11365.9521,  5186.6426,  1639.5631, 24948.2012,\n",
      "         2719.2798,  8728.9082,  7552.9785, 15456.2949,  7442.3052,  3597.5959,\n",
      "         4326.2134, 13143.3369,  9143.6279, 17270.3926])\n",
      "tensor([ 8569.8613, 14003.7881,  6353.1885,  9285.9922,  3500.7427, 12533.9229,\n",
      "         9304.7021, 11318.2275,  9301.8936,  8516.8291,  9290.3174,  2102.2646,\n",
      "         6059.1729,  3591.4707,  8920.6895,  4527.1831,  4783.2427,  5267.8184,\n",
      "         8704.1816,  5572.8662, 13430.2646,  3292.5298, 17716.7422,  8603.8232,\n",
      "         6743.1680, 35956.5898, 11316.2148, 47055.5312, 35069.3750, 10550.3281,\n",
      "         4827.9048, 29010.3184, 12347.1719,  1815.8759, 11881.6777,  4053.6936,\n",
      "         3832.0100,  8520.2656,  6393.6035, 26018.9512, 17929.3027,  9138.3857,\n",
      "         6970.0791, 10769.5791,  4740.7705,  9964.0566, 41676.0820,  6330.5752,\n",
      "        11300.3760,  7740.3369,  5631.7529,  4463.2051,  6764.7871,  4200.9316,\n",
      "         9172.7686, 36085.2188, 44445.3477,  6377.8208,  9456.3994, 17352.6797,\n",
      "        11010.6445, 10289.4707, 46182.8320, 16692.1406])\n",
      "tensor([ 4832.6074,  7650.7739, 46151.1250,  6673.0005, 10791.9600,  4134.0825,\n",
      "         5540.3325, 14539.2803, 38800.3867, 10086.8330, 43813.8672,  7331.8779,\n",
      "         8623.3125, 11371.9863,  8930.9346, 10959.3301, 12094.7510, 11455.2803,\n",
      "        48824.4492, 14235.0723, 18861.9668,  1136.3994,  4523.3594,  6088.1855,\n",
      "         1704.7002, 11881.9697, 18903.4922, 42112.2344, 11454.0215,  6891.6792,\n",
      "        30438.9199,  2913.5691,  6164.6860,  5976.8311,  4891.4346, 10652.5762,\n",
      "         4910.7397,  7243.8135,  2566.4707, 19853.7910,  4873.8315,  6182.4146,\n",
      "         3878.7839,  7151.0918,  2801.9724,  9283.5615,  5510.3447,  7378.3042,\n",
      "        18451.5840,  6605.6602, 34672.1484,  5319.6094, 11187.6562, 11196.8926,\n",
      "        24667.4199,  4559.3188,  6586.8999,  1917.3184, 11179.6074,  4372.8345,\n",
      "         3544.2539, 14142.7627, 10381.4785, 20664.3613])\n",
      "tensor([ 6347.0557,  4837.5825, 20099.7363, 19663.3809,  4765.3315,  3352.8745,\n",
      "        11987.7871, 17069.8730,  6289.7549,  4296.2710,  4388.5278, 15022.6826,\n",
      "        36308.0508, 15820.6992, 11218.9795,  8752.8184,  5379.5894,  7640.3091,\n",
      "         3481.8679,  9018.6416, 11268.0127, 47928.0312,  6616.0239, 15408.4268,\n",
      "         3940.8850, 21082.1602,  8596.8281, 10508.9199,  5227.9888, 36197.6992,\n",
      "        14711.7344, 17742.1055,  3531.7224,  6406.4106, 14410.9316, 43871.3320,\n",
      "         5209.5786,  6948.7007, 21472.4785, 34779.6133, 21797.0000,  4719.5239,\n",
      "        10744.9287, 11270.3340, 23781.3574,  5245.2271,  4992.3765,  6572.0107,\n",
      "         6738.9458,  4119.3862,  5581.6836,  7441.5010, 12163.5576,  1635.7336,\n",
      "        15828.8213,  9446.0977, 25729.1855,  7235.6587, 34166.2734,  7890.6313,\n",
      "         8568.6943, 11013.7119,  4076.4971,  3895.4170])\n",
      "tensor([ 7448.4038, 16495.7285,  7049.5171,  1711.0269,  5347.5425, 14254.6084,\n",
      "        48381.3633,  3577.9990,  8563.3232,  3611.9717,  4702.1309, 15010.1758,\n",
      "        11285.7783,  5120.5830,  8815.0703, 13112.6045, 18306.3945, 13952.1602,\n",
      "         6705.0220, 10237.2578, 19493.5508, 21774.3223, 19811.0117, 16173.2686,\n",
      "        39611.7578,  7985.6104,  9409.5928, 11082.5771, 10923.9336,  4960.0352,\n",
      "        41332.5312, 28950.4688, 22462.0430,  7639.4175,  4915.0601, 10602.7920,\n",
      "         4731.0303, 37742.5742, 24180.9336,  1664.9996,  8605.3613, 21978.6777,\n",
      "         9500.5732, 41180.8711, 13717.1807,  7201.7007, 12491.4844, 14239.9189,\n",
      "        25656.5762,  6035.6963, 12574.0488,  7518.0254, 14474.6748, 11023.4434,\n",
      "         1241.5649, 51194.5586,  5790.1455,  5267.5312,  7730.8525,  5551.1509,\n",
      "         9863.4717, 22192.4375, 46130.5273,  2690.1138])\n",
      "tensor([11397.8027, 39725.5195, 23326.9160,  6402.2915,  9390.1387,  5762.3604,\n",
      "        10214.6357,  2534.3938,  5167.4790,  7046.7222, 36021.0117, 11350.5879,\n",
      "         2220.0222,  6697.6084,  6766.8198, 32787.4570,  7730.8125,  1534.3044,\n",
      "        10544.1963, 29964.4688, 12680.1914,  6713.5645, 27043.2266, 36133.6328,\n",
      "         8283.6807,  8595.7656, 47496.4961,  4508.3540, 19252.2188, 15183.2344,\n",
      "         6079.6714,  1704.5681, 24901.6484,  2850.6838, 18648.4219, 19777.6309,\n",
      "        39793.6562,  4544.2349,  6657.9819,  4462.7217, 10823.6777, 18137.9727,\n",
      "        12198.5254, 11986.8477, 34241.5391,  5426.0776,  6196.4482,  6765.6182,\n",
      "        10631.4541,  7475.3062,  9182.0586,  9549.5654, 18218.1621,  6673.6426,\n",
      "        12839.8486, 12479.7090, 24106.9121,  4317.3154, 11856.4111, 11032.7969,\n",
      "        10713.6436, 30942.1914,  5247.9961, 12913.9922])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4790.2144,  2782.4663,  4386.8716, 17740.2930, 20773.6270, 13470.8604,\n",
      "        21344.8477, 11212.6543, 14383.3535, 11305.9346, 10826.0000, 22153.5918,\n",
      "        40337.0664, 20009.6328, 12271.1777,  5466.0791, 12079.0439, 47291.0547,\n",
      "        23288.9277,  9298.7139,  7612.2227,  8798.5928, 20633.8691,  8396.6006,\n",
      "        11214.5322,  7210.7290, 17716.6367, 40920.5703, 13289.4209, 32248.7012,\n",
      "         4846.9199,  8027.9678,  3861.2097, 17758.2520, 20088.1133,  9361.3271,\n",
      "        43578.9375, 11072.3447,  6784.1021,  6250.4351, 10680.0986, 15359.1045,\n",
      "        14210.5361,  6849.0259,  4345.8423, 16790.6562, 19144.5762, 11058.1572,\n",
      "         4532.1323, 38168.0039,  4987.0674,  6877.9800,  6660.0488,  6758.4141,\n",
      "        25311.5215, 20177.6719, 11299.3428,  4151.0288, 25332.6934,  5884.6030,\n",
      "         9898.4219,  3882.6570,  3392.9768,  4364.6133])\n",
      "tensor([ 3268.8467, 24881.2305,  9884.6934, 11218.5703,  6785.8169,  5012.6846,\n",
      "         8891.1396,  8978.1855,  5891.5181,  5225.6348,  7729.6455, 12755.5361,\n",
      "         8824.9102,  8152.0234, 10129.2852, 11286.5391,  8718.8525, 44585.4570,\n",
      "         7517.5283, 16570.5996,  1163.4626,  8556.9072,  2480.9790, 18971.7168,\n",
      "         2775.1921, 21318.5898,  5472.4492, 11276.6777, 62592.8750, 11197.0967,\n",
      "        17012.1875,  5209.1787, 11280.4385,  3659.3459,  3227.1211,  8609.9229,\n",
      "         8699.4893, 16297.8457, 10579.7109, 11244.3770,  7193.5654,  9874.5771,\n",
      "         2241.7705, 48673.5586,  8527.5322,  8941.0898, 40273.6445,  5743.7627,\n",
      "         4433.5488,  7345.0840, 11500.3193, 12425.2158,  8743.7490,  8424.0781,\n",
      "         4529.8950,  4676.9346,  5128.0859,  1137.4697,  4508.5103,  4537.2729,\n",
      "        39556.4961, 12346.6543, 24873.3848,  5514.5098])\n",
      "tensor([41003.0234, 30322.2207,  6426.7280,  3906.1270,  9914.9834, 10450.5518,\n",
      "         8765.2490,  8807.7295,  8739.6240,  2104.1133,  1632.0363, 17468.9844,\n",
      "        11460.4990,  4646.7588,  4402.2329, 32351.1719, 21760.0762, 47400.9414,\n",
      "         4753.6367, 18264.6367,  1705.6245,  7316.6323, 10206.4766, 18524.0332,\n",
      "        12235.8389, 33307.5508,  1731.6770, 35585.5742, 12219.4395,  9034.2324,\n",
      "         2643.2686, 37747.6836, 19023.2598,  4900.3213,  7731.4272,  2264.7219,\n",
      "         7935.2910,  1146.7966,  8825.0859, 11657.7188,  9541.6953, 18033.9688,\n",
      "         3350.4404, 36910.6094,  5584.3057, 11680.1318,  4762.3291,  3005.1008,\n",
      "        23746.4551,  1261.8590,  5594.8457,  8442.6670,  4911.9731, 26412.2129,\n",
      "        14481.9785,  2138.0708, 39586.8633,  2457.2112,  9985.4580,  4346.6533,\n",
      "         9788.8662,  9914.8770,  4784.8198,  6875.9609])\n",
      "tensor([ 9595.4395,  4832.7827, 36869.9336,  1824.2854,  7256.7231,  9620.3311,\n",
      "         8713.6016, 24869.8359, 20709.0195, 10338.9316,  4618.5430, 16977.8887,\n",
      "         5969.7231,  6069.8608, 39508.8711,  6986.6968, 36103.1719,  2203.7358,\n",
      "        11015.1748,  1137.0110,  8419.5869,  8964.0605, 10928.8486, 27302.6289,\n",
      "         5514.9214, 36898.7344,  3201.2451, 21677.2832,  4976.8608, 10792.8076,\n",
      "        39871.7031,  1135.9407,  7160.3301, 13489.0400,  5511.5137, 11326.7148,\n",
      "         9490.4531,  9896.7666,  9563.0293,  8844.6348, 11358.5850, 13415.0381,\n",
      "         8211.1006,  4952.9585,  6182.0483, 27724.2891,  4347.0234,  5729.0054,\n",
      "         4949.7588, 18767.7383,  8825.4463, 10698.1123, 47069.8359, 28468.9199,\n",
      "         6268.3389, 18804.7520,  9335.0225,  5518.2974,  5469.8594,  8334.5898,\n",
      "         8938.4619, 18618.7344,  6527.7969,  6338.7256])\n",
      "tensor([40824.5898,  4375.7065,  7629.2109, 16500.1699, 44145.9219, 19124.3691,\n",
      "         4746.3442,  1615.7667,  2927.0647, 18596.5156, 12494.1885, 11436.7383,\n",
      "        11801.3779, 17904.5273, 20781.4883,  2902.9065,  4267.7329, 39000.4609,\n",
      "         8052.0400,  9877.6074, 60021.3984,  9564.2031,  9785.7725, 28954.2754,\n",
      "        10790.3672,  8976.1406,  2585.2690,  9763.2705,  4958.0732, 13123.6904,\n",
      "        12333.8281, 18838.7031,  4008.6965,  4341.8853,  8932.0840, 18435.6348,\n",
      "         8151.5220, 43097.4141,  9841.7959,  6686.3301, 42760.5039,  4466.6216,\n",
      "         4482.7412,  8428.0693,  6878.5137,  9421.6201,  4983.4512, 35391.7617,\n",
      "        15011.1729, 27346.0430,  8975.1846, 10982.5010, 11065.9971,  4765.5669,\n",
      "        12815.4453,  8762.7617, 12148.6709, 12636.3799,  5348.0586,  6658.6318,\n",
      "         1631.6683,  5840.4814,  4816.4673, 17731.7051])\n",
      "tensor([ 4977.9277, 10079.9287,  2217.6013,  3375.3062, 39109.7539,  6239.0723,\n",
      "        15322.8506, 11538.4209, 11848.1406, 36325.0352, 21259.3789,  7630.3252,\n",
      "         7153.4839,  9385.6670,  5972.3779, 25517.1133, 11658.1152, 25354.4531,\n",
      "        24508.0273,  6344.1738, 14692.6689, 44423.8047, 14224.7549,  5093.8828,\n",
      "        14200.8975, 11048.3555,  4234.9268,  6985.5068,  4705.4697,  8269.0439,\n",
      "         6117.4946,  5345.6807,  5438.7490,  7887.6675, 21103.6543, 18473.0508,\n",
      "        19071.2871, 12830.8213,  2801.2588, 16886.6387, 18972.4941,  9694.4453,\n",
      "        46412.5352,  9566.9912,  5708.8672,  9282.6914, 13483.6182,  4930.2988,\n",
      "         3533.4658, 15403.1816,  6341.8433,  4766.0220, 17179.5215,  4487.3071,\n",
      "        28724.9434, 17515.5254, 26109.3281,  5669.1655,  3693.4280, 24707.1543,\n",
      "         5123.0977, 17129.8613, 11987.1680,  6073.2026])\n",
      "tensor([ 9869.8105,  7968.3511,  8410.0469,  5857.2939, 12655.3779,  5662.2251,\n",
      "        11578.8184,  7576.7598, 18259.2168, 40242.7969,  8797.1270, 27963.1992,\n",
      "         6389.9458, 23757.2539, 42656.6914, 19533.8320,  9602.3350,  5958.8755,\n",
      "        12493.1104, 15554.3066, 13217.0947, 35595.5898, 11265.8232, 13224.0566,\n",
      "         2494.0220,  7165.7222, 28624.6621,  4434.9209, 11200.5312,  4746.4692,\n",
      "         8767.5732,  5395.9326,  7080.1421,  3056.3882,  6375.3955, 14349.8906,\n",
      "         4988.3501,  2498.4143,  4435.0942,  9676.6895,  4456.7559, 25333.3320,\n",
      "         3857.7593, 13019.1611, 29141.3594, 10818.9297, 10336.7979, 13981.8506,\n",
      "        11258.9199, 20745.9883,  2731.8477, 21984.4707, 13126.6777,  7499.3477,\n",
      "        11004.0498, 11481.3750,  4747.0527, 63770.4297,  6360.9937,  8944.1152,\n",
      "        10118.4238, 16577.7793, 12105.3203, 23162.3965])\n",
      "tensor([ 2716.0391, 10766.3447, 17914.1328,  5458.0464,  9473.7930,  7786.8691,\n",
      "         4239.8926, 16420.6777,  3985.0098,  8956.8672, 14006.8447, 30203.5801,\n",
      "         6128.7974, 29158.3398, 18246.4961,  5927.6006,  5320.1294, 10869.6318,\n",
      "        16862.8984, 12020.1104, 48885.1367,  9086.0244, 11289.1094, 27808.7246,\n",
      "        10431.9648,  2967.0117, 38117.9766, 39597.4062, 20630.7500, 15230.3584,\n",
      "        29330.9824,  2689.4954, 11184.4941,  8520.0264,  2362.2290,  4438.2632,\n",
      "         6821.3232, 40103.8906,  9630.5283,  8174.4531,  9101.7979,  2709.1118,\n",
      "        11390.6504, 11723.9111,  6311.9712,  9783.7812, 25309.4883,  4734.0195,\n",
      "         4751.0698,  4536.2588, 11509.6084, 16739.4160,  6662.1602, 13822.8027,\n",
      "         9262.5146,  4853.6143, 41949.2422, 13047.3320, 25891.7871, 14003.8545,\n",
      "        10906.6855,  6704.9863,  5615.3691, 10348.4600])\n",
      "tensor([11576.1299,  5347.0938,  4012.1570,  4832.9170, 13129.6035,  3493.3052,\n",
      "        10987.7705, 10269.4600, 16173.1904, 12153.3301, 10755.5332,  8816.9209,\n",
      "        12478.4727, 12638.1953,  7954.5171, 10838.7627,  8026.6665, 12475.3516,\n",
      "         5483.0806,  4784.0469, 34428.3711,  4709.6787, 14193.6133,  4809.7949,\n",
      "         2709.2439,  8865.4697,  5856.4531, 18820.2930, 11247.5459,  9058.7305,\n",
      "        11408.6787,  4828.5142,  6805.5850, 13887.2041,  8596.4873, 10649.4697,\n",
      "         8615.2998,  1263.2490, 20309.9355,  8968.3301,  1837.2371, 24227.3379,\n",
      "        13473.8389,  6475.8110, 13712.7646,  8598.2676,  6748.5913, 21595.3828,\n",
      "         5261.7490,  1622.1885,  6650.1948,  6453.5278,  8796.3018, 10012.6191,\n",
      "         7533.3374,  7392.6172, 12251.2207, 32548.3398,  5649.7148, 38499.0742,\n",
      "         5654.8184, 11566.3008,  4805.2856, 18237.9766])\n",
      "tensor([20172.1641, 25111.4336,  2400.4021,  8588.3887,  2020.1770, 10702.6426,\n",
      "        24971.7656, 11003.6924, 38239.4102,  3935.1799,  4927.7622,  6572.6685,\n",
      "         6571.0244,  5857.0200,  6733.6309, 16851.3691,  5325.6509, 21098.5547,\n",
      "         5410.2686, 12268.6318, 10560.4922, 10576.4160,  1391.5287,  6603.6201,\n",
      "        10992.1768,  6665.0405,  6123.5933, 16232.8467,  6108.3462,  2585.8506,\n",
      "         9097.2939,  5385.3379,  4058.7124,  9182.1699,  3161.4541, 13429.0352,\n",
      "        16118.8936,  9270.5479, 12646.2070, 11232.4961, 38514.6719,  3206.4915,\n",
      "        14455.6445,  4281.1631,  4645.6904, 43896.3750, 12142.5781, 14988.4316,\n",
      "        11155.0889, 12360.8281,  9239.3896, 10138.4346,  6449.4521, 43254.4180,\n",
      "        28152.8223,  6284.9707, 14004.3096,  8823.2793,  4953.1499,  4714.8999,\n",
      "        40160.7891,  5017.6616,  4340.0557, 27292.3242])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20813.8711,  5368.5508,  6457.8433, 44069.0625, 34203.3672, 58571.0742,\n",
      "        13275.3184,  4472.5229, 47273.1094,  1969.6140,  4670.6118,  5377.8545,\n",
      "         3062.5083,  3904.8750,  5472.7295,  4689.0356, 44790.4922, 11938.2559,\n",
      "        36219.4062, 11115.5176, 10250.4209,  5811.5186,  4767.5073, 14358.3643,\n",
      "         7738.2622, 11346.8027,  4349.4619, 13343.0127, 12916.5557, 41034.2227,\n",
      "         6378.0747,  4957.9004, 11388.3428,  9581.1436, 11049.1953,  2250.8352,\n",
      "         9910.3594,  4707.6753,  4391.6519,  8548.1328,  6414.2407,  5288.9102,\n",
      "         9978.4072, 18836.8555, 31620.0020,  7098.9106, 19530.0527, 47974.2344,\n",
      "         1977.8149,  7494.2979, 11443.9365, 10882.8584, 22494.5723, 11313.6768,\n",
      "         3392.3652,  1131.5066,  9290.1396,  7117.9800,  1628.4709,  6503.9780,\n",
      "        12231.6133,  5000.0771,  8731.4805,  9491.4062])\n",
      "tensor([14064.4277, 11196.6904,  5507.6475, 13390.5586, 11129.3320, 14418.2803,\n",
      "         6999.2749,  6335.7681, 15169.1836, 30184.9375, 18150.9121, 46255.1133,\n",
      "        44641.1992, 14310.4971,  7237.4185,  1909.5275,  9991.0381, 15161.5342,\n",
      "        10370.9121, 10824.4844,  7112.5806,  1725.5869,  8886.0176,  3532.9143,\n",
      "        11512.4053, 23258.9980,  3994.1777, 37662.7617, 10435.0654,  4948.6729,\n",
      "        18707.2246,  6009.3750,  6265.1655,  5930.1489,  6227.6714, 26467.0977,\n",
      "         6728.2705,  8547.6914, 14001.2871, 10879.1729,  4661.2861,  1634.5734,\n",
      "        11013.7930,  4686.3887,  5124.1885, 10812.6650,  3394.4917, 34838.8711,\n",
      "         6013.1274, 32734.1855,  6770.1924,  4527.1177,  5855.9023, 38054.4922,\n",
      "         4441.2134, 13143.8652, 16378.9141,  9590.4580,  2459.7202, 18191.7344,\n",
      "        11520.0996,  8583.2227, 11262.1416, 10436.0957])\n",
      "tensor([ 3943.5955, 28105.2344, 10805.9551, 15648.6582, 11482.6348,  5127.6079,\n",
      "         6648.9106,  9012.5898, 40232.9336, 24904.3164,  4772.9556, 10056.7275,\n",
      "         6238.2979, 12645.1660,  5709.1646,  4158.2539, 11120.1904, 10676.0977,\n",
      "        23300.7910,  5246.0469,  5910.9438,  1826.8430, 10373.2334,  1252.4070,\n",
      "         3353.2839, 42969.8516, 20878.7852,  4822.7959, 38746.3555, 12387.7539,\n",
      "         9441.5166, 14925.9062, 41999.5195, 27107.3418, 24685.0215, 13673.9072,\n",
      "        19453.6855, 23065.4199, 12643.3779,  6032.5845, 10611.9785, 13462.5195,\n",
      "         2156.7517, 37464.0859,  8671.1914, 38511.6289,  5364.4507, 11259.7061,\n",
      "         9644.2529,  4888.4927,  9669.7842,  7960.8682,  5086.6528, 33475.8164,\n",
      "         4674.7974,  6312.1934,  3594.1709,  1625.4337, 11166.6670,  8709.8965,\n",
      "        10148.4678, 13660.8369,  5495.3916, 38245.5938])\n",
      "tensor([ 5793.8735,  8869.2197,  8827.7451, 12523.6045, 11030.4414, 52590.8281,\n",
      "         7949.7476,  4005.4226,  8878.8574, 20341.3789,  8334.4580,  3736.9246,\n",
      "         5150.3169,  8703.4561, 18963.1719,  8486.9014, 14164.9453,  4243.5898,\n",
      "        18772.8262, 42560.4297, 10932.0791, 18444.5488, 10896.6133,  9946.9463,\n",
      "         4571.4131, 13880.9492,  2730.1079, 11820.7783,  5177.4429, 11737.8486,\n",
      "         6611.5747,  5535.8774, 36536.8750, 11555.6875, 11093.9844, 13640.6631,\n",
      "        46460.5859,  1880.0699,  7623.5181, 16405.6543,  9583.7246,  5459.4141,\n",
      "         5836.8037, 39241.4414, 14833.3945,  6521.4097,  4773.0742, 47289.7266,\n",
      "         4518.2300, 10159.4766,  6059.3662, 19257.8496, 12741.1670,  8797.9082,\n",
      "        13352.4766,  9779.9639, 13224.6934, 37284.8203,  3579.8286,  1629.8335,\n",
      "        16069.0850, 19235.8223,  4032.2407,  6799.4580])\n",
      "tensor([20420.6055, 11008.3906, 39802.3516, 11094.9131,  3919.3872, 20135.9785,\n",
      "        11658.3789,  5240.1348,  1708.9258, 13030.3164,  5327.4004,  8782.4688,\n",
      "         6682.4009,  5587.0991, 17526.3105, 13887.9688, 11165.4180, 19749.3828,\n",
      "         7682.6699,  9080.3447, 26392.2598, 11883.8730,  7173.3599,  3309.7925,\n",
      "         4779.6025,  5426.9219, 14451.8350,  6746.7427, 26081.9590, 12112.2822,\n",
      "         5920.1040, 38998.5469, 35907.0117,  6341.7935,  6725.8091, 19496.7188,\n",
      "        12925.8857, 11399.6484,  4826.3560,  8612.3125,  5979.7310,  6272.4771,\n",
      "        26322.8652, 18322.1152,  9132.1406, 47896.7930, 44246.3320, 11961.2793,\n",
      "         2946.0974,  2731.9121,  8584.4238, 11443.1621,  3877.3042, 20624.8047,\n",
      "         2842.7607,  8023.1353,  7348.1421, 14407.8955,  3277.1609, 10231.5000,\n",
      "         5187.8267,  2322.6218, 15795.4727,  6255.7744])\n",
      "tensor([ 4687.7969, 24329.3379, 11394.0654,  8583.3838,  9535.4424, 10024.0820,\n",
      "         5191.5757, 11015.3203,  7467.0039,  4645.9453, 47417.1797,  4590.5381,\n",
      "         7281.5054,  7155.7979, 48316.0078,  7633.7207,  9179.2607, 48517.5625,\n",
      "         5211.0073,  4766.5093,  6338.0757,  4725.9038,  9861.1865,  9903.7754,\n",
      "        40720.5508,  1980.0699,  6933.2422,  8773.6094, 23887.6621,  7626.9932,\n",
      "        25718.6445, 13063.8828,  4989.5703,  7620.8408, 32108.6621,  3213.6221,\n",
      "         6185.3208, 14901.5166, 24096.1113,  3919.9067,  8859.7686,  8278.1475,\n",
      "         3366.6697,  9583.8936, 19040.8770,  5466.0967, 11312.4746,  2254.7966,\n",
      "         8902.8799,  3479.0305, 28443.2227, 10965.4463, 11027.3135,  4133.6416,\n",
      "        11068.2744, 17043.3418,  6282.2349, 18381.5977, 12649.6045, 12829.4551,\n",
      "         5265.0400,  9909.5566,  5377.0161, 12592.5342])\n",
      "tensor([ 5003.8530,  1712.4691,  9236.1387,  5301.3931, 10788.7412,  2352.9685,\n",
      "        26140.3594,  4809.4521,  6216.3872, 13929.6611,  2279.8958,  4113.1367,\n",
      "        16140.9229,  9249.4951, 28340.1895,  9625.9199,  5846.9175,  7742.2729,\n",
      "         8795.0664, 11411.6846,  9337.3311,  2714.4729,  4710.8916,  3234.1348,\n",
      "        36041.9883,  6621.8008,  6455.8628,  6658.1187, 11743.9346,  5390.6113,\n",
      "         2699.5684, 11033.6621,  5970.1597,  7409.4360, 11833.7822,  8162.7163,\n",
      "        10114.5957,  4518.8262, 19370.4336,  8515.7588, 12146.9707,  4452.5493,\n",
      "         5146.2993, 29186.4824, 15230.3242,  5397.6167, 17883.0195,  2117.3389,\n",
      "        40935.6445,  8059.6792,  3077.0955, 10795.9375, 12878.1699, 36189.1016,\n",
      "         4906.4097, 12905.0811,  8704.5488, 12404.8789,  1682.5970,  9202.4727,\n",
      "         8078.5361,  4436.6260,  7418.5220, 17776.6387])\n",
      "tensor([ 5241.4849,  7228.2158,  9471.1426,  4137.5225, 24513.0918,  4449.2339,\n",
      "         4185.0977, 45710.2070, 15179.2051,  2788.0889, 23241.4746,  5125.2158,\n",
      "        17063.4258,  8993.5400,  6488.8950,  5549.3247,  6712.0068,  6772.6304,\n",
      "        12612.1074, 39969.4414, 40142.9062, 23843.5332, 10499.2393,  4294.2544,\n",
      "        21220.4004, 11345.5186,  6640.5449, 23033.8848,  5152.1338,  5031.2695,\n",
      "         6201.7407,  5999.7490, 20296.8633, 24535.6992,  4884.5830,  4761.0718,\n",
      "         6046.6641, 11035.8584, 11554.2236, 26619.4922,  8219.2041,  7175.9937,\n",
      "        11107.4102, 11335.8467, 10096.9697, 14409.9561,  5258.5146,  7323.7349,\n",
      "        10407.0859,  9308.5625,  3502.8157,  8240.5898, 19361.9980, 37701.8750,\n",
      "        10141.1357, 13968.1250, 11212.9551,  4518.1045, 12979.3584,  2396.0959,\n",
      "        11735.8447,  4022.6128, 44241.9023,  5028.1250])\n",
      "tensor([ 9370.4639,  8987.6270, 12430.9531, 12251.3223,  1149.3959, 19988.1250,\n",
      "         5712.3555, 10806.8389,  6516.0615, 11363.2832,  2597.7791,  2134.9016,\n",
      "         9619.6699, 24520.2637, 10602.3848,  5272.1758, 13846.9619, 19521.9688,\n",
      "         9722.9551,  6515.7598,  6088.7759,  3378.9099, 11034.1230,  5136.1411,\n",
      "        37086.6680, 25026.4238, 24635.1055,  4356.1611,  6474.0132, 15006.5791,\n",
      "        17511.0918, 12363.5469,  2904.0879,  3615.3975,  2203.4719,  9222.4023,\n",
      "         3875.7341,  6379.7090, 18775.4609,  1631.8212,  9439.2363, 17150.6855,\n",
      "        11615.2969, 23261.0215,  7222.4863, 49577.6641, 33750.2930,  8827.2100,\n",
      "         5044.3613,  7746.2632,  8716.2393, 12103.8350, 10802.8574,  4237.1265,\n",
      "        18745.6289,  5256.2808, 23104.1172,  9273.9131, 11552.9043, 10841.7090,\n",
      "         2755.0210,  5630.4580,  4510.5015,  7965.5845])\n",
      "tensor([ 7296.7134, 23510.6797,  1261.4420,  6981.8662, 13256.1748, 10072.0547,\n",
      "        46113.5117, 12267.4463, 11232.9414,  4821.6626, 27375.9043, 23352.3184,\n",
      "         7337.7480, 13508.6172, 43943.8750, 15305.1963, 23082.9551, 12629.1660,\n",
      "         5425.0234,  6027.3472,  1633.9618,  9321.8613, 19933.4570,  1906.3583,\n",
      "        10256.5186, 12013.5322, 39047.2852, 12094.4775, 11342.8555,  8871.1514,\n",
      "         5194.6929, 18242.1328,  9723.1592, 36149.4844,  5454.9780,  1702.4553,\n",
      "         6417.0142,  9866.3047, 25678.7793, 23401.3066,  6292.8462, 10950.0391,\n",
      "        11386.5137, 11362.7549, 11908.1914,  6101.8604, 41920.0781, 11975.6758,\n",
      "         7443.6431,  8423.9121, 11356.6611, 42124.5156, 14115.0889,  9447.2500,\n",
      "        32471.2324,  3180.5100, 11967.3867, 38431.8398, 23862.7793, 24876.9414,\n",
      "        11020.7070, 18303.1406, 12474.2529, 11264.5410])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4040.4421,  5038.1357, 10978.5791,  9411.0049,  2302.3000, 10992.3867,\n",
      "         8936.3086, 10909.8730,  6764.9863,  2855.4375, 10115.0088,  4944.6729,\n",
      "        21195.8184, 10601.6318, 13405.3906, 36580.2812,  4697.8984, 19528.0117,\n",
      "        47937.2695, 12957.1182, 14313.8467,  8716.3652,  9821.0898, 10887.2500,\n",
      "        47916.9648, 13747.8721,  5550.9214, 11798.5205, 12044.3418, 39983.4258,\n",
      "         1141.4451, 13502.7852, 12100.5918, 14194.9961,  7262.3726, 25274.0840,\n",
      "         5748.8198,  8591.2148,  3987.9260, 10987.3252,  2020.5522, 11250.8926,\n",
      "        10989.1992, 21533.6562, 16218.6719,  4402.4639, 10232.0596, 13451.1221,\n",
      "        20144.6738, 10992.3857,  9858.0254, 10325.2061,  6405.0771,  7358.1758,\n",
      "         6956.8833,  6099.9766,  6753.0381, 19080.2051, 32584.9629, 11879.1045,\n",
      "         8937.6973, 10991.3584,  6666.0552,  5313.6807])\n",
      "tensor([12558.4961,  3847.6741,  5829.9429, 27709.7969,  9453.6953, 41817.2773,\n",
      "         9634.5381, 43921.1836,  2281.3782,  8457.8184, 24956.6738,  9704.6680,\n",
      "        10451.2744, 20155.6504,  2680.9492, 17879.2539, 37270.1523,  4836.1221,\n",
      "        14201.5137,  6961.4736,  4260.7441,  6781.3540,  9172.3047,  5207.8936,\n",
      "        10961.7842,  5114.4463, 45008.9570,  9668.9141,  7445.9180, 21112.5098,\n",
      "         2789.0574, 34969.1055, 17878.9004,  9377.9043,  4805.6055,  2201.0972,\n",
      "        10961.4395,  8727.1738,  5142.2842, 11475.5439, 40974.1641,  6653.7886,\n",
      "         8444.4736,  8310.8389,  3208.7871,  6526.5928, 21232.1816,  6313.7588,\n",
      "         7147.1050,  4922.9160, 34393.4492, 16115.3047, 29918.5391,  9046.3711,\n",
      "         5488.2622, 18129.7715, 45610.9219, 10763.5752,  7526.7065,  5204.4038,\n",
      "        18944.2148,  9800.8887,  3914.3958,  6593.5083])\n",
      "tensor([ 5400.9805,  2217.4692, 14643.6221,  7228.7710, 17550.7637, 11534.8730,\n",
      "         6429.9795,  6235.9092,  2137.6536,  5313.0581,  5210.3770,  9166.4688,\n",
      "        19658.3535,  1877.9294,  6787.5308,  4618.7397,  8971.3008, 23632.3750,\n",
      "        11168.2471,  1728.8970,  8017.0610, 19107.7793,  9230.8125,  2128.4312,\n",
      "        11145.7910, 12982.8750,  8635.2988,  6604.7139,  2055.3250,  8591.3857,\n",
      "         6751.0796,  3536.5894,  6607.4302,  8753.9961,  5002.7827, 11433.1113,\n",
      "         5244.6787, 11016.3340,  7150.3506,  9778.3477, 39245.1641,  1639.5631,\n",
      "        10355.6406, 10680.1963,  7727.2534,  9781.5039,  1744.4650, 44397.3906,\n",
      "         4931.6470, 19515.5410, 10008.9678, 17081.0801, 46247.9648, 10922.4590,\n",
      "         5257.6685, 18806.1445,  5119.3516,  8733.2295, 11281.5967,  8526.8809,\n",
      "        11842.6240, 11298.1533,  3537.7029, 10740.0205])\n",
      "tensor([ 4768.7769, 17984.5977,  5028.1465, 30063.5801, 20335.8613,  2899.4893,\n",
      "        11008.7568, 42000.0352,  4708.9570,  9386.1611, 44400.4062,  1711.3367,\n",
      "         8697.2246, 12233.8281,  6750.3789,  6652.5288, 22493.6602,  2457.5020,\n",
      "         2198.1899, 22198.8750, 10976.2461, 10491.6934,  4667.6074, 48885.2422,\n",
      "        10806.4873,  9193.8389, 10537.9121,  8953.5049, 41097.1602,  6270.9673,\n",
      "         8047.2007,  9036.5869, 11117.1660, 10797.3359,  8988.1592,  6738.0117,\n",
      "        13073.8184, 12904.9053,  2750.1782,  5495.3623, 12635.6611, 14923.2422,\n",
      "         5525.4204,  7148.5503,  7634.4023, 46544.0820, 23945.7148, 14004.6660,\n",
      "        10959.6943,  3761.2920,  1674.6323,  6123.9985, 10621.7578, 12476.7812,\n",
      "         3526.3516, 19964.7461, 17663.1445,  6585.8774, 38169.4375, 14289.9834,\n",
      "         8583.9736, 54108.4180,  4892.4019,  9391.3457])\n",
      "tensor([15003.8145, 11922.0859,  3310.4224, 46718.1641,  1972.9500,  4883.8662,\n",
      "         2353.8313, 17560.3789,  1712.2271,  2527.8186,  3535.1780, 28039.5391,\n",
      "         5390.6162,  2207.6975,  9636.7324, 12557.6055,  4415.1587, 10594.5020,\n",
      "        19444.2656,  6677.5752,  4830.6299, 14043.4766,  8653.0039,  1621.8827,\n",
      "         6990.8960,  2150.4690,  4472.8804,  2261.5688,  8549.4805, 11837.1602,\n",
      "         5406.6313,  6403.4287,  5620.7578, 14051.1152,  7265.7026,  5379.2900,\n",
      "         8186.8477, 27218.4375,  1694.7964, 22218.1152,  7443.8477,  4801.4756,\n",
      "        26823.7402,  8766.9248,  9313.5957, 13457.9609,  3956.0715, 13936.8818,\n",
      "        47437.5430,  6658.3477, 16753.3984, 36397.5742,  6186.1270,  9820.2334,\n",
      "        17496.3066,  4274.2217, 18817.6211,  8712.5498,  9288.0264, 11272.3311,\n",
      "         4149.7358, 18415.4375,  6364.9355,  8606.1201])\n",
      "tensor([20277.8066,  4320.4106, 11228.0234,  7609.9360, 26993.5117, 18823.5957,\n",
      "        16796.4121,  5116.5005, 19129.8770,  5062.8535, 10986.4014, 12222.8984,\n",
      "        10782.4883, 19726.1484,  4921.7368, 12648.7031, 11314.8672, 23514.5645,\n",
      "         6071.1353,  4756.5337, 11661.3965,  2639.0430,  3757.8447,  9523.9785,\n",
      "        12644.5889,  7724.7695, 10715.7383, 14571.8906, 10334.8125, 10991.8525,\n",
      "        11372.7080, 14453.7627, 10313.1445, 42856.8398, 10796.3506, 10090.1299,\n",
      "        19431.9727,  9365.2812,  8124.4082, 38392.7461, 21491.9805, 13844.5059,\n",
      "         8699.7979,  5271.1968, 26926.5137, 13555.0049,  9597.2881, 21828.0293,\n",
      "        15181.1689,  8601.3291,  3281.9583, 14154.8193,  5836.5205,  9163.1084,\n",
      "        24058.4453,  7161.5146, 11353.2275, 38092.4102,  3393.5928, 11163.5684,\n",
      "         9813.9365,  4917.9258,  4611.5186,  8587.1279])\n",
      "tensor([ 8678.6367,  4669.0410,  2407.0042, 12890.0576, 42705.1836,  4889.9995,\n",
      "        11339.5977, 18223.4512, 35491.6406, 13770.0977,  1708.0013,  9086.5273,\n",
      "        18232.0332, 18955.2207,  3484.3311,  6748.7524, 14283.4590, 17593.3750,\n",
      "         7688.3145, 20167.3359, 29762.3105, 12323.9355,  9210.7949, 36036.2812,\n",
      "         5451.4233,  8989.9814, 10882.7529, 13342.3301,  7372.1353,  6684.9663,\n",
      "        26125.6738,  3736.4646,  4752.7188,  8302.5361, 10952.2207, 13041.9209,\n",
      "         8688.8584, 10446.0498, 19434.5449,  7441.0532,  9074.2441,  5702.2310,\n",
      "        13675.2305,  5871.2061,  9450.8184, 13204.2861, 12265.8857,  3369.4167,\n",
      "        45841.4688,  5398.8911,  3522.6023, 12142.8438, 12629.8965,  9095.0684,\n",
      "        23306.5469,  7153.5537, 24499.0840, 17578.2148, 46217.6641, 14799.0312,\n",
      "         2721.3208,  4853.6582, 11247.4453,  6628.2686])\n",
      "tensor([ 5889.1040,  6788.5342, 12401.7461, 10376.2949,  6658.2373,  7705.1616,\n",
      "         9141.2588,  6674.1318, 15019.7598, 26875.1562,  5227.2632,  9655.7354,\n",
      "         8734.1172,  9143.5732,  1632.5645,  9755.0039,  3404.4211, 10092.3564,\n",
      "         8107.7363, 10264.4424,  5312.5171,  7121.3081,  4869.0752,  3021.8091,\n",
      "        38310.2305,  4360.3711, 10451.9365, 38258.9219, 40941.2852,  8624.3711,\n",
      "         5214.5659, 16710.0039,  2523.1694,  2200.8308, 23807.2402,  5050.9492,\n",
      "         4708.5298,  4745.0073,  9931.6240, 37484.4492,  6286.6333, 16776.3047,\n",
      "         8116.2690,  9921.9004, 47403.8789, 27000.9844, 11176.2168, 18328.2383,\n",
      "        39722.7461,  5312.1699,  5757.4136,  6258.0811, 11038.9854, 14001.1338,\n",
      "        17844.1328, 11070.5352,  3238.4358,  9748.9102, 10929.7871,  8217.4004,\n",
      "        37465.3438, 14744.0938, 12030.7793,  5221.3887])\n",
      "tensor([11537.5742,  4297.9951, 43223.4648, 42661.0117, 34593.8672, 10015.4395,\n",
      "         8583.3223,  6716.7861,  4788.8638,  6417.5674, 40690.5430, 11891.9922,\n",
      "         6655.1758,  1748.7740, 10825.2539, 10751.9248, 10726.4121, 23552.6621,\n",
      "        14109.8145, 37607.5273,  9724.5303,  4343.5864, 46199.6367, 46889.2617,\n",
      "         3353.4702,  5465.4326, 19593.2031, 26730.3984,  5934.3799,  8070.9092,\n",
      "        10594.2256,  5412.7876, 28287.8984, 10584.0400, 10493.9453,  4791.5850,\n",
      "         4863.7627, 18866.6875,  6018.9741,  3443.0640, 10897.6035,  6753.0000,\n",
      "         2210.1880,  7731.8579,  7162.0122, 36837.4688, 20334.6172, 12797.2100,\n",
      "         2221.5645, 16450.8945, 15817.9854, 11193.2080, 19480.2383,  9630.3975,\n",
      "         4934.0996,  6203.9019, 17101.9160,  6014.0322,  6416.2925, 48675.5195,\n",
      "        17500.2051,  9104.3770,  6457.3037, 11540.8369])\n",
      "tensor([19350.3691,  5012.4712,  7196.8672,  9957.4824, 13140.4922, 12757.7354,\n",
      "         6940.9097,  6971.5854, 12656.7852, 28476.7344,  5548.1787, 12835.6465,\n",
      "         6718.9780, 42983.4570,  5377.4580,  5966.8872, 30284.6426, 17942.1055,\n",
      "         7400.1914, 11366.3506,  6726.0503, 38711.0000, 12032.3262,  5898.1665,\n",
      "        25382.2969, 13648.6016, 25147.0195,  9850.4316,  3554.2029,  9485.2559,\n",
      "         6755.6372, 38349.7500, 40861.9336,  9411.1689,  7441.2729,  3591.6248,\n",
      "         5272.6763, 47412.0312,  9610.9561,  9979.2393, 11200.0918, 24671.6641,\n",
      "         1839.4099, 25081.7676,  7077.1895,  8592.1816, 10761.4844,  8564.4863,\n",
      "        18650.8203, 15731.3271, 11073.1758, 14256.1924,  4307.1519, 43753.3359,\n",
      "         5983.9121,  2483.7361,  5824.1021,  9174.1357,  6211.9331,  8083.9199,\n",
      "         6688.9746, 12838.9932,  5809.5537,  9584.8838])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16417.6914,  1981.5819,  6330.9521, 19085.0898,  6350.9326, 12609.8867,\n",
      "         8218.7500, 11170.6738,  6756.5708, 21308.4199,  2331.5190,  4986.4912,\n",
      "         5238.0103, 17434.5801,  7421.1943,  5599.7192, 37133.8984, 10278.5635,\n",
      "        12112.9785, 19892.5918,  6475.1812,  5243.5674,  4500.3394, 19069.4746,\n",
      "        10274.5830,  6761.2153, 24476.4785,  4428.8877, 19594.8105,  5282.9453,\n",
      "         5150.5874,  4738.2681,  5455.4463,  4853.2388, 17626.2402,  5602.9517,\n",
      "        16657.7168, 45863.2031, 29523.1660,  6356.2705,  5151.9502, 33900.6523,\n",
      "        11226.9932, 11830.6074,  5003.3179,  7512.2671, 27037.9141,  2211.1309,\n",
      "         1727.7850, 15439.3281,  3850.3679, 22478.5996,  8551.3467,  4793.2402,\n",
      "         9872.7012,  6184.2993, 20630.2832, 46475.2383, 43287.3750, 13725.4717,\n",
      "         8732.0859,  8792.4785,  9254.5244,  8662.5176])\n",
      "tensor([ 3766.8838,  7804.1606,  3972.9248,  9516.3633, 27117.9941,  6777.4302,\n",
      "        20149.3223,  6548.1948, 41279.2422, 14768.0205, 11568.4766, 11300.4541,\n",
      "        12265.5068,  5527.3135, 12387.4072, 34472.8398,  6664.6860, 45702.0234,\n",
      "         9393.3584, 22412.6484,  9948.9697, 10720.6953,  4708.3374,  4522.0054,\n",
      "        10736.8711, 26489.5762,  6093.4038,  5822.7246,  9964.0596,  5415.6611,\n",
      "         4433.7495,  8769.6445, 32905.8164, 15238.4053,  8965.7959,  5185.2397,\n",
      "        10065.4131,  6763.1831, 24919.3574,  4846.9849,  8517.6445, 12129.6143,\n",
      "        10807.3350, 11664.4541,  6604.6367, 12731.0000,  6657.5737, 22060.1719,\n",
      "         7085.8442,  6832.8516,  6677.3218,  7671.1797, 36023.1484, 46661.4414,\n",
      "         9642.2871,  7810.5654,  5266.3657,  4929.9517,  8559.9883, 30269.4277,\n",
      "        10390.9131, 11504.0811,  3398.1965,  9873.8760])\n",
      "tensor([ 1242.8160, 27626.1797, 18608.2617,  9950.3486,  3980.1387, 11854.6738,\n",
      "         3704.3545, 13670.9150, 13393.7559, 22331.5664, 10693.0615, 36225.4023,\n",
      "        10600.5479,  6710.1919,  2395.1716, 19524.6582, 46145.6523, 17748.5059,\n",
      "         2404.7339,  6663.0903, 15746.4619, 28058.4707, 46654.7266, 21659.9297,\n",
      "         6543.0454, 13919.8232,  8823.9854,  7985.8149, 41919.0977, 13994.2930,\n",
      "         8491.9072,  3176.8159, 27941.2871, 23244.7910, 13065.5439,  7156.5244,\n",
      "        39532.2461,  9414.9199, 33907.5469,  7206.2949, 14133.0381,  2045.6853,\n",
      "         6389.8535, 11950.1514, 10106.1338,  4895.6797, 41661.6016,  4318.3760,\n",
      "         7034.8438,  3556.9224, 17174.6816, 10823.2266,  6211.3403, 14249.3955,\n",
      "        11674.8730,  6105.5820, 18310.7422,  4292.8584,  8277.5234,  4743.8374,\n",
      "         6112.3530,  6149.6147, 10751.5498,  9047.6494])\n",
      "tensor([21925.6836,  4357.0435,  8595.2559, 11884.0488, 12874.1396, 55135.4023,\n",
      "         3393.3564, 10746.0781,  3855.9429,  5859.6704, 10240.5527,  5929.4653,\n",
      "        24915.0469,  7222.7861,  4772.8760,  6775.9609, 46988.3516, 12950.0713,\n",
      "        16085.1279,  4698.2017,  4340.4409,  2741.9480,  1880.4871,  4934.7051,\n",
      "         7045.4990, 14049.8330,  3044.2134, 10563.2197, 23582.4688,  5253.5239,\n",
      "        37829.7227,  1964.7800,  5926.8462, 12491.8818, 44202.6523,  8553.9541,\n",
      "         8712.3389, 44061.4805,  6322.3701,  5008.2500, 16586.4980,  8827.5264,\n",
      "         4662.8926, 13228.8467,  9095.5312, 11396.9004,  4285.9956, 25154.1719,\n",
      "        39385.6328,  6198.7520, 10787.6904, 10832.7705, 11130.3535, 13557.7188,\n",
      "         3947.4131,  1526.3120, 33353.5000, 39836.5195,  5210.1001, 11111.9248,\n",
      "        19214.7051,  5257.9595, 23563.0156, 40932.4297])\n",
      "tensor([42284.9180, 28868.6641,  2026.9741, 18989.3652, 21806.1816, 11355.8174,\n",
      "        11185.9141, 39727.6133,  4894.7534,  4779.6025,  6216.3872,  8844.6348,\n",
      "        16776.3047,  5425.0234, 13846.9619, 17593.3750,  2689.4954, 10976.2461,\n",
      "         9884.6934, 41949.2422,  9138.3857,  8807.7295, 16796.4121,  8559.9883,\n",
      "        39586.8633,  3526.3516,  8716.2393, 12905.0811,  6664.6860, 19071.2871,\n",
      "        10431.9648,  6775.9609,  9948.9697, 41180.8711, 13393.7559,  8598.2676,\n",
      "         5327.4004,  4833.8750, 20277.8066,  1137.0110, 22218.1152, 13145.5684,\n",
      "        10250.4209,  6697.6084,  5257.5078, 48173.3594,  7625.1558, 11837.1602,\n",
      "         3234.1348, 11833.7822,  5427.3716,  2585.8506, 12838.9932, 11018.9854,\n",
      "        18817.6211, 36189.1016,  6674.1318,  8792.4785, 20630.2832,  4709.6787,\n",
      "         9046.3711,  6272.4771, 15554.3066, 11068.2744])\n",
      "tensor([43753.3359,  2480.9790,  6684.9663,  4909.7246, 12533.9229,  8709.8965,\n",
      "         5000.0771,  6728.2705, 12491.4844,  3268.8467, 28868.6641,  8591.2148,\n",
      "         9182.0586, 12235.8389,  6112.3530, 11048.3555,  6322.3701,  4297.9951,\n",
      "         8116.2690,  4234.9268,  2352.9685,  6648.9106, 15439.3281, 17844.1328,\n",
      "         4752.7188, 18232.0332,  4350.5142,  4449.2339,  5654.8184,  2362.2290,\n",
      "         5347.5425, 18804.7520,  5257.9595,  5050.9492, 10824.4844,  8588.3887,\n",
      "        10991.8525,  2731.8477, 16739.4160,  6837.3687, 17740.2930,  8595.2559,\n",
      "         4766.5093,  4267.7329,  8816.9209, 11353.2275,  4747.0527, 14410.9316,\n",
      "        25729.1855,  6961.4736, 40941.2852, 14064.4277,  4058.7124, 39109.7539,\n",
      "         2680.9492,  9946.9463, 21677.2832,  5345.6807, 13508.6172,  7633.7207,\n",
      "         4294.2544, 19539.2422,  2045.6853,  4766.0220])\n",
      "tensor([ 6196.4482,  3005.1008,  5605.0166,  9878.7461,  9462.1533,  4784.8198,\n",
      "         4266.1660,  9783.7812, 17352.6797, 62592.8750,  6009.9824, 34166.2734,\n",
      "         9172.7686,  6616.0239, 10090.1299, 17716.6367, 34393.4492, 29523.1660,\n",
      "         2775.1921,  8347.1641,  5240.7651,  7050.0215,  6571.0244, 12638.1953,\n",
      "        11013.7119,  9222.4023, 10373.2334, 11460.4990,  3847.6741, 17009.3359,\n",
      "        14249.3955,  5288.9102, 16455.7070,  9360.9238,  4977.2495, 15169.1836,\n",
      "        38239.4102,  4360.3711,  6764.7871,  6117.4946, 12485.3594, 11555.6875,\n",
      "        44246.3320,  9962.6123,  6093.4038,  6415.2651, 17434.5801, 20099.7363,\n",
      "         9748.9102,  7201.7007,  2302.3000, 12360.8281,  4827.9048, 18091.3730,\n",
      "         5093.8828, 24956.6738,  5415.6611,  5857.2939, 42983.4570, 10787.6904,\n",
      "        18823.5957,  7475.3062,  3985.0098, 12493.1104])\n",
      "tensor([ 6680.3022, 11335.8467, 13352.4766, 19663.3809, 10118.4238, 14235.0723,\n",
      "        18444.5488,  8553.9541, 10148.4678,  6361.5400,  6738.9458, 37747.6836,\n",
      "        12491.8818, 46151.1250,  4040.5583,  5400.9805, 18303.1406,  5080.0962,\n",
      "        14224.7549,  8938.4619, 11168.2471, 11170.6738,  9321.8613, 10942.1318,\n",
      "        11228.0234, 39611.7578, 12644.5889, 18317.6250,  8732.0859, 13483.6182,\n",
      "         7441.5010,  4805.6055, 13770.0977,  8798.5928, 18629.5156, 25333.3320,\n",
      "        13143.8652,  4884.5830,  9875.6807, 19533.8320,  3398.1965,  8825.0859,\n",
      "        10959.3301,  2967.0117, 10019.4902, 18861.9668,  9716.7441,  4433.2295,\n",
      "        19530.0527,  4133.6416, 30166.6191,  5253.5239,  9046.0400, 11316.2148,\n",
      "         6088.7759,  5927.6006, 18310.7422, 23162.3965, 14692.6689,  9174.1357,\n",
      "        10806.8389, 11386.5137, 10159.4766, 11615.2969])\n",
      "tensor([11100.1875, 40242.7969,  7730.8525, 11891.9922,  4435.0942,  9861.0254,\n",
      "         3227.1211,  8116.6802,  8162.7163,  3404.4211,  9290.3174,  8797.9082,\n",
      "        13844.7969,  9236.1387,  6360.9937, 12254.8135, 39245.1641,  6705.0220,\n",
      "        11554.2236, 23757.2539,  6999.2749, 18157.8770,  7742.2729, 38310.2305,\n",
      "        15413.5967,  5702.2310, 10812.6650,  2102.2646,  6356.2705,  5466.0967,\n",
      "        13140.4922, 29010.3184,  1137.4697,  2026.9741,  4888.4927,  7935.2910,\n",
      "         4832.9170,  5271.1968, 13880.9492,  7098.9106,  4809.7949,  3385.3992,\n",
      "         4243.5898,  9453.6953,  3471.4097,  7147.4727, 19107.7793, 13073.8184,\n",
      "         8869.2197,  6338.0757, 16297.8457,  9921.9004, 45863.2031,  3293.2917,\n",
      "         6088.1855, 10711.7891,  3261.9417,  4687.7969,  6933.2422, 12635.6611,\n",
      "         6128.7974,  8107.7363,  4510.5015,  9634.5381])\n",
      "tensor([ 1149.3959, 44213.4219,  4461.8037,  7345.0840, 44641.1992,  1837.2371,\n",
      "        17174.6816, 10422.9170,  6403.4287, 24321.3496,  5191.5757, 10658.5049,\n",
      "        10232.0596,  4921.8262,  6079.6714, 37464.0859, 23258.9980,  1880.4871,\n",
      "         7168.9038,  5214.5659,  8877.5127,  2137.6536,  4345.8423, 22494.5723,\n",
      "         8186.8477,  8520.2656, 14004.3096,  4518.2300, 48673.5586,  2899.4893,\n",
      "         4340.0557,  8902.8799, 19496.7188,  6406.4106,  9370.4639, 40920.5703,\n",
      "         5282.9453,  4826.3560, 11961.2793,  7209.4917, 10370.9121, 15230.3242,\n",
      "         1252.4070,  5028.1465, 16710.0039,  2198.1899, 38392.7461,  4441.2134,\n",
      "        11922.0859,  9095.0684,  8486.9014, 44397.3906, 10621.7578,  4846.9199,\n",
      "         6255.7744, 17748.5059,  6101.8604,  4032.2407, 14254.6084, 16862.8984,\n",
      "         8569.8613,  7620.8408,  1146.7966, 21880.8203])\n",
      "tensor([10796.3506, 11388.3428, 42124.5156, 11661.3965,  9898.4219, 16420.6777,\n",
      "         8410.0469,  7726.8540, 21223.6758, 10825.2539,  6149.6147,  5748.8198,\n",
      "         7281.5054, 14833.3945, 41501.6562, 11197.0967,  7046.7222,  9365.2812,\n",
      "        19023.2598, 10823.2266, 35907.0117,  1534.3044,  6849.0259, 40861.9336,\n",
      "        42969.8516, 32548.3398,  5452.0752,  8563.3232,  6014.0322, 23033.8848,\n",
      "         9874.5771, 17765.9785, 33907.5469, 18306.3945, 11048.9639,  6046.6641,\n",
      "        12265.5068, 47400.9414,  6551.7500, 12592.5342, 42760.5039, 47412.0312,\n",
      "         6426.7280,  7147.1050, 11276.6777, 14988.4316,  5028.1250,  9755.0039,\n",
      "         6405.0771, 41920.0781,  4158.2539, 10792.8076,  9301.8936,  5729.0054,\n",
      "         9166.4688,  8731.4805,  9432.9258,  5150.3169, 13256.1748,  5972.3779,\n",
      "         5301.3931,  6628.2686,  5811.5186, 35069.3750])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1391.5287, 16420.4941,  4721.4297,  1632.0363,  4317.3154,  3375.3062,\n",
      "         9137.7803,  6986.6968,  9490.4531, 11218.9795,  9877.6074, 33475.8164,\n",
      "         5934.3799,  5312.1699,  9304.7021,  9235.0645,  4662.8926, 18972.4941,\n",
      "         9630.3975, 18061.0137, 47403.8789, 11512.4053,  1632.5645,  2264.7219,\n",
      "         4040.4421, 14349.8906, 10763.5752,  4669.0410,  8334.5898,  6746.7427,\n",
      "         2407.0042, 25154.1719, 28403.3711,  2155.6814, 10584.0400,  5241.3555,\n",
      "         6099.9766, 20773.6270,  8026.6665, 30063.5801,  4452.5493,  6948.7007,\n",
      "         9270.5479,  3502.8157, 11268.0127, 11155.0889,  4869.0752,  9535.4424,\n",
      "        19964.7461, 11443.9365, 17929.3027, 46182.8320,  2527.8186,  7729.6455,\n",
      "         9595.4395, 39802.3516,  3947.4131,  4559.5342,  4590.5381,  2020.1770,\n",
      "        41279.2422, 41332.5312, 14643.6221,  5548.1787])\n",
      "tensor([17914.1328,  6712.0068,  5482.2915, 19453.6855,  5920.1040, 23300.7910,\n",
      "         1635.7336, 12479.7090,  6061.5435,  4889.0366, 12646.2070,  4930.2988,\n",
      "         2457.2112, 44501.3984,  8794.9893,  5851.0708,  6770.1924,  4912.5459,\n",
      "         3484.3311,  3875.7341, 28101.3340, 19480.2383,  6891.6792,  9694.4453,\n",
      "         7378.3042, 23632.3750,  5397.6167,  6358.1851,  8891.1396, 20462.9980,\n",
      "        37484.4492, 23261.0215, 10436.0957,  5379.2900, 21774.3223, 16232.8467,\n",
      "         8549.4805, 11674.1299,  7235.6587, 13041.9209, 47273.1094,  6527.7969,\n",
      "         9850.4316, 12649.6045, 11030.4414, 40160.7891, 21982.4746, 11801.3779,\n",
      "         7730.8125, 18765.8750,  6425.8906,  4343.5864,  9553.4902,  6579.8521,\n",
      "         3980.1387, 39722.7461, 17270.3926, 12478.4727, 41999.5195, 12233.8281,\n",
      "        38511.6289, 19305.1934,  5822.7246, 17879.6328])\n",
      "tensor([ 5211.0073, 39871.7031,  9387.7686,  6182.4146,  7731.8579,  8886.0176,\n",
      "         7085.8442,  6186.1270, 14829.3516, 13887.9688, 10950.0391,  5377.0161,\n",
      "         5210.3770, 27107.3418, 13405.3906, 14133.0381,  5515.8096,  6971.5854,\n",
      "        23945.7148,  3213.6221, 11184.4941,  8936.3086, 39000.4609,  2156.7517,\n",
      "         4791.5850,  4559.3188, 11033.6621,  7727.2534, 12129.6143,  1631.8212,\n",
      "         4661.2861,  4012.1570,  5390.6113, 10715.7383,  6765.6182, 29964.4688,\n",
      "         8424.0781, 11015.1748,  4853.6143, 12815.4453,  1629.8335,  3479.0305,\n",
      "         4508.5103,  5620.7578, 12645.1660, 10920.3223,  8653.0039,  9669.7842,\n",
      "        10348.4600,  5003.8530,  2721.3208,  8218.7500, 36910.6094, 37270.1523,\n",
      "         5207.8936,  5966.8872,  5167.4790,  7448.4038, 40103.8906, 18137.9727,\n",
      "         5540.3325, 37829.7227,  7196.8672,  8920.6895])\n",
      "tensor([ 7337.7480, 10991.3584, 17942.1055, 11312.4746, 13224.6934, 12574.0488,\n",
      "         8023.1353,  8609.9229,  5560.0654, 10336.7979, 39983.4258,  9193.8389,\n",
      "        10008.9678, 46412.5352,  8342.9092,  9583.8936, 27941.2871,  6108.3462,\n",
      "        19121.4629,  5014.1821,  8027.9678, 10491.6934, 12146.9707, 27709.7969,\n",
      "         5550.9214,  3615.3975,  9724.5303, 21925.6836,  9283.5615,  2396.0959,\n",
      "         2639.0430,  6673.0005, 11093.9844, 19361.9980,  4260.7441,  6777.4302,\n",
      "         6653.7886,  9610.9561, 21806.1816,  2755.0210, 10381.4785, 10987.3252,\n",
      "        11735.8447, 10602.3848,  8976.1406,  7027.6987])\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "tensor([ 3279.8687, 21454.4941,  1720.3538,  6801.4375, 11946.6260,  7742.1099,\n",
      "        21736.3281,  4916.9531,  5515.8096, 17009.3359, 38433.5234,  8549.1367,\n",
      "        16099.3672,  7492.9854, 18091.3730, 41501.6562,  2661.1912, 42211.1367,\n",
      "        16455.7070,  4433.9160,  6571.5439, 26236.5801,  6318.7979,  5795.9058,\n",
      "        47305.3047, 11488.3174, 29101.7520, 19305.1934, 12479.5273,  7730.7632,\n",
      "        10118.0635,  9579.1553, 38126.2461,  7168.9038,  5482.2915, 47462.8945,\n",
      "         9051.9004,  5383.5361, 17352.4258, 10480.8955, 11945.1328,  8083.1782,\n",
      "         8347.1641, 40018.2305,  6686.4312,  2219.4451,  6662.1387,  7076.8926,\n",
      "         4790.3115,  9434.7314,  7102.4077, 10043.2490, 27533.9121,  5240.7651,\n",
      "         3533.2556,  4673.3921, 48173.3594,  4179.4453,  6133.8828,  9716.7441,\n",
      "         4461.8037,  5148.5527, 13974.4551,  7050.0215])\n",
      "loss: 9133714571264.000000  [   64/ 3630]\n",
      "tensor([18061.0137,  4877.9810,  7726.8540, 19121.4629,  8519.2305,  1769.5316,\n",
      "        18629.5156, 36124.5742, 10601.4121,  5824.7817,  9094.7920, 11743.2988,\n",
      "        25737.9805,  5014.1821,  4914.3413,  3777.2117, 15526.5752,  5241.3555,\n",
      "         4833.8750, 33131.8438,  6797.2192, 18157.8770,  6025.5762,  1121.8739,\n",
      "        10658.5049, 21982.4746, 12485.3594,  6659.9111,  5263.4888, 11611.7080,\n",
      "         7079.3530, 12265.9785,  7147.4727,  8415.5195,  8877.5127,  8303.0703,\n",
      "         2257.4753, 11741.7256,  5093.7124,  7978.5215, 10143.2617,  7672.9897,\n",
      "         5523.9819,  8594.0195,  9254.6621, 12818.6055,  9378.4590, 13099.8857,\n",
      "        19120.2246,  2130.6758, 10019.4902,  4266.1660,  7168.0327,  3500.6123,\n",
      "         2136.8823, 14319.0312,  4921.8262,  6123.5688,  8649.2002, 11107.3457,\n",
      "        14829.3516, 11273.8643, 15413.5967,  7144.8628])\n",
      "tensor([ 6505.3262, 19539.2422,  7325.0483,  7625.1558,  7257.4282, 12154.0332,\n",
      "         9654.1816,  4721.4297, 24890.1055,  1242.2600,  6425.8906,  6837.3687,\n",
      "         3981.9768,  3261.9417,  3471.4097, 19043.1621,  1646.4297, 17693.6172,\n",
      "         1621.3402, 10607.0059, 38792.6875,  9440.7178,  9387.7686, 39783.3828,\n",
      "        23503.1953, 13352.0996,  6309.6631, 46599.1094,  2710.7117, 21190.7207,\n",
      "        22028.7109, 13744.0557,  9360.9238,  4189.1133,  4795.6567,  8534.6719,\n",
      "        45826.9258, 22395.7441,  7027.6987,  2416.9551, 10799.3418, 14984.9824,\n",
      "        10776.0654, 28205.9336, 10226.2842,  4894.7534, 11253.4209, 10422.9170,\n",
      "         8116.6802, 15170.0693,  9632.6973,  4818.7358,  7263.6362,  9890.5205,\n",
      "        12254.8135, 12927.1318, 11370.8428, 13635.6377, 11307.3711,  1737.3760,\n",
      "        13125.3525, 35064.6719,  9091.8574,  3591.4800])\n",
      "tensor([13143.3428,  6009.9824,  3293.2917, 21348.7051, 44213.4219, 16150.7910,\n",
      "         8199.8369, 11881.3584, 26506.6816,  4022.5654, 21223.6758,  5621.9448,\n",
      "         6500.2358,  4977.2495,  5334.7085,  7209.4917, 10107.2207,  5560.0654,\n",
      "         9962.6123,  8280.6230,  9137.7803,  9861.0254,  7371.7720,  4912.5459,\n",
      "         4431.6924,  5699.8374, 10104.5605, 11754.4150, 17448.0000,  9046.0400,\n",
      "        23445.8926,  2867.1196, 16884.9238,  5427.3716, 36808.3750,  3070.8086,\n",
      "         4737.9204,  4889.0366, 13596.5107,  2727.3950, 11439.6240, 13915.5215,\n",
      "         8059.4062,  8835.2646,  9235.0645,  9462.1533, 11335.2959,  3410.3240,\n",
      "        27322.7344,  7987.8325,  7537.1641, 10278.1416,  3866.8552, 38321.4492,\n",
      "        15973.6201,  9878.7461,  8068.1851, 10741.0586,  5080.0962,  4350.5142,\n",
      "         8585.8467, 39774.2773, 12026.0957,  8794.9893])\n",
      "tensor([ 4533.9902,  4835.7505, 12877.6006, 28403.3711,  2473.3340, 17178.6816,\n",
      "         7765.3745,  7631.4805,  5851.0708,  3176.2876, 14478.3301, 16114.3955,\n",
      "        13844.7969, 14002.8252,  6750.4321,  4670.9858, 24393.6230, 13145.5684,\n",
      "         6412.4795,  5318.4067,  5466.6616, 14374.9912, 46200.9844,  9432.9258,\n",
      "         7583.2061,  5033.0952,  4559.5342,  5138.2568,  1627.2825,  6361.5400,\n",
      "         2974.1260,  5484.5405,  1967.0227,  9670.2822, 11674.1299,  4337.7354,\n",
      "        24321.3496, 11015.1846, 20462.9980,  6747.9126, 14007.2217,  8931.1758,\n",
      "        11100.1875, 37079.3711, 11018.9854,  2196.4731,  6579.8521, 30166.6191,\n",
      "        11048.9639,  6660.8281,  5727.1279,  6551.7500,  6414.1782, 16176.0850,\n",
      "        48549.1797,  4685.1455, 44627.9375, 30364.5371,  6143.1284,  4515.1943,\n",
      "        44501.3984,  4265.1128, 17879.6328, 10711.7891])\n",
      "tensor([11842.4424, 28101.3340,  7148.2329,  9520.0020, 10801.6104,  9272.5332,\n",
      "        11906.1250, 38709.1758,  4040.5583,  7624.6299, 11291.3857, 12244.5312,\n",
      "         6496.8862, 10977.2061, 34617.8398, 13019.5244, 10461.9795, 14436.8926,\n",
      "        17765.9785, 24915.2207, 18638.6133, 12846.3525, 18765.8750,  4928.3643,\n",
      "         7160.3081,  7060.2222, 21002.7578,  6358.1851,  4990.2998, 16420.4941,\n",
      "        19120.6562, 10942.1318,  2720.7209,  1515.3448,  9634.1523,  6923.6221,\n",
      "         5632.9487, 17108.6309, 17315.6914,  6951.1992,  6941.6724, 40419.0195,\n",
      "         8233.0977, 33732.6875, 12002.3662, 13937.6660,  4852.9478, 11261.8174,\n",
      "        10577.0869,  3756.6216, 38822.0898, 27082.6641,  8252.2842,  6020.6094,\n",
      "        13616.3584, 12269.6885,  5605.0166,  8342.9092, 10920.3223, 11497.8213,\n",
      "        20641.5938,  9198.3574, 36632.5430,  4561.1885])\n",
      "tensor([ 7789.6348, 19435.9277,  4501.2344,  5452.0752, 11085.5869,  9239.9736,\n",
      "         7152.6714, 18317.6250,  5354.0747, 46220.9961, 25421.9336,  9916.5957,\n",
      "        22606.0645, 11931.1250, 11280.1006,  6415.2651,  6680.3022,  6061.5435,\n",
      "         3925.7583,  5257.5078, 14113.4727, 11044.6406, 20411.7734,  9553.4902,\n",
      "         2155.6814, 14119.6201,  2154.3611, 12495.2910, 11643.1836, 42303.6914,\n",
      "        15412.6855,  4433.2295, 14426.0742,  6358.7764, 21880.8203, 18099.5254,\n",
      "        10967.5176,  3385.3992,  6437.9717,  4909.7246, 28923.1367,  4719.7363,\n",
      "         9880.0684,  5375.0381,  7371.1211,  6325.6953,  9875.6807, 19798.0547,\n",
      "         1986.9333,  5940.5850, 11365.9521,  5186.6426,  1639.5631, 24948.2012,\n",
      "         2719.2798,  8728.9082,  7552.9785, 15456.2949,  7442.3052,  3597.5959,\n",
      "         4326.2134, 13143.3369,  9143.6279, 17270.3926])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8569.8613, 14003.7881,  6353.1885,  9285.9922,  3500.7427, 12533.9229,\n",
      "         9304.7021, 11318.2275,  9301.8936,  8516.8291,  9290.3174,  2102.2646,\n",
      "         6059.1729,  3591.4707,  8920.6895,  4527.1831,  4783.2427,  5267.8184,\n",
      "         8704.1816,  5572.8662, 13430.2646,  3292.5298, 17716.7422,  8603.8232,\n",
      "         6743.1680, 35956.5898, 11316.2148, 47055.5312, 35069.3750, 10550.3281,\n",
      "         4827.9048, 29010.3184, 12347.1719,  1815.8759, 11881.6777,  4053.6936,\n",
      "         3832.0100,  8520.2656,  6393.6035, 26018.9512, 17929.3027,  9138.3857,\n",
      "         6970.0791, 10769.5791,  4740.7705,  9964.0566, 41676.0820,  6330.5752,\n",
      "        11300.3760,  7740.3369,  5631.7529,  4463.2051,  6764.7871,  4200.9316,\n",
      "         9172.7686, 36085.2188, 44445.3477,  6377.8208,  9456.3994, 17352.6797,\n",
      "        11010.6445, 10289.4707, 46182.8320, 16692.1406])\n",
      "tensor([ 4832.6074,  7650.7739, 46151.1250,  6673.0005, 10791.9600,  4134.0825,\n",
      "         5540.3325, 14539.2803, 38800.3867, 10086.8330, 43813.8672,  7331.8779,\n",
      "         8623.3125, 11371.9863,  8930.9346, 10959.3301, 12094.7510, 11455.2803,\n",
      "        48824.4492, 14235.0723, 18861.9668,  1136.3994,  4523.3594,  6088.1855,\n",
      "         1704.7002, 11881.9697, 18903.4922, 42112.2344, 11454.0215,  6891.6792,\n",
      "        30438.9199,  2913.5691,  6164.6860,  5976.8311,  4891.4346, 10652.5762,\n",
      "         4910.7397,  7243.8135,  2566.4707, 19853.7910,  4873.8315,  6182.4146,\n",
      "         3878.7839,  7151.0918,  2801.9724,  9283.5615,  5510.3447,  7378.3042,\n",
      "        18451.5840,  6605.6602, 34672.1484,  5319.6094, 11187.6562, 11196.8926,\n",
      "        24667.4199,  4559.3188,  6586.8999,  1917.3184, 11179.6074,  4372.8345,\n",
      "         3544.2539, 14142.7627, 10381.4785, 20664.3613])\n",
      "tensor([ 6347.0557,  4837.5825, 20099.7363, 19663.3809,  4765.3315,  3352.8745,\n",
      "        11987.7871, 17069.8730,  6289.7549,  4296.2710,  4388.5278, 15022.6826,\n",
      "        36308.0508, 15820.6992, 11218.9795,  8752.8184,  5379.5894,  7640.3091,\n",
      "         3481.8679,  9018.6416, 11268.0127, 47928.0312,  6616.0239, 15408.4268,\n",
      "         3940.8850, 21082.1602,  8596.8281, 10508.9199,  5227.9888, 36197.6992,\n",
      "        14711.7344, 17742.1055,  3531.7224,  6406.4106, 14410.9316, 43871.3320,\n",
      "         5209.5786,  6948.7007, 21472.4785, 34779.6133, 21797.0000,  4719.5239,\n",
      "        10744.9287, 11270.3340, 23781.3574,  5245.2271,  4992.3765,  6572.0107,\n",
      "         6738.9458,  4119.3862,  5581.6836,  7441.5010, 12163.5576,  1635.7336,\n",
      "        15828.8213,  9446.0977, 25729.1855,  7235.6587, 34166.2734,  7890.6313,\n",
      "         8568.6943, 11013.7119,  4076.4971,  3895.4170])\n",
      "tensor([ 7448.4038, 16495.7285,  7049.5171,  1711.0269,  5347.5425, 14254.6084,\n",
      "        48381.3633,  3577.9990,  8563.3232,  3611.9717,  4702.1309, 15010.1758,\n",
      "        11285.7783,  5120.5830,  8815.0703, 13112.6045, 18306.3945, 13952.1602,\n",
      "         6705.0220, 10237.2578, 19493.5508, 21774.3223, 19811.0117, 16173.2686,\n",
      "        39611.7578,  7985.6104,  9409.5928, 11082.5771, 10923.9336,  4960.0352,\n",
      "        41332.5312, 28950.4688, 22462.0430,  7639.4175,  4915.0601, 10602.7920,\n",
      "         4731.0303, 37742.5742, 24180.9336,  1664.9996,  8605.3613, 21978.6777,\n",
      "         9500.5732, 41180.8711, 13717.1807,  7201.7007, 12491.4844, 14239.9189,\n",
      "        25656.5762,  6035.6963, 12574.0488,  7518.0254, 14474.6748, 11023.4434,\n",
      "         1241.5649, 51194.5586,  5790.1455,  5267.5312,  7730.8525,  5551.1509,\n",
      "         9863.4717, 22192.4375, 46130.5273,  2690.1138])\n",
      "tensor([11397.8027, 39725.5195, 23326.9160,  6402.2915,  9390.1387,  5762.3604,\n",
      "        10214.6357,  2534.3938,  5167.4790,  7046.7222, 36021.0117, 11350.5879,\n",
      "         2220.0222,  6697.6084,  6766.8198, 32787.4570,  7730.8125,  1534.3044,\n",
      "        10544.1963, 29964.4688, 12680.1914,  6713.5645, 27043.2266, 36133.6328,\n",
      "         8283.6807,  8595.7656, 47496.4961,  4508.3540, 19252.2188, 15183.2344,\n",
      "         6079.6714,  1704.5681, 24901.6484,  2850.6838, 18648.4219, 19777.6309,\n",
      "        39793.6562,  4544.2349,  6657.9819,  4462.7217, 10823.6777, 18137.9727,\n",
      "        12198.5254, 11986.8477, 34241.5391,  5426.0776,  6196.4482,  6765.6182,\n",
      "        10631.4541,  7475.3062,  9182.0586,  9549.5654, 18218.1621,  6673.6426,\n",
      "        12839.8486, 12479.7090, 24106.9121,  4317.3154, 11856.4111, 11032.7969,\n",
      "        10713.6436, 30942.1914,  5247.9961, 12913.9922])\n",
      "tensor([ 4790.2144,  2782.4663,  4386.8716, 17740.2930, 20773.6270, 13470.8604,\n",
      "        21344.8477, 11212.6543, 14383.3535, 11305.9346, 10826.0000, 22153.5918,\n",
      "        40337.0664, 20009.6328, 12271.1777,  5466.0791, 12079.0439, 47291.0547,\n",
      "        23288.9277,  9298.7139,  7612.2227,  8798.5928, 20633.8691,  8396.6006,\n",
      "        11214.5322,  7210.7290, 17716.6367, 40920.5703, 13289.4209, 32248.7012,\n",
      "         4846.9199,  8027.9678,  3861.2097, 17758.2520, 20088.1133,  9361.3271,\n",
      "        43578.9375, 11072.3447,  6784.1021,  6250.4351, 10680.0986, 15359.1045,\n",
      "        14210.5361,  6849.0259,  4345.8423, 16790.6562, 19144.5762, 11058.1572,\n",
      "         4532.1323, 38168.0039,  4987.0674,  6877.9800,  6660.0488,  6758.4141,\n",
      "        25311.5215, 20177.6719, 11299.3428,  4151.0288, 25332.6934,  5884.6030,\n",
      "         9898.4219,  3882.6570,  3392.9768,  4364.6133])\n",
      "tensor([ 3268.8467, 24881.2305,  9884.6934, 11218.5703,  6785.8169,  5012.6846,\n",
      "         8891.1396,  8978.1855,  5891.5181,  5225.6348,  7729.6455, 12755.5361,\n",
      "         8824.9102,  8152.0234, 10129.2852, 11286.5391,  8718.8525, 44585.4570,\n",
      "         7517.5283, 16570.5996,  1163.4626,  8556.9072,  2480.9790, 18971.7168,\n",
      "         2775.1921, 21318.5898,  5472.4492, 11276.6777, 62592.8750, 11197.0967,\n",
      "        17012.1875,  5209.1787, 11280.4385,  3659.3459,  3227.1211,  8609.9229,\n",
      "         8699.4893, 16297.8457, 10579.7109, 11244.3770,  7193.5654,  9874.5771,\n",
      "         2241.7705, 48673.5586,  8527.5322,  8941.0898, 40273.6445,  5743.7627,\n",
      "         4433.5488,  7345.0840, 11500.3193, 12425.2158,  8743.7490,  8424.0781,\n",
      "         4529.8950,  4676.9346,  5128.0859,  1137.4697,  4508.5103,  4537.2729,\n",
      "        39556.4961, 12346.6543, 24873.3848,  5514.5098])\n",
      "tensor([41003.0234, 30322.2207,  6426.7280,  3906.1270,  9914.9834, 10450.5518,\n",
      "         8765.2490,  8807.7295,  8739.6240,  2104.1133,  1632.0363, 17468.9844,\n",
      "        11460.4990,  4646.7588,  4402.2329, 32351.1719, 21760.0762, 47400.9414,\n",
      "         4753.6367, 18264.6367,  1705.6245,  7316.6323, 10206.4766, 18524.0332,\n",
      "        12235.8389, 33307.5508,  1731.6770, 35585.5742, 12219.4395,  9034.2324,\n",
      "         2643.2686, 37747.6836, 19023.2598,  4900.3213,  7731.4272,  2264.7219,\n",
      "         7935.2910,  1146.7966,  8825.0859, 11657.7188,  9541.6953, 18033.9688,\n",
      "         3350.4404, 36910.6094,  5584.3057, 11680.1318,  4762.3291,  3005.1008,\n",
      "        23746.4551,  1261.8590,  5594.8457,  8442.6670,  4911.9731, 26412.2129,\n",
      "        14481.9785,  2138.0708, 39586.8633,  2457.2112,  9985.4580,  4346.6533,\n",
      "         9788.8662,  9914.8770,  4784.8198,  6875.9609])\n",
      "tensor([ 9595.4395,  4832.7827, 36869.9336,  1824.2854,  7256.7231,  9620.3311,\n",
      "         8713.6016, 24869.8359, 20709.0195, 10338.9316,  4618.5430, 16977.8887,\n",
      "         5969.7231,  6069.8608, 39508.8711,  6986.6968, 36103.1719,  2203.7358,\n",
      "        11015.1748,  1137.0110,  8419.5869,  8964.0605, 10928.8486, 27302.6289,\n",
      "         5514.9214, 36898.7344,  3201.2451, 21677.2832,  4976.8608, 10792.8076,\n",
      "        39871.7031,  1135.9407,  7160.3301, 13489.0400,  5511.5137, 11326.7148,\n",
      "         9490.4531,  9896.7666,  9563.0293,  8844.6348, 11358.5850, 13415.0381,\n",
      "         8211.1006,  4952.9585,  6182.0483, 27724.2891,  4347.0234,  5729.0054,\n",
      "         4949.7588, 18767.7383,  8825.4463, 10698.1123, 47069.8359, 28468.9199,\n",
      "         6268.3389, 18804.7520,  9335.0225,  5518.2974,  5469.8594,  8334.5898,\n",
      "         8938.4619, 18618.7344,  6527.7969,  6338.7256])\n",
      "tensor([40824.5898,  4375.7065,  7629.2109, 16500.1699, 44145.9219, 19124.3691,\n",
      "         4746.3442,  1615.7667,  2927.0647, 18596.5156, 12494.1885, 11436.7383,\n",
      "        11801.3779, 17904.5273, 20781.4883,  2902.9065,  4267.7329, 39000.4609,\n",
      "         8052.0400,  9877.6074, 60021.3984,  9564.2031,  9785.7725, 28954.2754,\n",
      "        10790.3672,  8976.1406,  2585.2690,  9763.2705,  4958.0732, 13123.6904,\n",
      "        12333.8281, 18838.7031,  4008.6965,  4341.8853,  8932.0840, 18435.6348,\n",
      "         8151.5220, 43097.4141,  9841.7959,  6686.3301, 42760.5039,  4466.6216,\n",
      "         4482.7412,  8428.0693,  6878.5137,  9421.6201,  4983.4512, 35391.7617,\n",
      "        15011.1729, 27346.0430,  8975.1846, 10982.5010, 11065.9971,  4765.5669,\n",
      "        12815.4453,  8762.7617, 12148.6709, 12636.3799,  5348.0586,  6658.6318,\n",
      "         1631.6683,  5840.4814,  4816.4673, 17731.7051])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4977.9277, 10079.9287,  2217.6013,  3375.3062, 39109.7539,  6239.0723,\n",
      "        15322.8506, 11538.4209, 11848.1406, 36325.0352, 21259.3789,  7630.3252,\n",
      "         7153.4839,  9385.6670,  5972.3779, 25517.1133, 11658.1152, 25354.4531,\n",
      "        24508.0273,  6344.1738, 14692.6689, 44423.8047, 14224.7549,  5093.8828,\n",
      "        14200.8975, 11048.3555,  4234.9268,  6985.5068,  4705.4697,  8269.0439,\n",
      "         6117.4946,  5345.6807,  5438.7490,  7887.6675, 21103.6543, 18473.0508,\n",
      "        19071.2871, 12830.8213,  2801.2588, 16886.6387, 18972.4941,  9694.4453,\n",
      "        46412.5352,  9566.9912,  5708.8672,  9282.6914, 13483.6182,  4930.2988,\n",
      "         3533.4658, 15403.1816,  6341.8433,  4766.0220, 17179.5215,  4487.3071,\n",
      "        28724.9434, 17515.5254, 26109.3281,  5669.1655,  3693.4280, 24707.1543,\n",
      "         5123.0977, 17129.8613, 11987.1680,  6073.2026])\n",
      "tensor([ 9869.8105,  7968.3511,  8410.0469,  5857.2939, 12655.3779,  5662.2251,\n",
      "        11578.8184,  7576.7598, 18259.2168, 40242.7969,  8797.1270, 27963.1992,\n",
      "         6389.9458, 23757.2539, 42656.6914, 19533.8320,  9602.3350,  5958.8755,\n",
      "        12493.1104, 15554.3066, 13217.0947, 35595.5898, 11265.8232, 13224.0566,\n",
      "         2494.0220,  7165.7222, 28624.6621,  4434.9209, 11200.5312,  4746.4692,\n",
      "         8767.5732,  5395.9326,  7080.1421,  3056.3882,  6375.3955, 14349.8906,\n",
      "         4988.3501,  2498.4143,  4435.0942,  9676.6895,  4456.7559, 25333.3320,\n",
      "         3857.7593, 13019.1611, 29141.3594, 10818.9297, 10336.7979, 13981.8506,\n",
      "        11258.9199, 20745.9883,  2731.8477, 21984.4707, 13126.6777,  7499.3477,\n",
      "        11004.0498, 11481.3750,  4747.0527, 63770.4297,  6360.9937,  8944.1152,\n",
      "        10118.4238, 16577.7793, 12105.3203, 23162.3965])\n",
      "tensor([ 2716.0391, 10766.3447, 17914.1328,  5458.0464,  9473.7930,  7786.8691,\n",
      "         4239.8926, 16420.6777,  3985.0098,  8956.8672, 14006.8447, 30203.5801,\n",
      "         6128.7974, 29158.3398, 18246.4961,  5927.6006,  5320.1294, 10869.6318,\n",
      "        16862.8984, 12020.1104, 48885.1367,  9086.0244, 11289.1094, 27808.7246,\n",
      "        10431.9648,  2967.0117, 38117.9766, 39597.4062, 20630.7500, 15230.3584,\n",
      "        29330.9824,  2689.4954, 11184.4941,  8520.0264,  2362.2290,  4438.2632,\n",
      "         6821.3232, 40103.8906,  9630.5283,  8174.4531,  9101.7979,  2709.1118,\n",
      "        11390.6504, 11723.9111,  6311.9712,  9783.7812, 25309.4883,  4734.0195,\n",
      "         4751.0698,  4536.2588, 11509.6084, 16739.4160,  6662.1602, 13822.8027,\n",
      "         9262.5146,  4853.6143, 41949.2422, 13047.3320, 25891.7871, 14003.8545,\n",
      "        10906.6855,  6704.9863,  5615.3691, 10348.4600])\n",
      "tensor([11576.1299,  5347.0938,  4012.1570,  4832.9170, 13129.6035,  3493.3052,\n",
      "        10987.7705, 10269.4600, 16173.1904, 12153.3301, 10755.5332,  8816.9209,\n",
      "        12478.4727, 12638.1953,  7954.5171, 10838.7627,  8026.6665, 12475.3516,\n",
      "         5483.0806,  4784.0469, 34428.3711,  4709.6787, 14193.6133,  4809.7949,\n",
      "         2709.2439,  8865.4697,  5856.4531, 18820.2930, 11247.5459,  9058.7305,\n",
      "        11408.6787,  4828.5142,  6805.5850, 13887.2041,  8596.4873, 10649.4697,\n",
      "         8615.2998,  1263.2490, 20309.9355,  8968.3301,  1837.2371, 24227.3379,\n",
      "        13473.8389,  6475.8110, 13712.7646,  8598.2676,  6748.5913, 21595.3828,\n",
      "         5261.7490,  1622.1885,  6650.1948,  6453.5278,  8796.3018, 10012.6191,\n",
      "         7533.3374,  7392.6172, 12251.2207, 32548.3398,  5649.7148, 38499.0742,\n",
      "         5654.8184, 11566.3008,  4805.2856, 18237.9766])\n",
      "tensor([20172.1641, 25111.4336,  2400.4021,  8588.3887,  2020.1770, 10702.6426,\n",
      "        24971.7656, 11003.6924, 38239.4102,  3935.1799,  4927.7622,  6572.6685,\n",
      "         6571.0244,  5857.0200,  6733.6309, 16851.3691,  5325.6509, 21098.5547,\n",
      "         5410.2686, 12268.6318, 10560.4922, 10576.4160,  1391.5287,  6603.6201,\n",
      "        10992.1768,  6665.0405,  6123.5933, 16232.8467,  6108.3462,  2585.8506,\n",
      "         9097.2939,  5385.3379,  4058.7124,  9182.1699,  3161.4541, 13429.0352,\n",
      "        16118.8936,  9270.5479, 12646.2070, 11232.4961, 38514.6719,  3206.4915,\n",
      "        14455.6445,  4281.1631,  4645.6904, 43896.3750, 12142.5781, 14988.4316,\n",
      "        11155.0889, 12360.8281,  9239.3896, 10138.4346,  6449.4521, 43254.4180,\n",
      "        28152.8223,  6284.9707, 14004.3096,  8823.2793,  4953.1499,  4714.8999,\n",
      "        40160.7891,  5017.6616,  4340.0557, 27292.3242])\n",
      "tensor([20813.8711,  5368.5508,  6457.8433, 44069.0625, 34203.3672, 58571.0742,\n",
      "        13275.3184,  4472.5229, 47273.1094,  1969.6140,  4670.6118,  5377.8545,\n",
      "         3062.5083,  3904.8750,  5472.7295,  4689.0356, 44790.4922, 11938.2559,\n",
      "        36219.4062, 11115.5176, 10250.4209,  5811.5186,  4767.5073, 14358.3643,\n",
      "         7738.2622, 11346.8027,  4349.4619, 13343.0127, 12916.5557, 41034.2227,\n",
      "         6378.0747,  4957.9004, 11388.3428,  9581.1436, 11049.1953,  2250.8352,\n",
      "         9910.3594,  4707.6753,  4391.6519,  8548.1328,  6414.2407,  5288.9102,\n",
      "         9978.4072, 18836.8555, 31620.0020,  7098.9106, 19530.0527, 47974.2344,\n",
      "         1977.8149,  7494.2979, 11443.9365, 10882.8584, 22494.5723, 11313.6768,\n",
      "         3392.3652,  1131.5066,  9290.1396,  7117.9800,  1628.4709,  6503.9780,\n",
      "        12231.6133,  5000.0771,  8731.4805,  9491.4062])\n",
      "tensor([14064.4277, 11196.6904,  5507.6475, 13390.5586, 11129.3320, 14418.2803,\n",
      "         6999.2749,  6335.7681, 15169.1836, 30184.9375, 18150.9121, 46255.1133,\n",
      "        44641.1992, 14310.4971,  7237.4185,  1909.5275,  9991.0381, 15161.5342,\n",
      "        10370.9121, 10824.4844,  7112.5806,  1725.5869,  8886.0176,  3532.9143,\n",
      "        11512.4053, 23258.9980,  3994.1777, 37662.7617, 10435.0654,  4948.6729,\n",
      "        18707.2246,  6009.3750,  6265.1655,  5930.1489,  6227.6714, 26467.0977,\n",
      "         6728.2705,  8547.6914, 14001.2871, 10879.1729,  4661.2861,  1634.5734,\n",
      "        11013.7930,  4686.3887,  5124.1885, 10812.6650,  3394.4917, 34838.8711,\n",
      "         6013.1274, 32734.1855,  6770.1924,  4527.1177,  5855.9023, 38054.4922,\n",
      "         4441.2134, 13143.8652, 16378.9141,  9590.4580,  2459.7202, 18191.7344,\n",
      "        11520.0996,  8583.2227, 11262.1416, 10436.0957])\n",
      "tensor([ 3943.5955, 28105.2344, 10805.9551, 15648.6582, 11482.6348,  5127.6079,\n",
      "         6648.9106,  9012.5898, 40232.9336, 24904.3164,  4772.9556, 10056.7275,\n",
      "         6238.2979, 12645.1660,  5709.1646,  4158.2539, 11120.1904, 10676.0977,\n",
      "        23300.7910,  5246.0469,  5910.9438,  1826.8430, 10373.2334,  1252.4070,\n",
      "         3353.2839, 42969.8516, 20878.7852,  4822.7959, 38746.3555, 12387.7539,\n",
      "         9441.5166, 14925.9062, 41999.5195, 27107.3418, 24685.0215, 13673.9072,\n",
      "        19453.6855, 23065.4199, 12643.3779,  6032.5845, 10611.9785, 13462.5195,\n",
      "         2156.7517, 37464.0859,  8671.1914, 38511.6289,  5364.4507, 11259.7061,\n",
      "         9644.2529,  4888.4927,  9669.7842,  7960.8682,  5086.6528, 33475.8164,\n",
      "         4674.7974,  6312.1934,  3594.1709,  1625.4337, 11166.6670,  8709.8965,\n",
      "        10148.4678, 13660.8369,  5495.3916, 38245.5938])\n",
      "tensor([ 5793.8735,  8869.2197,  8827.7451, 12523.6045, 11030.4414, 52590.8281,\n",
      "         7949.7476,  4005.4226,  8878.8574, 20341.3789,  8334.4580,  3736.9246,\n",
      "         5150.3169,  8703.4561, 18963.1719,  8486.9014, 14164.9453,  4243.5898,\n",
      "        18772.8262, 42560.4297, 10932.0791, 18444.5488, 10896.6133,  9946.9463,\n",
      "         4571.4131, 13880.9492,  2730.1079, 11820.7783,  5177.4429, 11737.8486,\n",
      "         6611.5747,  5535.8774, 36536.8750, 11555.6875, 11093.9844, 13640.6631,\n",
      "        46460.5859,  1880.0699,  7623.5181, 16405.6543,  9583.7246,  5459.4141,\n",
      "         5836.8037, 39241.4414, 14833.3945,  6521.4097,  4773.0742, 47289.7266,\n",
      "         4518.2300, 10159.4766,  6059.3662, 19257.8496, 12741.1670,  8797.9082,\n",
      "        13352.4766,  9779.9639, 13224.6934, 37284.8203,  3579.8286,  1629.8335,\n",
      "        16069.0850, 19235.8223,  4032.2407,  6799.4580])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20420.6055, 11008.3906, 39802.3516, 11094.9131,  3919.3872, 20135.9785,\n",
      "        11658.3789,  5240.1348,  1708.9258, 13030.3164,  5327.4004,  8782.4688,\n",
      "         6682.4009,  5587.0991, 17526.3105, 13887.9688, 11165.4180, 19749.3828,\n",
      "         7682.6699,  9080.3447, 26392.2598, 11883.8730,  7173.3599,  3309.7925,\n",
      "         4779.6025,  5426.9219, 14451.8350,  6746.7427, 26081.9590, 12112.2822,\n",
      "         5920.1040, 38998.5469, 35907.0117,  6341.7935,  6725.8091, 19496.7188,\n",
      "        12925.8857, 11399.6484,  4826.3560,  8612.3125,  5979.7310,  6272.4771,\n",
      "        26322.8652, 18322.1152,  9132.1406, 47896.7930, 44246.3320, 11961.2793,\n",
      "         2946.0974,  2731.9121,  8584.4238, 11443.1621,  3877.3042, 20624.8047,\n",
      "         2842.7607,  8023.1353,  7348.1421, 14407.8955,  3277.1609, 10231.5000,\n",
      "         5187.8267,  2322.6218, 15795.4727,  6255.7744])\n",
      "tensor([ 4687.7969, 24329.3379, 11394.0654,  8583.3838,  9535.4424, 10024.0820,\n",
      "         5191.5757, 11015.3203,  7467.0039,  4645.9453, 47417.1797,  4590.5381,\n",
      "         7281.5054,  7155.7979, 48316.0078,  7633.7207,  9179.2607, 48517.5625,\n",
      "         5211.0073,  4766.5093,  6338.0757,  4725.9038,  9861.1865,  9903.7754,\n",
      "        40720.5508,  1980.0699,  6933.2422,  8773.6094, 23887.6621,  7626.9932,\n",
      "        25718.6445, 13063.8828,  4989.5703,  7620.8408, 32108.6621,  3213.6221,\n",
      "         6185.3208, 14901.5166, 24096.1113,  3919.9067,  8859.7686,  8278.1475,\n",
      "         3366.6697,  9583.8936, 19040.8770,  5466.0967, 11312.4746,  2254.7966,\n",
      "         8902.8799,  3479.0305, 28443.2227, 10965.4463, 11027.3135,  4133.6416,\n",
      "        11068.2744, 17043.3418,  6282.2349, 18381.5977, 12649.6045, 12829.4551,\n",
      "         5265.0400,  9909.5566,  5377.0161, 12592.5342])\n",
      "tensor([ 5003.8530,  1712.4691,  9236.1387,  5301.3931, 10788.7412,  2352.9685,\n",
      "        26140.3594,  4809.4521,  6216.3872, 13929.6611,  2279.8958,  4113.1367,\n",
      "        16140.9229,  9249.4951, 28340.1895,  9625.9199,  5846.9175,  7742.2729,\n",
      "         8795.0664, 11411.6846,  9337.3311,  2714.4729,  4710.8916,  3234.1348,\n",
      "        36041.9883,  6621.8008,  6455.8628,  6658.1187, 11743.9346,  5390.6113,\n",
      "         2699.5684, 11033.6621,  5970.1597,  7409.4360, 11833.7822,  8162.7163,\n",
      "        10114.5957,  4518.8262, 19370.4336,  8515.7588, 12146.9707,  4452.5493,\n",
      "         5146.2993, 29186.4824, 15230.3242,  5397.6167, 17883.0195,  2117.3389,\n",
      "        40935.6445,  8059.6792,  3077.0955, 10795.9375, 12878.1699, 36189.1016,\n",
      "         4906.4097, 12905.0811,  8704.5488, 12404.8789,  1682.5970,  9202.4727,\n",
      "         8078.5361,  4436.6260,  7418.5220, 17776.6387])\n",
      "tensor([ 5241.4849,  7228.2158,  9471.1426,  4137.5225, 24513.0918,  4449.2339,\n",
      "         4185.0977, 45710.2070, 15179.2051,  2788.0889, 23241.4746,  5125.2158,\n",
      "        17063.4258,  8993.5400,  6488.8950,  5549.3247,  6712.0068,  6772.6304,\n",
      "        12612.1074, 39969.4414, 40142.9062, 23843.5332, 10499.2393,  4294.2544,\n",
      "        21220.4004, 11345.5186,  6640.5449, 23033.8848,  5152.1338,  5031.2695,\n",
      "         6201.7407,  5999.7490, 20296.8633, 24535.6992,  4884.5830,  4761.0718,\n",
      "         6046.6641, 11035.8584, 11554.2236, 26619.4922,  8219.2041,  7175.9937,\n",
      "        11107.4102, 11335.8467, 10096.9697, 14409.9561,  5258.5146,  7323.7349,\n",
      "        10407.0859,  9308.5625,  3502.8157,  8240.5898, 19361.9980, 37701.8750,\n",
      "        10141.1357, 13968.1250, 11212.9551,  4518.1045, 12979.3584,  2396.0959,\n",
      "        11735.8447,  4022.6128, 44241.9023,  5028.1250])\n",
      "tensor([ 9370.4639,  8987.6270, 12430.9531, 12251.3223,  1149.3959, 19988.1250,\n",
      "         5712.3555, 10806.8389,  6516.0615, 11363.2832,  2597.7791,  2134.9016,\n",
      "         9619.6699, 24520.2637, 10602.3848,  5272.1758, 13846.9619, 19521.9688,\n",
      "         9722.9551,  6515.7598,  6088.7759,  3378.9099, 11034.1230,  5136.1411,\n",
      "        37086.6680, 25026.4238, 24635.1055,  4356.1611,  6474.0132, 15006.5791,\n",
      "        17511.0918, 12363.5469,  2904.0879,  3615.3975,  2203.4719,  9222.4023,\n",
      "         3875.7341,  6379.7090, 18775.4609,  1631.8212,  9439.2363, 17150.6855,\n",
      "        11615.2969, 23261.0215,  7222.4863, 49577.6641, 33750.2930,  8827.2100,\n",
      "         5044.3613,  7746.2632,  8716.2393, 12103.8350, 10802.8574,  4237.1265,\n",
      "        18745.6289,  5256.2808, 23104.1172,  9273.9131, 11552.9043, 10841.7090,\n",
      "         2755.0210,  5630.4580,  4510.5015,  7965.5845])\n",
      "tensor([ 7296.7134, 23510.6797,  1261.4420,  6981.8662, 13256.1748, 10072.0547,\n",
      "        46113.5117, 12267.4463, 11232.9414,  4821.6626, 27375.9043, 23352.3184,\n",
      "         7337.7480, 13508.6172, 43943.8750, 15305.1963, 23082.9551, 12629.1660,\n",
      "         5425.0234,  6027.3472,  1633.9618,  9321.8613, 19933.4570,  1906.3583,\n",
      "        10256.5186, 12013.5322, 39047.2852, 12094.4775, 11342.8555,  8871.1514,\n",
      "         5194.6929, 18242.1328,  9723.1592, 36149.4844,  5454.9780,  1702.4553,\n",
      "         6417.0142,  9866.3047, 25678.7793, 23401.3066,  6292.8462, 10950.0391,\n",
      "        11386.5137, 11362.7549, 11908.1914,  6101.8604, 41920.0781, 11975.6758,\n",
      "         7443.6431,  8423.9121, 11356.6611, 42124.5156, 14115.0889,  9447.2500,\n",
      "        32471.2324,  3180.5100, 11967.3867, 38431.8398, 23862.7793, 24876.9414,\n",
      "        11020.7070, 18303.1406, 12474.2529, 11264.5410])\n",
      "tensor([ 4040.4421,  5038.1357, 10978.5791,  9411.0049,  2302.3000, 10992.3867,\n",
      "         8936.3086, 10909.8730,  6764.9863,  2855.4375, 10115.0088,  4944.6729,\n",
      "        21195.8184, 10601.6318, 13405.3906, 36580.2812,  4697.8984, 19528.0117,\n",
      "        47937.2695, 12957.1182, 14313.8467,  8716.3652,  9821.0898, 10887.2500,\n",
      "        47916.9648, 13747.8721,  5550.9214, 11798.5205, 12044.3418, 39983.4258,\n",
      "         1141.4451, 13502.7852, 12100.5918, 14194.9961,  7262.3726, 25274.0840,\n",
      "         5748.8198,  8591.2148,  3987.9260, 10987.3252,  2020.5522, 11250.8926,\n",
      "        10989.1992, 21533.6562, 16218.6719,  4402.4639, 10232.0596, 13451.1221,\n",
      "        20144.6738, 10992.3857,  9858.0254, 10325.2061,  6405.0771,  7358.1758,\n",
      "         6956.8833,  6099.9766,  6753.0381, 19080.2051, 32584.9629, 11879.1045,\n",
      "         8937.6973, 10991.3584,  6666.0552,  5313.6807])\n",
      "tensor([12558.4961,  3847.6741,  5829.9429, 27709.7969,  9453.6953, 41817.2773,\n",
      "         9634.5381, 43921.1836,  2281.3782,  8457.8184, 24956.6738,  9704.6680,\n",
      "        10451.2744, 20155.6504,  2680.9492, 17879.2539, 37270.1523,  4836.1221,\n",
      "        14201.5137,  6961.4736,  4260.7441,  6781.3540,  9172.3047,  5207.8936,\n",
      "        10961.7842,  5114.4463, 45008.9570,  9668.9141,  7445.9180, 21112.5098,\n",
      "         2789.0574, 34969.1055, 17878.9004,  9377.9043,  4805.6055,  2201.0972,\n",
      "        10961.4395,  8727.1738,  5142.2842, 11475.5439, 40974.1641,  6653.7886,\n",
      "         8444.4736,  8310.8389,  3208.7871,  6526.5928, 21232.1816,  6313.7588,\n",
      "         7147.1050,  4922.9160, 34393.4492, 16115.3047, 29918.5391,  9046.3711,\n",
      "         5488.2622, 18129.7715, 45610.9219, 10763.5752,  7526.7065,  5204.4038,\n",
      "        18944.2148,  9800.8887,  3914.3958,  6593.5083])\n",
      "tensor([ 5400.9805,  2217.4692, 14643.6221,  7228.7710, 17550.7637, 11534.8730,\n",
      "         6429.9795,  6235.9092,  2137.6536,  5313.0581,  5210.3770,  9166.4688,\n",
      "        19658.3535,  1877.9294,  6787.5308,  4618.7397,  8971.3008, 23632.3750,\n",
      "        11168.2471,  1728.8970,  8017.0610, 19107.7793,  9230.8125,  2128.4312,\n",
      "        11145.7910, 12982.8750,  8635.2988,  6604.7139,  2055.3250,  8591.3857,\n",
      "         6751.0796,  3536.5894,  6607.4302,  8753.9961,  5002.7827, 11433.1113,\n",
      "         5244.6787, 11016.3340,  7150.3506,  9778.3477, 39245.1641,  1639.5631,\n",
      "        10355.6406, 10680.1963,  7727.2534,  9781.5039,  1744.4650, 44397.3906,\n",
      "         4931.6470, 19515.5410, 10008.9678, 17081.0801, 46247.9648, 10922.4590,\n",
      "         5257.6685, 18806.1445,  5119.3516,  8733.2295, 11281.5967,  8526.8809,\n",
      "        11842.6240, 11298.1533,  3537.7029, 10740.0205])\n",
      "tensor([ 4768.7769, 17984.5977,  5028.1465, 30063.5801, 20335.8613,  2899.4893,\n",
      "        11008.7568, 42000.0352,  4708.9570,  9386.1611, 44400.4062,  1711.3367,\n",
      "         8697.2246, 12233.8281,  6750.3789,  6652.5288, 22493.6602,  2457.5020,\n",
      "         2198.1899, 22198.8750, 10976.2461, 10491.6934,  4667.6074, 48885.2422,\n",
      "        10806.4873,  9193.8389, 10537.9121,  8953.5049, 41097.1602,  6270.9673,\n",
      "         8047.2007,  9036.5869, 11117.1660, 10797.3359,  8988.1592,  6738.0117,\n",
      "        13073.8184, 12904.9053,  2750.1782,  5495.3623, 12635.6611, 14923.2422,\n",
      "         5525.4204,  7148.5503,  7634.4023, 46544.0820, 23945.7148, 14004.6660,\n",
      "        10959.6943,  3761.2920,  1674.6323,  6123.9985, 10621.7578, 12476.7812,\n",
      "         3526.3516, 19964.7461, 17663.1445,  6585.8774, 38169.4375, 14289.9834,\n",
      "         8583.9736, 54108.4180,  4892.4019,  9391.3457])\n",
      "tensor([15003.8145, 11922.0859,  3310.4224, 46718.1641,  1972.9500,  4883.8662,\n",
      "         2353.8313, 17560.3789,  1712.2271,  2527.8186,  3535.1780, 28039.5391,\n",
      "         5390.6162,  2207.6975,  9636.7324, 12557.6055,  4415.1587, 10594.5020,\n",
      "        19444.2656,  6677.5752,  4830.6299, 14043.4766,  8653.0039,  1621.8827,\n",
      "         6990.8960,  2150.4690,  4472.8804,  2261.5688,  8549.4805, 11837.1602,\n",
      "         5406.6313,  6403.4287,  5620.7578, 14051.1152,  7265.7026,  5379.2900,\n",
      "         8186.8477, 27218.4375,  1694.7964, 22218.1152,  7443.8477,  4801.4756,\n",
      "        26823.7402,  8766.9248,  9313.5957, 13457.9609,  3956.0715, 13936.8818,\n",
      "        47437.5430,  6658.3477, 16753.3984, 36397.5742,  6186.1270,  9820.2334,\n",
      "        17496.3066,  4274.2217, 18817.6211,  8712.5498,  9288.0264, 11272.3311,\n",
      "         4149.7358, 18415.4375,  6364.9355,  8606.1201])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20277.8066,  4320.4106, 11228.0234,  7609.9360, 26993.5117, 18823.5957,\n",
      "        16796.4121,  5116.5005, 19129.8770,  5062.8535, 10986.4014, 12222.8984,\n",
      "        10782.4883, 19726.1484,  4921.7368, 12648.7031, 11314.8672, 23514.5645,\n",
      "         6071.1353,  4756.5337, 11661.3965,  2639.0430,  3757.8447,  9523.9785,\n",
      "        12644.5889,  7724.7695, 10715.7383, 14571.8906, 10334.8125, 10991.8525,\n",
      "        11372.7080, 14453.7627, 10313.1445, 42856.8398, 10796.3506, 10090.1299,\n",
      "        19431.9727,  9365.2812,  8124.4082, 38392.7461, 21491.9805, 13844.5059,\n",
      "         8699.7979,  5271.1968, 26926.5137, 13555.0049,  9597.2881, 21828.0293,\n",
      "        15181.1689,  8601.3291,  3281.9583, 14154.8193,  5836.5205,  9163.1084,\n",
      "        24058.4453,  7161.5146, 11353.2275, 38092.4102,  3393.5928, 11163.5684,\n",
      "         9813.9365,  4917.9258,  4611.5186,  8587.1279])\n",
      "tensor([ 8678.6367,  4669.0410,  2407.0042, 12890.0576, 42705.1836,  4889.9995,\n",
      "        11339.5977, 18223.4512, 35491.6406, 13770.0977,  1708.0013,  9086.5273,\n",
      "        18232.0332, 18955.2207,  3484.3311,  6748.7524, 14283.4590, 17593.3750,\n",
      "         7688.3145, 20167.3359, 29762.3105, 12323.9355,  9210.7949, 36036.2812,\n",
      "         5451.4233,  8989.9814, 10882.7529, 13342.3301,  7372.1353,  6684.9663,\n",
      "        26125.6738,  3736.4646,  4752.7188,  8302.5361, 10952.2207, 13041.9209,\n",
      "         8688.8584, 10446.0498, 19434.5449,  7441.0532,  9074.2441,  5702.2310,\n",
      "        13675.2305,  5871.2061,  9450.8184, 13204.2861, 12265.8857,  3369.4167,\n",
      "        45841.4688,  5398.8911,  3522.6023, 12142.8438, 12629.8965,  9095.0684,\n",
      "        23306.5469,  7153.5537, 24499.0840, 17578.2148, 46217.6641, 14799.0312,\n",
      "         2721.3208,  4853.6582, 11247.4453,  6628.2686])\n",
      "tensor([ 5889.1040,  6788.5342, 12401.7461, 10376.2949,  6658.2373,  7705.1616,\n",
      "         9141.2588,  6674.1318, 15019.7598, 26875.1562,  5227.2632,  9655.7354,\n",
      "         8734.1172,  9143.5732,  1632.5645,  9755.0039,  3404.4211, 10092.3564,\n",
      "         8107.7363, 10264.4424,  5312.5171,  7121.3081,  4869.0752,  3021.8091,\n",
      "        38310.2305,  4360.3711, 10451.9365, 38258.9219, 40941.2852,  8624.3711,\n",
      "         5214.5659, 16710.0039,  2523.1694,  2200.8308, 23807.2402,  5050.9492,\n",
      "         4708.5298,  4745.0073,  9931.6240, 37484.4492,  6286.6333, 16776.3047,\n",
      "         8116.2690,  9921.9004, 47403.8789, 27000.9844, 11176.2168, 18328.2383,\n",
      "        39722.7461,  5312.1699,  5757.4136,  6258.0811, 11038.9854, 14001.1338,\n",
      "        17844.1328, 11070.5352,  3238.4358,  9748.9102, 10929.7871,  8217.4004,\n",
      "        37465.3438, 14744.0938, 12030.7793,  5221.3887])\n",
      "tensor([11537.5742,  4297.9951, 43223.4648, 42661.0117, 34593.8672, 10015.4395,\n",
      "         8583.3223,  6716.7861,  4788.8638,  6417.5674, 40690.5430, 11891.9922,\n",
      "         6655.1758,  1748.7740, 10825.2539, 10751.9248, 10726.4121, 23552.6621,\n",
      "        14109.8145, 37607.5273,  9724.5303,  4343.5864, 46199.6367, 46889.2617,\n",
      "         3353.4702,  5465.4326, 19593.2031, 26730.3984,  5934.3799,  8070.9092,\n",
      "        10594.2256,  5412.7876, 28287.8984, 10584.0400, 10493.9453,  4791.5850,\n",
      "         4863.7627, 18866.6875,  6018.9741,  3443.0640, 10897.6035,  6753.0000,\n",
      "         2210.1880,  7731.8579,  7162.0122, 36837.4688, 20334.6172, 12797.2100,\n",
      "         2221.5645, 16450.8945, 15817.9854, 11193.2080, 19480.2383,  9630.3975,\n",
      "         4934.0996,  6203.9019, 17101.9160,  6014.0322,  6416.2925, 48675.5195,\n",
      "        17500.2051,  9104.3770,  6457.3037, 11540.8369])\n",
      "tensor([19350.3691,  5012.4712,  7196.8672,  9957.4824, 13140.4922, 12757.7354,\n",
      "         6940.9097,  6971.5854, 12656.7852, 28476.7344,  5548.1787, 12835.6465,\n",
      "         6718.9780, 42983.4570,  5377.4580,  5966.8872, 30284.6426, 17942.1055,\n",
      "         7400.1914, 11366.3506,  6726.0503, 38711.0000, 12032.3262,  5898.1665,\n",
      "        25382.2969, 13648.6016, 25147.0195,  9850.4316,  3554.2029,  9485.2559,\n",
      "         6755.6372, 38349.7500, 40861.9336,  9411.1689,  7441.2729,  3591.6248,\n",
      "         5272.6763, 47412.0312,  9610.9561,  9979.2393, 11200.0918, 24671.6641,\n",
      "         1839.4099, 25081.7676,  7077.1895,  8592.1816, 10761.4844,  8564.4863,\n",
      "        18650.8203, 15731.3271, 11073.1758, 14256.1924,  4307.1519, 43753.3359,\n",
      "         5983.9121,  2483.7361,  5824.1021,  9174.1357,  6211.9331,  8083.9199,\n",
      "         6688.9746, 12838.9932,  5809.5537,  9584.8838])\n",
      "tensor([16417.6914,  1981.5819,  6330.9521, 19085.0898,  6350.9326, 12609.8867,\n",
      "         8218.7500, 11170.6738,  6756.5708, 21308.4199,  2331.5190,  4986.4912,\n",
      "         5238.0103, 17434.5801,  7421.1943,  5599.7192, 37133.8984, 10278.5635,\n",
      "        12112.9785, 19892.5918,  6475.1812,  5243.5674,  4500.3394, 19069.4746,\n",
      "        10274.5830,  6761.2153, 24476.4785,  4428.8877, 19594.8105,  5282.9453,\n",
      "         5150.5874,  4738.2681,  5455.4463,  4853.2388, 17626.2402,  5602.9517,\n",
      "        16657.7168, 45863.2031, 29523.1660,  6356.2705,  5151.9502, 33900.6523,\n",
      "        11226.9932, 11830.6074,  5003.3179,  7512.2671, 27037.9141,  2211.1309,\n",
      "         1727.7850, 15439.3281,  3850.3679, 22478.5996,  8551.3467,  4793.2402,\n",
      "         9872.7012,  6184.2993, 20630.2832, 46475.2383, 43287.3750, 13725.4717,\n",
      "         8732.0859,  8792.4785,  9254.5244,  8662.5176])\n",
      "tensor([ 3766.8838,  7804.1606,  3972.9248,  9516.3633, 27117.9941,  6777.4302,\n",
      "        20149.3223,  6548.1948, 41279.2422, 14768.0205, 11568.4766, 11300.4541,\n",
      "        12265.5068,  5527.3135, 12387.4072, 34472.8398,  6664.6860, 45702.0234,\n",
      "         9393.3584, 22412.6484,  9948.9697, 10720.6953,  4708.3374,  4522.0054,\n",
      "        10736.8711, 26489.5762,  6093.4038,  5822.7246,  9964.0596,  5415.6611,\n",
      "         4433.7495,  8769.6445, 32905.8164, 15238.4053,  8965.7959,  5185.2397,\n",
      "        10065.4131,  6763.1831, 24919.3574,  4846.9849,  8517.6445, 12129.6143,\n",
      "        10807.3350, 11664.4541,  6604.6367, 12731.0000,  6657.5737, 22060.1719,\n",
      "         7085.8442,  6832.8516,  6677.3218,  7671.1797, 36023.1484, 46661.4414,\n",
      "         9642.2871,  7810.5654,  5266.3657,  4929.9517,  8559.9883, 30269.4277,\n",
      "        10390.9131, 11504.0811,  3398.1965,  9873.8760])\n",
      "tensor([ 1242.8160, 27626.1797, 18608.2617,  9950.3486,  3980.1387, 11854.6738,\n",
      "         3704.3545, 13670.9150, 13393.7559, 22331.5664, 10693.0615, 36225.4023,\n",
      "        10600.5479,  6710.1919,  2395.1716, 19524.6582, 46145.6523, 17748.5059,\n",
      "         2404.7339,  6663.0903, 15746.4619, 28058.4707, 46654.7266, 21659.9297,\n",
      "         6543.0454, 13919.8232,  8823.9854,  7985.8149, 41919.0977, 13994.2930,\n",
      "         8491.9072,  3176.8159, 27941.2871, 23244.7910, 13065.5439,  7156.5244,\n",
      "        39532.2461,  9414.9199, 33907.5469,  7206.2949, 14133.0381,  2045.6853,\n",
      "         6389.8535, 11950.1514, 10106.1338,  4895.6797, 41661.6016,  4318.3760,\n",
      "         7034.8438,  3556.9224, 17174.6816, 10823.2266,  6211.3403, 14249.3955,\n",
      "        11674.8730,  6105.5820, 18310.7422,  4292.8584,  8277.5234,  4743.8374,\n",
      "         6112.3530,  6149.6147, 10751.5498,  9047.6494])\n",
      "tensor([21925.6836,  4357.0435,  8595.2559, 11884.0488, 12874.1396, 55135.4023,\n",
      "         3393.3564, 10746.0781,  3855.9429,  5859.6704, 10240.5527,  5929.4653,\n",
      "        24915.0469,  7222.7861,  4772.8760,  6775.9609, 46988.3516, 12950.0713,\n",
      "        16085.1279,  4698.2017,  4340.4409,  2741.9480,  1880.4871,  4934.7051,\n",
      "         7045.4990, 14049.8330,  3044.2134, 10563.2197, 23582.4688,  5253.5239,\n",
      "        37829.7227,  1964.7800,  5926.8462, 12491.8818, 44202.6523,  8553.9541,\n",
      "         8712.3389, 44061.4805,  6322.3701,  5008.2500, 16586.4980,  8827.5264,\n",
      "         4662.8926, 13228.8467,  9095.5312, 11396.9004,  4285.9956, 25154.1719,\n",
      "        39385.6328,  6198.7520, 10787.6904, 10832.7705, 11130.3535, 13557.7188,\n",
      "         3947.4131,  1526.3120, 33353.5000, 39836.5195,  5210.1001, 11111.9248,\n",
      "        19214.7051,  5257.9595, 23563.0156, 40932.4297])\n",
      "tensor([42284.9180, 28868.6641,  2026.9741, 18989.3652, 21806.1816, 11355.8174,\n",
      "        11185.9141, 39727.6133,  4894.7534,  4779.6025,  6216.3872,  8844.6348,\n",
      "        16776.3047,  5425.0234, 13846.9619, 17593.3750,  2689.4954, 10976.2461,\n",
      "         9884.6934, 41949.2422,  9138.3857,  8807.7295, 16796.4121,  8559.9883,\n",
      "        39586.8633,  3526.3516,  8716.2393, 12905.0811,  6664.6860, 19071.2871,\n",
      "        10431.9648,  6775.9609,  9948.9697, 41180.8711, 13393.7559,  8598.2676,\n",
      "         5327.4004,  4833.8750, 20277.8066,  1137.0110, 22218.1152, 13145.5684,\n",
      "        10250.4209,  6697.6084,  5257.5078, 48173.3594,  7625.1558, 11837.1602,\n",
      "         3234.1348, 11833.7822,  5427.3716,  2585.8506, 12838.9932, 11018.9854,\n",
      "        18817.6211, 36189.1016,  6674.1318,  8792.4785, 20630.2832,  4709.6787,\n",
      "         9046.3711,  6272.4771, 15554.3066, 11068.2744])\n",
      "tensor([43753.3359,  2480.9790,  6684.9663,  4909.7246, 12533.9229,  8709.8965,\n",
      "         5000.0771,  6728.2705, 12491.4844,  3268.8467, 28868.6641,  8591.2148,\n",
      "         9182.0586, 12235.8389,  6112.3530, 11048.3555,  6322.3701,  4297.9951,\n",
      "         8116.2690,  4234.9268,  2352.9685,  6648.9106, 15439.3281, 17844.1328,\n",
      "         4752.7188, 18232.0332,  4350.5142,  4449.2339,  5654.8184,  2362.2290,\n",
      "         5347.5425, 18804.7520,  5257.9595,  5050.9492, 10824.4844,  8588.3887,\n",
      "        10991.8525,  2731.8477, 16739.4160,  6837.3687, 17740.2930,  8595.2559,\n",
      "         4766.5093,  4267.7329,  8816.9209, 11353.2275,  4747.0527, 14410.9316,\n",
      "        25729.1855,  6961.4736, 40941.2852, 14064.4277,  4058.7124, 39109.7539,\n",
      "         2680.9492,  9946.9463, 21677.2832,  5345.6807, 13508.6172,  7633.7207,\n",
      "         4294.2544, 19539.2422,  2045.6853,  4766.0220])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6196.4482,  3005.1008,  5605.0166,  9878.7461,  9462.1533,  4784.8198,\n",
      "         4266.1660,  9783.7812, 17352.6797, 62592.8750,  6009.9824, 34166.2734,\n",
      "         9172.7686,  6616.0239, 10090.1299, 17716.6367, 34393.4492, 29523.1660,\n",
      "         2775.1921,  8347.1641,  5240.7651,  7050.0215,  6571.0244, 12638.1953,\n",
      "        11013.7119,  9222.4023, 10373.2334, 11460.4990,  3847.6741, 17009.3359,\n",
      "        14249.3955,  5288.9102, 16455.7070,  9360.9238,  4977.2495, 15169.1836,\n",
      "        38239.4102,  4360.3711,  6764.7871,  6117.4946, 12485.3594, 11555.6875,\n",
      "        44246.3320,  9962.6123,  6093.4038,  6415.2651, 17434.5801, 20099.7363,\n",
      "         9748.9102,  7201.7007,  2302.3000, 12360.8281,  4827.9048, 18091.3730,\n",
      "         5093.8828, 24956.6738,  5415.6611,  5857.2939, 42983.4570, 10787.6904,\n",
      "        18823.5957,  7475.3062,  3985.0098, 12493.1104])\n",
      "tensor([ 6680.3022, 11335.8467, 13352.4766, 19663.3809, 10118.4238, 14235.0723,\n",
      "        18444.5488,  8553.9541, 10148.4678,  6361.5400,  6738.9458, 37747.6836,\n",
      "        12491.8818, 46151.1250,  4040.5583,  5400.9805, 18303.1406,  5080.0962,\n",
      "        14224.7549,  8938.4619, 11168.2471, 11170.6738,  9321.8613, 10942.1318,\n",
      "        11228.0234, 39611.7578, 12644.5889, 18317.6250,  8732.0859, 13483.6182,\n",
      "         7441.5010,  4805.6055, 13770.0977,  8798.5928, 18629.5156, 25333.3320,\n",
      "        13143.8652,  4884.5830,  9875.6807, 19533.8320,  3398.1965,  8825.0859,\n",
      "        10959.3301,  2967.0117, 10019.4902, 18861.9668,  9716.7441,  4433.2295,\n",
      "        19530.0527,  4133.6416, 30166.6191,  5253.5239,  9046.0400, 11316.2148,\n",
      "         6088.7759,  5927.6006, 18310.7422, 23162.3965, 14692.6689,  9174.1357,\n",
      "        10806.8389, 11386.5137, 10159.4766, 11615.2969])\n",
      "tensor([11100.1875, 40242.7969,  7730.8525, 11891.9922,  4435.0942,  9861.0254,\n",
      "         3227.1211,  8116.6802,  8162.7163,  3404.4211,  9290.3174,  8797.9082,\n",
      "        13844.7969,  9236.1387,  6360.9937, 12254.8135, 39245.1641,  6705.0220,\n",
      "        11554.2236, 23757.2539,  6999.2749, 18157.8770,  7742.2729, 38310.2305,\n",
      "        15413.5967,  5702.2310, 10812.6650,  2102.2646,  6356.2705,  5466.0967,\n",
      "        13140.4922, 29010.3184,  1137.4697,  2026.9741,  4888.4927,  7935.2910,\n",
      "         4832.9170,  5271.1968, 13880.9492,  7098.9106,  4809.7949,  3385.3992,\n",
      "         4243.5898,  9453.6953,  3471.4097,  7147.4727, 19107.7793, 13073.8184,\n",
      "         8869.2197,  6338.0757, 16297.8457,  9921.9004, 45863.2031,  3293.2917,\n",
      "         6088.1855, 10711.7891,  3261.9417,  4687.7969,  6933.2422, 12635.6611,\n",
      "         6128.7974,  8107.7363,  4510.5015,  9634.5381])\n",
      "tensor([ 1149.3959, 44213.4219,  4461.8037,  7345.0840, 44641.1992,  1837.2371,\n",
      "        17174.6816, 10422.9170,  6403.4287, 24321.3496,  5191.5757, 10658.5049,\n",
      "        10232.0596,  4921.8262,  6079.6714, 37464.0859, 23258.9980,  1880.4871,\n",
      "         7168.9038,  5214.5659,  8877.5127,  2137.6536,  4345.8423, 22494.5723,\n",
      "         8186.8477,  8520.2656, 14004.3096,  4518.2300, 48673.5586,  2899.4893,\n",
      "         4340.0557,  8902.8799, 19496.7188,  6406.4106,  9370.4639, 40920.5703,\n",
      "         5282.9453,  4826.3560, 11961.2793,  7209.4917, 10370.9121, 15230.3242,\n",
      "         1252.4070,  5028.1465, 16710.0039,  2198.1899, 38392.7461,  4441.2134,\n",
      "        11922.0859,  9095.0684,  8486.9014, 44397.3906, 10621.7578,  4846.9199,\n",
      "         6255.7744, 17748.5059,  6101.8604,  4032.2407, 14254.6084, 16862.8984,\n",
      "         8569.8613,  7620.8408,  1146.7966, 21880.8203])\n",
      "tensor([10796.3506, 11388.3428, 42124.5156, 11661.3965,  9898.4219, 16420.6777,\n",
      "         8410.0469,  7726.8540, 21223.6758, 10825.2539,  6149.6147,  5748.8198,\n",
      "         7281.5054, 14833.3945, 41501.6562, 11197.0967,  7046.7222,  9365.2812,\n",
      "        19023.2598, 10823.2266, 35907.0117,  1534.3044,  6849.0259, 40861.9336,\n",
      "        42969.8516, 32548.3398,  5452.0752,  8563.3232,  6014.0322, 23033.8848,\n",
      "         9874.5771, 17765.9785, 33907.5469, 18306.3945, 11048.9639,  6046.6641,\n",
      "        12265.5068, 47400.9414,  6551.7500, 12592.5342, 42760.5039, 47412.0312,\n",
      "         6426.7280,  7147.1050, 11276.6777, 14988.4316,  5028.1250,  9755.0039,\n",
      "         6405.0771, 41920.0781,  4158.2539, 10792.8076,  9301.8936,  5729.0054,\n",
      "         9166.4688,  8731.4805,  9432.9258,  5150.3169, 13256.1748,  5972.3779,\n",
      "         5301.3931,  6628.2686,  5811.5186, 35069.3750])\n",
      "tensor([ 1391.5287, 16420.4941,  4721.4297,  1632.0363,  4317.3154,  3375.3062,\n",
      "         9137.7803,  6986.6968,  9490.4531, 11218.9795,  9877.6074, 33475.8164,\n",
      "         5934.3799,  5312.1699,  9304.7021,  9235.0645,  4662.8926, 18972.4941,\n",
      "         9630.3975, 18061.0137, 47403.8789, 11512.4053,  1632.5645,  2264.7219,\n",
      "         4040.4421, 14349.8906, 10763.5752,  4669.0410,  8334.5898,  6746.7427,\n",
      "         2407.0042, 25154.1719, 28403.3711,  2155.6814, 10584.0400,  5241.3555,\n",
      "         6099.9766, 20773.6270,  8026.6665, 30063.5801,  4452.5493,  6948.7007,\n",
      "         9270.5479,  3502.8157, 11268.0127, 11155.0889,  4869.0752,  9535.4424,\n",
      "        19964.7461, 11443.9365, 17929.3027, 46182.8320,  2527.8186,  7729.6455,\n",
      "         9595.4395, 39802.3516,  3947.4131,  4559.5342,  4590.5381,  2020.1770,\n",
      "        41279.2422, 41332.5312, 14643.6221,  5548.1787])\n",
      "tensor([17914.1328,  6712.0068,  5482.2915, 19453.6855,  5920.1040, 23300.7910,\n",
      "         1635.7336, 12479.7090,  6061.5435,  4889.0366, 12646.2070,  4930.2988,\n",
      "         2457.2112, 44501.3984,  8794.9893,  5851.0708,  6770.1924,  4912.5459,\n",
      "         3484.3311,  3875.7341, 28101.3340, 19480.2383,  6891.6792,  9694.4453,\n",
      "         7378.3042, 23632.3750,  5397.6167,  6358.1851,  8891.1396, 20462.9980,\n",
      "        37484.4492, 23261.0215, 10436.0957,  5379.2900, 21774.3223, 16232.8467,\n",
      "         8549.4805, 11674.1299,  7235.6587, 13041.9209, 47273.1094,  6527.7969,\n",
      "         9850.4316, 12649.6045, 11030.4414, 40160.7891, 21982.4746, 11801.3779,\n",
      "         7730.8125, 18765.8750,  6425.8906,  4343.5864,  9553.4902,  6579.8521,\n",
      "         3980.1387, 39722.7461, 17270.3926, 12478.4727, 41999.5195, 12233.8281,\n",
      "        38511.6289, 19305.1934,  5822.7246, 17879.6328])\n",
      "tensor([ 5211.0073, 39871.7031,  9387.7686,  6182.4146,  7731.8579,  8886.0176,\n",
      "         7085.8442,  6186.1270, 14829.3516, 13887.9688, 10950.0391,  5377.0161,\n",
      "         5210.3770, 27107.3418, 13405.3906, 14133.0381,  5515.8096,  6971.5854,\n",
      "        23945.7148,  3213.6221, 11184.4941,  8936.3086, 39000.4609,  2156.7517,\n",
      "         4791.5850,  4559.3188, 11033.6621,  7727.2534, 12129.6143,  1631.8212,\n",
      "         4661.2861,  4012.1570,  5390.6113, 10715.7383,  6765.6182, 29964.4688,\n",
      "         8424.0781, 11015.1748,  4853.6143, 12815.4453,  1629.8335,  3479.0305,\n",
      "         4508.5103,  5620.7578, 12645.1660, 10920.3223,  8653.0039,  9669.7842,\n",
      "        10348.4600,  5003.8530,  2721.3208,  8218.7500, 36910.6094, 37270.1523,\n",
      "         5207.8936,  5966.8872,  5167.4790,  7448.4038, 40103.8906, 18137.9727,\n",
      "         5540.3325, 37829.7227,  7196.8672,  8920.6895])\n",
      "tensor([ 7337.7480, 10991.3584, 17942.1055, 11312.4746, 13224.6934, 12574.0488,\n",
      "         8023.1353,  8609.9229,  5560.0654, 10336.7979, 39983.4258,  9193.8389,\n",
      "        10008.9678, 46412.5352,  8342.9092,  9583.8936, 27941.2871,  6108.3462,\n",
      "        19121.4629,  5014.1821,  8027.9678, 10491.6934, 12146.9707, 27709.7969,\n",
      "         5550.9214,  3615.3975,  9724.5303, 21925.6836,  9283.5615,  2396.0959,\n",
      "         2639.0430,  6673.0005, 11093.9844, 19361.9980,  4260.7441,  6777.4302,\n",
      "         6653.7886,  9610.9561, 21806.1816,  2755.0210, 10381.4785, 10987.3252,\n",
      "        11735.8447, 10602.3848,  8976.1406,  7027.6987])\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "tensor([ 3279.8687, 21454.4941,  1720.3538,  6801.4375, 11946.6260,  7742.1099,\n",
      "        21736.3281,  4916.9531,  5515.8096, 17009.3359, 38433.5234,  8549.1367,\n",
      "        16099.3672,  7492.9854, 18091.3730, 41501.6562,  2661.1912, 42211.1367,\n",
      "        16455.7070,  4433.9160,  6571.5439, 26236.5801,  6318.7979,  5795.9058,\n",
      "        47305.3047, 11488.3174, 29101.7520, 19305.1934, 12479.5273,  7730.7632,\n",
      "        10118.0635,  9579.1553, 38126.2461,  7168.9038,  5482.2915, 47462.8945,\n",
      "         9051.9004,  5383.5361, 17352.4258, 10480.8955, 11945.1328,  8083.1782,\n",
      "         8347.1641, 40018.2305,  6686.4312,  2219.4451,  6662.1387,  7076.8926,\n",
      "         4790.3115,  9434.7314,  7102.4077, 10043.2490, 27533.9121,  5240.7651,\n",
      "         3533.2556,  4673.3921, 48173.3594,  4179.4453,  6133.8828,  9716.7441,\n",
      "         4461.8037,  5148.5527, 13974.4551,  7050.0215])\n",
      "loss: 9133714571264.000000  [   64/ 3630]\n",
      "tensor([18061.0137,  4877.9810,  7726.8540, 19121.4629,  8519.2305,  1769.5316,\n",
      "        18629.5156, 36124.5742, 10601.4121,  5824.7817,  9094.7920, 11743.2988,\n",
      "        25737.9805,  5014.1821,  4914.3413,  3777.2117, 15526.5752,  5241.3555,\n",
      "         4833.8750, 33131.8438,  6797.2192, 18157.8770,  6025.5762,  1121.8739,\n",
      "        10658.5049, 21982.4746, 12485.3594,  6659.9111,  5263.4888, 11611.7080,\n",
      "         7079.3530, 12265.9785,  7147.4727,  8415.5195,  8877.5127,  8303.0703,\n",
      "         2257.4753, 11741.7256,  5093.7124,  7978.5215, 10143.2617,  7672.9897,\n",
      "         5523.9819,  8594.0195,  9254.6621, 12818.6055,  9378.4590, 13099.8857,\n",
      "        19120.2246,  2130.6758, 10019.4902,  4266.1660,  7168.0327,  3500.6123,\n",
      "         2136.8823, 14319.0312,  4921.8262,  6123.5688,  8649.2002, 11107.3457,\n",
      "        14829.3516, 11273.8643, 15413.5967,  7144.8628])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6505.3262, 19539.2422,  7325.0483,  7625.1558,  7257.4282, 12154.0332,\n",
      "         9654.1816,  4721.4297, 24890.1055,  1242.2600,  6425.8906,  6837.3687,\n",
      "         3981.9768,  3261.9417,  3471.4097, 19043.1621,  1646.4297, 17693.6172,\n",
      "         1621.3402, 10607.0059, 38792.6875,  9440.7178,  9387.7686, 39783.3828,\n",
      "        23503.1953, 13352.0996,  6309.6631, 46599.1094,  2710.7117, 21190.7207,\n",
      "        22028.7109, 13744.0557,  9360.9238,  4189.1133,  4795.6567,  8534.6719,\n",
      "        45826.9258, 22395.7441,  7027.6987,  2416.9551, 10799.3418, 14984.9824,\n",
      "        10776.0654, 28205.9336, 10226.2842,  4894.7534, 11253.4209, 10422.9170,\n",
      "         8116.6802, 15170.0693,  9632.6973,  4818.7358,  7263.6362,  9890.5205,\n",
      "        12254.8135, 12927.1318, 11370.8428, 13635.6377, 11307.3711,  1737.3760,\n",
      "        13125.3525, 35064.6719,  9091.8574,  3591.4800])\n",
      "tensor([13143.3428,  6009.9824,  3293.2917, 21348.7051, 44213.4219, 16150.7910,\n",
      "         8199.8369, 11881.3584, 26506.6816,  4022.5654, 21223.6758,  5621.9448,\n",
      "         6500.2358,  4977.2495,  5334.7085,  7209.4917, 10107.2207,  5560.0654,\n",
      "         9962.6123,  8280.6230,  9137.7803,  9861.0254,  7371.7720,  4912.5459,\n",
      "         4431.6924,  5699.8374, 10104.5605, 11754.4150, 17448.0000,  9046.0400,\n",
      "        23445.8926,  2867.1196, 16884.9238,  5427.3716, 36808.3750,  3070.8086,\n",
      "         4737.9204,  4889.0366, 13596.5107,  2727.3950, 11439.6240, 13915.5215,\n",
      "         8059.4062,  8835.2646,  9235.0645,  9462.1533, 11335.2959,  3410.3240,\n",
      "        27322.7344,  7987.8325,  7537.1641, 10278.1416,  3866.8552, 38321.4492,\n",
      "        15973.6201,  9878.7461,  8068.1851, 10741.0586,  5080.0962,  4350.5142,\n",
      "         8585.8467, 39774.2773, 12026.0957,  8794.9893])\n",
      "tensor([ 4533.9902,  4835.7505, 12877.6006, 28403.3711,  2473.3340, 17178.6816,\n",
      "         7765.3745,  7631.4805,  5851.0708,  3176.2876, 14478.3301, 16114.3955,\n",
      "        13844.7969, 14002.8252,  6750.4321,  4670.9858, 24393.6230, 13145.5684,\n",
      "         6412.4795,  5318.4067,  5466.6616, 14374.9912, 46200.9844,  9432.9258,\n",
      "         7583.2061,  5033.0952,  4559.5342,  5138.2568,  1627.2825,  6361.5400,\n",
      "         2974.1260,  5484.5405,  1967.0227,  9670.2822, 11674.1299,  4337.7354,\n",
      "        24321.3496, 11015.1846, 20462.9980,  6747.9126, 14007.2217,  8931.1758,\n",
      "        11100.1875, 37079.3711, 11018.9854,  2196.4731,  6579.8521, 30166.6191,\n",
      "        11048.9639,  6660.8281,  5727.1279,  6551.7500,  6414.1782, 16176.0850,\n",
      "        48549.1797,  4685.1455, 44627.9375, 30364.5371,  6143.1284,  4515.1943,\n",
      "        44501.3984,  4265.1128, 17879.6328, 10711.7891])\n",
      "tensor([11842.4424, 28101.3340,  7148.2329,  9520.0020, 10801.6104,  9272.5332,\n",
      "        11906.1250, 38709.1758,  4040.5583,  7624.6299, 11291.3857, 12244.5312,\n",
      "         6496.8862, 10977.2061, 34617.8398, 13019.5244, 10461.9795, 14436.8926,\n",
      "        17765.9785, 24915.2207, 18638.6133, 12846.3525, 18765.8750,  4928.3643,\n",
      "         7160.3081,  7060.2222, 21002.7578,  6358.1851,  4990.2998, 16420.4941,\n",
      "        19120.6562, 10942.1318,  2720.7209,  1515.3448,  9634.1523,  6923.6221,\n",
      "         5632.9487, 17108.6309, 17315.6914,  6951.1992,  6941.6724, 40419.0195,\n",
      "         8233.0977, 33732.6875, 12002.3662, 13937.6660,  4852.9478, 11261.8174,\n",
      "        10577.0869,  3756.6216, 38822.0898, 27082.6641,  8252.2842,  6020.6094,\n",
      "        13616.3584, 12269.6885,  5605.0166,  8342.9092, 10920.3223, 11497.8213,\n",
      "        20641.5938,  9198.3574, 36632.5430,  4561.1885])\n",
      "tensor([ 7789.6348, 19435.9277,  4501.2344,  5452.0752, 11085.5869,  9239.9736,\n",
      "         7152.6714, 18317.6250,  5354.0747, 46220.9961, 25421.9336,  9916.5957,\n",
      "        22606.0645, 11931.1250, 11280.1006,  6415.2651,  6680.3022,  6061.5435,\n",
      "         3925.7583,  5257.5078, 14113.4727, 11044.6406, 20411.7734,  9553.4902,\n",
      "         2155.6814, 14119.6201,  2154.3611, 12495.2910, 11643.1836, 42303.6914,\n",
      "        15412.6855,  4433.2295, 14426.0742,  6358.7764, 21880.8203, 18099.5254,\n",
      "        10967.5176,  3385.3992,  6437.9717,  4909.7246, 28923.1367,  4719.7363,\n",
      "         9880.0684,  5375.0381,  7371.1211,  6325.6953,  9875.6807, 19798.0547,\n",
      "         1986.9333,  5940.5850, 11365.9521,  5186.6426,  1639.5631, 24948.2012,\n",
      "         2719.2798,  8728.9082,  7552.9785, 15456.2949,  7442.3052,  3597.5959,\n",
      "         4326.2134, 13143.3369,  9143.6279, 17270.3926])\n",
      "tensor([ 8569.8613, 14003.7881,  6353.1885,  9285.9922,  3500.7427, 12533.9229,\n",
      "         9304.7021, 11318.2275,  9301.8936,  8516.8291,  9290.3174,  2102.2646,\n",
      "         6059.1729,  3591.4707,  8920.6895,  4527.1831,  4783.2427,  5267.8184,\n",
      "         8704.1816,  5572.8662, 13430.2646,  3292.5298, 17716.7422,  8603.8232,\n",
      "         6743.1680, 35956.5898, 11316.2148, 47055.5312, 35069.3750, 10550.3281,\n",
      "         4827.9048, 29010.3184, 12347.1719,  1815.8759, 11881.6777,  4053.6936,\n",
      "         3832.0100,  8520.2656,  6393.6035, 26018.9512, 17929.3027,  9138.3857,\n",
      "         6970.0791, 10769.5791,  4740.7705,  9964.0566, 41676.0820,  6330.5752,\n",
      "        11300.3760,  7740.3369,  5631.7529,  4463.2051,  6764.7871,  4200.9316,\n",
      "         9172.7686, 36085.2188, 44445.3477,  6377.8208,  9456.3994, 17352.6797,\n",
      "        11010.6445, 10289.4707, 46182.8320, 16692.1406])\n",
      "tensor([ 4832.6074,  7650.7739, 46151.1250,  6673.0005, 10791.9600,  4134.0825,\n",
      "         5540.3325, 14539.2803, 38800.3867, 10086.8330, 43813.8672,  7331.8779,\n",
      "         8623.3125, 11371.9863,  8930.9346, 10959.3301, 12094.7510, 11455.2803,\n",
      "        48824.4492, 14235.0723, 18861.9668,  1136.3994,  4523.3594,  6088.1855,\n",
      "         1704.7002, 11881.9697, 18903.4922, 42112.2344, 11454.0215,  6891.6792,\n",
      "        30438.9199,  2913.5691,  6164.6860,  5976.8311,  4891.4346, 10652.5762,\n",
      "         4910.7397,  7243.8135,  2566.4707, 19853.7910,  4873.8315,  6182.4146,\n",
      "         3878.7839,  7151.0918,  2801.9724,  9283.5615,  5510.3447,  7378.3042,\n",
      "        18451.5840,  6605.6602, 34672.1484,  5319.6094, 11187.6562, 11196.8926,\n",
      "        24667.4199,  4559.3188,  6586.8999,  1917.3184, 11179.6074,  4372.8345,\n",
      "         3544.2539, 14142.7627, 10381.4785, 20664.3613])\n",
      "tensor([ 6347.0557,  4837.5825, 20099.7363, 19663.3809,  4765.3315,  3352.8745,\n",
      "        11987.7871, 17069.8730,  6289.7549,  4296.2710,  4388.5278, 15022.6826,\n",
      "        36308.0508, 15820.6992, 11218.9795,  8752.8184,  5379.5894,  7640.3091,\n",
      "         3481.8679,  9018.6416, 11268.0127, 47928.0312,  6616.0239, 15408.4268,\n",
      "         3940.8850, 21082.1602,  8596.8281, 10508.9199,  5227.9888, 36197.6992,\n",
      "        14711.7344, 17742.1055,  3531.7224,  6406.4106, 14410.9316, 43871.3320,\n",
      "         5209.5786,  6948.7007, 21472.4785, 34779.6133, 21797.0000,  4719.5239,\n",
      "        10744.9287, 11270.3340, 23781.3574,  5245.2271,  4992.3765,  6572.0107,\n",
      "         6738.9458,  4119.3862,  5581.6836,  7441.5010, 12163.5576,  1635.7336,\n",
      "        15828.8213,  9446.0977, 25729.1855,  7235.6587, 34166.2734,  7890.6313,\n",
      "         8568.6943, 11013.7119,  4076.4971,  3895.4170])\n",
      "tensor([ 7448.4038, 16495.7285,  7049.5171,  1711.0269,  5347.5425, 14254.6084,\n",
      "        48381.3633,  3577.9990,  8563.3232,  3611.9717,  4702.1309, 15010.1758,\n",
      "        11285.7783,  5120.5830,  8815.0703, 13112.6045, 18306.3945, 13952.1602,\n",
      "         6705.0220, 10237.2578, 19493.5508, 21774.3223, 19811.0117, 16173.2686,\n",
      "        39611.7578,  7985.6104,  9409.5928, 11082.5771, 10923.9336,  4960.0352,\n",
      "        41332.5312, 28950.4688, 22462.0430,  7639.4175,  4915.0601, 10602.7920,\n",
      "         4731.0303, 37742.5742, 24180.9336,  1664.9996,  8605.3613, 21978.6777,\n",
      "         9500.5732, 41180.8711, 13717.1807,  7201.7007, 12491.4844, 14239.9189,\n",
      "        25656.5762,  6035.6963, 12574.0488,  7518.0254, 14474.6748, 11023.4434,\n",
      "         1241.5649, 51194.5586,  5790.1455,  5267.5312,  7730.8525,  5551.1509,\n",
      "         9863.4717, 22192.4375, 46130.5273,  2690.1138])\n",
      "tensor([11397.8027, 39725.5195, 23326.9160,  6402.2915,  9390.1387,  5762.3604,\n",
      "        10214.6357,  2534.3938,  5167.4790,  7046.7222, 36021.0117, 11350.5879,\n",
      "         2220.0222,  6697.6084,  6766.8198, 32787.4570,  7730.8125,  1534.3044,\n",
      "        10544.1963, 29964.4688, 12680.1914,  6713.5645, 27043.2266, 36133.6328,\n",
      "         8283.6807,  8595.7656, 47496.4961,  4508.3540, 19252.2188, 15183.2344,\n",
      "         6079.6714,  1704.5681, 24901.6484,  2850.6838, 18648.4219, 19777.6309,\n",
      "        39793.6562,  4544.2349,  6657.9819,  4462.7217, 10823.6777, 18137.9727,\n",
      "        12198.5254, 11986.8477, 34241.5391,  5426.0776,  6196.4482,  6765.6182,\n",
      "        10631.4541,  7475.3062,  9182.0586,  9549.5654, 18218.1621,  6673.6426,\n",
      "        12839.8486, 12479.7090, 24106.9121,  4317.3154, 11856.4111, 11032.7969,\n",
      "        10713.6436, 30942.1914,  5247.9961, 12913.9922])\n",
      "tensor([ 4790.2144,  2782.4663,  4386.8716, 17740.2930, 20773.6270, 13470.8604,\n",
      "        21344.8477, 11212.6543, 14383.3535, 11305.9346, 10826.0000, 22153.5918,\n",
      "        40337.0664, 20009.6328, 12271.1777,  5466.0791, 12079.0439, 47291.0547,\n",
      "        23288.9277,  9298.7139,  7612.2227,  8798.5928, 20633.8691,  8396.6006,\n",
      "        11214.5322,  7210.7290, 17716.6367, 40920.5703, 13289.4209, 32248.7012,\n",
      "         4846.9199,  8027.9678,  3861.2097, 17758.2520, 20088.1133,  9361.3271,\n",
      "        43578.9375, 11072.3447,  6784.1021,  6250.4351, 10680.0986, 15359.1045,\n",
      "        14210.5361,  6849.0259,  4345.8423, 16790.6562, 19144.5762, 11058.1572,\n",
      "         4532.1323, 38168.0039,  4987.0674,  6877.9800,  6660.0488,  6758.4141,\n",
      "        25311.5215, 20177.6719, 11299.3428,  4151.0288, 25332.6934,  5884.6030,\n",
      "         9898.4219,  3882.6570,  3392.9768,  4364.6133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3268.8467, 24881.2305,  9884.6934, 11218.5703,  6785.8169,  5012.6846,\n",
      "         8891.1396,  8978.1855,  5891.5181,  5225.6348,  7729.6455, 12755.5361,\n",
      "         8824.9102,  8152.0234, 10129.2852, 11286.5391,  8718.8525, 44585.4570,\n",
      "         7517.5283, 16570.5996,  1163.4626,  8556.9072,  2480.9790, 18971.7168,\n",
      "         2775.1921, 21318.5898,  5472.4492, 11276.6777, 62592.8750, 11197.0967,\n",
      "        17012.1875,  5209.1787, 11280.4385,  3659.3459,  3227.1211,  8609.9229,\n",
      "         8699.4893, 16297.8457, 10579.7109, 11244.3770,  7193.5654,  9874.5771,\n",
      "         2241.7705, 48673.5586,  8527.5322,  8941.0898, 40273.6445,  5743.7627,\n",
      "         4433.5488,  7345.0840, 11500.3193, 12425.2158,  8743.7490,  8424.0781,\n",
      "         4529.8950,  4676.9346,  5128.0859,  1137.4697,  4508.5103,  4537.2729,\n",
      "        39556.4961, 12346.6543, 24873.3848,  5514.5098])\n",
      "tensor([41003.0234, 30322.2207,  6426.7280,  3906.1270,  9914.9834, 10450.5518,\n",
      "         8765.2490,  8807.7295,  8739.6240,  2104.1133,  1632.0363, 17468.9844,\n",
      "        11460.4990,  4646.7588,  4402.2329, 32351.1719, 21760.0762, 47400.9414,\n",
      "         4753.6367, 18264.6367,  1705.6245,  7316.6323, 10206.4766, 18524.0332,\n",
      "        12235.8389, 33307.5508,  1731.6770, 35585.5742, 12219.4395,  9034.2324,\n",
      "         2643.2686, 37747.6836, 19023.2598,  4900.3213,  7731.4272,  2264.7219,\n",
      "         7935.2910,  1146.7966,  8825.0859, 11657.7188,  9541.6953, 18033.9688,\n",
      "         3350.4404, 36910.6094,  5584.3057, 11680.1318,  4762.3291,  3005.1008,\n",
      "        23746.4551,  1261.8590,  5594.8457,  8442.6670,  4911.9731, 26412.2129,\n",
      "        14481.9785,  2138.0708, 39586.8633,  2457.2112,  9985.4580,  4346.6533,\n",
      "         9788.8662,  9914.8770,  4784.8198,  6875.9609])\n",
      "tensor([ 9595.4395,  4832.7827, 36869.9336,  1824.2854,  7256.7231,  9620.3311,\n",
      "         8713.6016, 24869.8359, 20709.0195, 10338.9316,  4618.5430, 16977.8887,\n",
      "         5969.7231,  6069.8608, 39508.8711,  6986.6968, 36103.1719,  2203.7358,\n",
      "        11015.1748,  1137.0110,  8419.5869,  8964.0605, 10928.8486, 27302.6289,\n",
      "         5514.9214, 36898.7344,  3201.2451, 21677.2832,  4976.8608, 10792.8076,\n",
      "        39871.7031,  1135.9407,  7160.3301, 13489.0400,  5511.5137, 11326.7148,\n",
      "         9490.4531,  9896.7666,  9563.0293,  8844.6348, 11358.5850, 13415.0381,\n",
      "         8211.1006,  4952.9585,  6182.0483, 27724.2891,  4347.0234,  5729.0054,\n",
      "         4949.7588, 18767.7383,  8825.4463, 10698.1123, 47069.8359, 28468.9199,\n",
      "         6268.3389, 18804.7520,  9335.0225,  5518.2974,  5469.8594,  8334.5898,\n",
      "         8938.4619, 18618.7344,  6527.7969,  6338.7256])\n",
      "tensor([40824.5898,  4375.7065,  7629.2109, 16500.1699, 44145.9219, 19124.3691,\n",
      "         4746.3442,  1615.7667,  2927.0647, 18596.5156, 12494.1885, 11436.7383,\n",
      "        11801.3779, 17904.5273, 20781.4883,  2902.9065,  4267.7329, 39000.4609,\n",
      "         8052.0400,  9877.6074, 60021.3984,  9564.2031,  9785.7725, 28954.2754,\n",
      "        10790.3672,  8976.1406,  2585.2690,  9763.2705,  4958.0732, 13123.6904,\n",
      "        12333.8281, 18838.7031,  4008.6965,  4341.8853,  8932.0840, 18435.6348,\n",
      "         8151.5220, 43097.4141,  9841.7959,  6686.3301, 42760.5039,  4466.6216,\n",
      "         4482.7412,  8428.0693,  6878.5137,  9421.6201,  4983.4512, 35391.7617,\n",
      "        15011.1729, 27346.0430,  8975.1846, 10982.5010, 11065.9971,  4765.5669,\n",
      "        12815.4453,  8762.7617, 12148.6709, 12636.3799,  5348.0586,  6658.6318,\n",
      "         1631.6683,  5840.4814,  4816.4673, 17731.7051])\n",
      "tensor([ 4977.9277, 10079.9287,  2217.6013,  3375.3062, 39109.7539,  6239.0723,\n",
      "        15322.8506, 11538.4209, 11848.1406, 36325.0352, 21259.3789,  7630.3252,\n",
      "         7153.4839,  9385.6670,  5972.3779, 25517.1133, 11658.1152, 25354.4531,\n",
      "        24508.0273,  6344.1738, 14692.6689, 44423.8047, 14224.7549,  5093.8828,\n",
      "        14200.8975, 11048.3555,  4234.9268,  6985.5068,  4705.4697,  8269.0439,\n",
      "         6117.4946,  5345.6807,  5438.7490,  7887.6675, 21103.6543, 18473.0508,\n",
      "        19071.2871, 12830.8213,  2801.2588, 16886.6387, 18972.4941,  9694.4453,\n",
      "        46412.5352,  9566.9912,  5708.8672,  9282.6914, 13483.6182,  4930.2988,\n",
      "         3533.4658, 15403.1816,  6341.8433,  4766.0220, 17179.5215,  4487.3071,\n",
      "        28724.9434, 17515.5254, 26109.3281,  5669.1655,  3693.4280, 24707.1543,\n",
      "         5123.0977, 17129.8613, 11987.1680,  6073.2026])\n",
      "tensor([ 9869.8105,  7968.3511,  8410.0469,  5857.2939, 12655.3779,  5662.2251,\n",
      "        11578.8184,  7576.7598, 18259.2168, 40242.7969,  8797.1270, 27963.1992,\n",
      "         6389.9458, 23757.2539, 42656.6914, 19533.8320,  9602.3350,  5958.8755,\n",
      "        12493.1104, 15554.3066, 13217.0947, 35595.5898, 11265.8232, 13224.0566,\n",
      "         2494.0220,  7165.7222, 28624.6621,  4434.9209, 11200.5312,  4746.4692,\n",
      "         8767.5732,  5395.9326,  7080.1421,  3056.3882,  6375.3955, 14349.8906,\n",
      "         4988.3501,  2498.4143,  4435.0942,  9676.6895,  4456.7559, 25333.3320,\n",
      "         3857.7593, 13019.1611, 29141.3594, 10818.9297, 10336.7979, 13981.8506,\n",
      "        11258.9199, 20745.9883,  2731.8477, 21984.4707, 13126.6777,  7499.3477,\n",
      "        11004.0498, 11481.3750,  4747.0527, 63770.4297,  6360.9937,  8944.1152,\n",
      "        10118.4238, 16577.7793, 12105.3203, 23162.3965])\n",
      "tensor([ 2716.0391, 10766.3447, 17914.1328,  5458.0464,  9473.7930,  7786.8691,\n",
      "         4239.8926, 16420.6777,  3985.0098,  8956.8672, 14006.8447, 30203.5801,\n",
      "         6128.7974, 29158.3398, 18246.4961,  5927.6006,  5320.1294, 10869.6318,\n",
      "        16862.8984, 12020.1104, 48885.1367,  9086.0244, 11289.1094, 27808.7246,\n",
      "        10431.9648,  2967.0117, 38117.9766, 39597.4062, 20630.7500, 15230.3584,\n",
      "        29330.9824,  2689.4954, 11184.4941,  8520.0264,  2362.2290,  4438.2632,\n",
      "         6821.3232, 40103.8906,  9630.5283,  8174.4531,  9101.7979,  2709.1118,\n",
      "        11390.6504, 11723.9111,  6311.9712,  9783.7812, 25309.4883,  4734.0195,\n",
      "         4751.0698,  4536.2588, 11509.6084, 16739.4160,  6662.1602, 13822.8027,\n",
      "         9262.5146,  4853.6143, 41949.2422, 13047.3320, 25891.7871, 14003.8545,\n",
      "        10906.6855,  6704.9863,  5615.3691, 10348.4600])\n",
      "tensor([11576.1299,  5347.0938,  4012.1570,  4832.9170, 13129.6035,  3493.3052,\n",
      "        10987.7705, 10269.4600, 16173.1904, 12153.3301, 10755.5332,  8816.9209,\n",
      "        12478.4727, 12638.1953,  7954.5171, 10838.7627,  8026.6665, 12475.3516,\n",
      "         5483.0806,  4784.0469, 34428.3711,  4709.6787, 14193.6133,  4809.7949,\n",
      "         2709.2439,  8865.4697,  5856.4531, 18820.2930, 11247.5459,  9058.7305,\n",
      "        11408.6787,  4828.5142,  6805.5850, 13887.2041,  8596.4873, 10649.4697,\n",
      "         8615.2998,  1263.2490, 20309.9355,  8968.3301,  1837.2371, 24227.3379,\n",
      "        13473.8389,  6475.8110, 13712.7646,  8598.2676,  6748.5913, 21595.3828,\n",
      "         5261.7490,  1622.1885,  6650.1948,  6453.5278,  8796.3018, 10012.6191,\n",
      "         7533.3374,  7392.6172, 12251.2207, 32548.3398,  5649.7148, 38499.0742,\n",
      "         5654.8184, 11566.3008,  4805.2856, 18237.9766])\n",
      "tensor([20172.1641, 25111.4336,  2400.4021,  8588.3887,  2020.1770, 10702.6426,\n",
      "        24971.7656, 11003.6924, 38239.4102,  3935.1799,  4927.7622,  6572.6685,\n",
      "         6571.0244,  5857.0200,  6733.6309, 16851.3691,  5325.6509, 21098.5547,\n",
      "         5410.2686, 12268.6318, 10560.4922, 10576.4160,  1391.5287,  6603.6201,\n",
      "        10992.1768,  6665.0405,  6123.5933, 16232.8467,  6108.3462,  2585.8506,\n",
      "         9097.2939,  5385.3379,  4058.7124,  9182.1699,  3161.4541, 13429.0352,\n",
      "        16118.8936,  9270.5479, 12646.2070, 11232.4961, 38514.6719,  3206.4915,\n",
      "        14455.6445,  4281.1631,  4645.6904, 43896.3750, 12142.5781, 14988.4316,\n",
      "        11155.0889, 12360.8281,  9239.3896, 10138.4346,  6449.4521, 43254.4180,\n",
      "        28152.8223,  6284.9707, 14004.3096,  8823.2793,  4953.1499,  4714.8999,\n",
      "        40160.7891,  5017.6616,  4340.0557, 27292.3242])\n",
      "tensor([20813.8711,  5368.5508,  6457.8433, 44069.0625, 34203.3672, 58571.0742,\n",
      "        13275.3184,  4472.5229, 47273.1094,  1969.6140,  4670.6118,  5377.8545,\n",
      "         3062.5083,  3904.8750,  5472.7295,  4689.0356, 44790.4922, 11938.2559,\n",
      "        36219.4062, 11115.5176, 10250.4209,  5811.5186,  4767.5073, 14358.3643,\n",
      "         7738.2622, 11346.8027,  4349.4619, 13343.0127, 12916.5557, 41034.2227,\n",
      "         6378.0747,  4957.9004, 11388.3428,  9581.1436, 11049.1953,  2250.8352,\n",
      "         9910.3594,  4707.6753,  4391.6519,  8548.1328,  6414.2407,  5288.9102,\n",
      "         9978.4072, 18836.8555, 31620.0020,  7098.9106, 19530.0527, 47974.2344,\n",
      "         1977.8149,  7494.2979, 11443.9365, 10882.8584, 22494.5723, 11313.6768,\n",
      "         3392.3652,  1131.5066,  9290.1396,  7117.9800,  1628.4709,  6503.9780,\n",
      "        12231.6133,  5000.0771,  8731.4805,  9491.4062])\n",
      "tensor([14064.4277, 11196.6904,  5507.6475, 13390.5586, 11129.3320, 14418.2803,\n",
      "         6999.2749,  6335.7681, 15169.1836, 30184.9375, 18150.9121, 46255.1133,\n",
      "        44641.1992, 14310.4971,  7237.4185,  1909.5275,  9991.0381, 15161.5342,\n",
      "        10370.9121, 10824.4844,  7112.5806,  1725.5869,  8886.0176,  3532.9143,\n",
      "        11512.4053, 23258.9980,  3994.1777, 37662.7617, 10435.0654,  4948.6729,\n",
      "        18707.2246,  6009.3750,  6265.1655,  5930.1489,  6227.6714, 26467.0977,\n",
      "         6728.2705,  8547.6914, 14001.2871, 10879.1729,  4661.2861,  1634.5734,\n",
      "        11013.7930,  4686.3887,  5124.1885, 10812.6650,  3394.4917, 34838.8711,\n",
      "         6013.1274, 32734.1855,  6770.1924,  4527.1177,  5855.9023, 38054.4922,\n",
      "         4441.2134, 13143.8652, 16378.9141,  9590.4580,  2459.7202, 18191.7344,\n",
      "        11520.0996,  8583.2227, 11262.1416, 10436.0957])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3943.5955, 28105.2344, 10805.9551, 15648.6582, 11482.6348,  5127.6079,\n",
      "         6648.9106,  9012.5898, 40232.9336, 24904.3164,  4772.9556, 10056.7275,\n",
      "         6238.2979, 12645.1660,  5709.1646,  4158.2539, 11120.1904, 10676.0977,\n",
      "        23300.7910,  5246.0469,  5910.9438,  1826.8430, 10373.2334,  1252.4070,\n",
      "         3353.2839, 42969.8516, 20878.7852,  4822.7959, 38746.3555, 12387.7539,\n",
      "         9441.5166, 14925.9062, 41999.5195, 27107.3418, 24685.0215, 13673.9072,\n",
      "        19453.6855, 23065.4199, 12643.3779,  6032.5845, 10611.9785, 13462.5195,\n",
      "         2156.7517, 37464.0859,  8671.1914, 38511.6289,  5364.4507, 11259.7061,\n",
      "         9644.2529,  4888.4927,  9669.7842,  7960.8682,  5086.6528, 33475.8164,\n",
      "         4674.7974,  6312.1934,  3594.1709,  1625.4337, 11166.6670,  8709.8965,\n",
      "        10148.4678, 13660.8369,  5495.3916, 38245.5938])\n",
      "tensor([ 5793.8735,  8869.2197,  8827.7451, 12523.6045, 11030.4414, 52590.8281,\n",
      "         7949.7476,  4005.4226,  8878.8574, 20341.3789,  8334.4580,  3736.9246,\n",
      "         5150.3169,  8703.4561, 18963.1719,  8486.9014, 14164.9453,  4243.5898,\n",
      "        18772.8262, 42560.4297, 10932.0791, 18444.5488, 10896.6133,  9946.9463,\n",
      "         4571.4131, 13880.9492,  2730.1079, 11820.7783,  5177.4429, 11737.8486,\n",
      "         6611.5747,  5535.8774, 36536.8750, 11555.6875, 11093.9844, 13640.6631,\n",
      "        46460.5859,  1880.0699,  7623.5181, 16405.6543,  9583.7246,  5459.4141,\n",
      "         5836.8037, 39241.4414, 14833.3945,  6521.4097,  4773.0742, 47289.7266,\n",
      "         4518.2300, 10159.4766,  6059.3662, 19257.8496, 12741.1670,  8797.9082,\n",
      "        13352.4766,  9779.9639, 13224.6934, 37284.8203,  3579.8286,  1629.8335,\n",
      "        16069.0850, 19235.8223,  4032.2407,  6799.4580])\n",
      "tensor([20420.6055, 11008.3906, 39802.3516, 11094.9131,  3919.3872, 20135.9785,\n",
      "        11658.3789,  5240.1348,  1708.9258, 13030.3164,  5327.4004,  8782.4688,\n",
      "         6682.4009,  5587.0991, 17526.3105, 13887.9688, 11165.4180, 19749.3828,\n",
      "         7682.6699,  9080.3447, 26392.2598, 11883.8730,  7173.3599,  3309.7925,\n",
      "         4779.6025,  5426.9219, 14451.8350,  6746.7427, 26081.9590, 12112.2822,\n",
      "         5920.1040, 38998.5469, 35907.0117,  6341.7935,  6725.8091, 19496.7188,\n",
      "        12925.8857, 11399.6484,  4826.3560,  8612.3125,  5979.7310,  6272.4771,\n",
      "        26322.8652, 18322.1152,  9132.1406, 47896.7930, 44246.3320, 11961.2793,\n",
      "         2946.0974,  2731.9121,  8584.4238, 11443.1621,  3877.3042, 20624.8047,\n",
      "         2842.7607,  8023.1353,  7348.1421, 14407.8955,  3277.1609, 10231.5000,\n",
      "         5187.8267,  2322.6218, 15795.4727,  6255.7744])\n",
      "tensor([ 4687.7969, 24329.3379, 11394.0654,  8583.3838,  9535.4424, 10024.0820,\n",
      "         5191.5757, 11015.3203,  7467.0039,  4645.9453, 47417.1797,  4590.5381,\n",
      "         7281.5054,  7155.7979, 48316.0078,  7633.7207,  9179.2607, 48517.5625,\n",
      "         5211.0073,  4766.5093,  6338.0757,  4725.9038,  9861.1865,  9903.7754,\n",
      "        40720.5508,  1980.0699,  6933.2422,  8773.6094, 23887.6621,  7626.9932,\n",
      "        25718.6445, 13063.8828,  4989.5703,  7620.8408, 32108.6621,  3213.6221,\n",
      "         6185.3208, 14901.5166, 24096.1113,  3919.9067,  8859.7686,  8278.1475,\n",
      "         3366.6697,  9583.8936, 19040.8770,  5466.0967, 11312.4746,  2254.7966,\n",
      "         8902.8799,  3479.0305, 28443.2227, 10965.4463, 11027.3135,  4133.6416,\n",
      "        11068.2744, 17043.3418,  6282.2349, 18381.5977, 12649.6045, 12829.4551,\n",
      "         5265.0400,  9909.5566,  5377.0161, 12592.5342])\n",
      "tensor([ 5003.8530,  1712.4691,  9236.1387,  5301.3931, 10788.7412,  2352.9685,\n",
      "        26140.3594,  4809.4521,  6216.3872, 13929.6611,  2279.8958,  4113.1367,\n",
      "        16140.9229,  9249.4951, 28340.1895,  9625.9199,  5846.9175,  7742.2729,\n",
      "         8795.0664, 11411.6846,  9337.3311,  2714.4729,  4710.8916,  3234.1348,\n",
      "        36041.9883,  6621.8008,  6455.8628,  6658.1187, 11743.9346,  5390.6113,\n",
      "         2699.5684, 11033.6621,  5970.1597,  7409.4360, 11833.7822,  8162.7163,\n",
      "        10114.5957,  4518.8262, 19370.4336,  8515.7588, 12146.9707,  4452.5493,\n",
      "         5146.2993, 29186.4824, 15230.3242,  5397.6167, 17883.0195,  2117.3389,\n",
      "        40935.6445,  8059.6792,  3077.0955, 10795.9375, 12878.1699, 36189.1016,\n",
      "         4906.4097, 12905.0811,  8704.5488, 12404.8789,  1682.5970,  9202.4727,\n",
      "         8078.5361,  4436.6260,  7418.5220, 17776.6387])\n",
      "tensor([ 5241.4849,  7228.2158,  9471.1426,  4137.5225, 24513.0918,  4449.2339,\n",
      "         4185.0977, 45710.2070, 15179.2051,  2788.0889, 23241.4746,  5125.2158,\n",
      "        17063.4258,  8993.5400,  6488.8950,  5549.3247,  6712.0068,  6772.6304,\n",
      "        12612.1074, 39969.4414, 40142.9062, 23843.5332, 10499.2393,  4294.2544,\n",
      "        21220.4004, 11345.5186,  6640.5449, 23033.8848,  5152.1338,  5031.2695,\n",
      "         6201.7407,  5999.7490, 20296.8633, 24535.6992,  4884.5830,  4761.0718,\n",
      "         6046.6641, 11035.8584, 11554.2236, 26619.4922,  8219.2041,  7175.9937,\n",
      "        11107.4102, 11335.8467, 10096.9697, 14409.9561,  5258.5146,  7323.7349,\n",
      "        10407.0859,  9308.5625,  3502.8157,  8240.5898, 19361.9980, 37701.8750,\n",
      "        10141.1357, 13968.1250, 11212.9551,  4518.1045, 12979.3584,  2396.0959,\n",
      "        11735.8447,  4022.6128, 44241.9023,  5028.1250])\n",
      "tensor([ 9370.4639,  8987.6270, 12430.9531, 12251.3223,  1149.3959, 19988.1250,\n",
      "         5712.3555, 10806.8389,  6516.0615, 11363.2832,  2597.7791,  2134.9016,\n",
      "         9619.6699, 24520.2637, 10602.3848,  5272.1758, 13846.9619, 19521.9688,\n",
      "         9722.9551,  6515.7598,  6088.7759,  3378.9099, 11034.1230,  5136.1411,\n",
      "        37086.6680, 25026.4238, 24635.1055,  4356.1611,  6474.0132, 15006.5791,\n",
      "        17511.0918, 12363.5469,  2904.0879,  3615.3975,  2203.4719,  9222.4023,\n",
      "         3875.7341,  6379.7090, 18775.4609,  1631.8212,  9439.2363, 17150.6855,\n",
      "        11615.2969, 23261.0215,  7222.4863, 49577.6641, 33750.2930,  8827.2100,\n",
      "         5044.3613,  7746.2632,  8716.2393, 12103.8350, 10802.8574,  4237.1265,\n",
      "        18745.6289,  5256.2808, 23104.1172,  9273.9131, 11552.9043, 10841.7090,\n",
      "         2755.0210,  5630.4580,  4510.5015,  7965.5845])\n",
      "tensor([ 7296.7134, 23510.6797,  1261.4420,  6981.8662, 13256.1748, 10072.0547,\n",
      "        46113.5117, 12267.4463, 11232.9414,  4821.6626, 27375.9043, 23352.3184,\n",
      "         7337.7480, 13508.6172, 43943.8750, 15305.1963, 23082.9551, 12629.1660,\n",
      "         5425.0234,  6027.3472,  1633.9618,  9321.8613, 19933.4570,  1906.3583,\n",
      "        10256.5186, 12013.5322, 39047.2852, 12094.4775, 11342.8555,  8871.1514,\n",
      "         5194.6929, 18242.1328,  9723.1592, 36149.4844,  5454.9780,  1702.4553,\n",
      "         6417.0142,  9866.3047, 25678.7793, 23401.3066,  6292.8462, 10950.0391,\n",
      "        11386.5137, 11362.7549, 11908.1914,  6101.8604, 41920.0781, 11975.6758,\n",
      "         7443.6431,  8423.9121, 11356.6611, 42124.5156, 14115.0889,  9447.2500,\n",
      "        32471.2324,  3180.5100, 11967.3867, 38431.8398, 23862.7793, 24876.9414,\n",
      "        11020.7070, 18303.1406, 12474.2529, 11264.5410])\n",
      "tensor([ 4040.4421,  5038.1357, 10978.5791,  9411.0049,  2302.3000, 10992.3867,\n",
      "         8936.3086, 10909.8730,  6764.9863,  2855.4375, 10115.0088,  4944.6729,\n",
      "        21195.8184, 10601.6318, 13405.3906, 36580.2812,  4697.8984, 19528.0117,\n",
      "        47937.2695, 12957.1182, 14313.8467,  8716.3652,  9821.0898, 10887.2500,\n",
      "        47916.9648, 13747.8721,  5550.9214, 11798.5205, 12044.3418, 39983.4258,\n",
      "         1141.4451, 13502.7852, 12100.5918, 14194.9961,  7262.3726, 25274.0840,\n",
      "         5748.8198,  8591.2148,  3987.9260, 10987.3252,  2020.5522, 11250.8926,\n",
      "        10989.1992, 21533.6562, 16218.6719,  4402.4639, 10232.0596, 13451.1221,\n",
      "        20144.6738, 10992.3857,  9858.0254, 10325.2061,  6405.0771,  7358.1758,\n",
      "         6956.8833,  6099.9766,  6753.0381, 19080.2051, 32584.9629, 11879.1045,\n",
      "         8937.6973, 10991.3584,  6666.0552,  5313.6807])\n",
      "tensor([12558.4961,  3847.6741,  5829.9429, 27709.7969,  9453.6953, 41817.2773,\n",
      "         9634.5381, 43921.1836,  2281.3782,  8457.8184, 24956.6738,  9704.6680,\n",
      "        10451.2744, 20155.6504,  2680.9492, 17879.2539, 37270.1523,  4836.1221,\n",
      "        14201.5137,  6961.4736,  4260.7441,  6781.3540,  9172.3047,  5207.8936,\n",
      "        10961.7842,  5114.4463, 45008.9570,  9668.9141,  7445.9180, 21112.5098,\n",
      "         2789.0574, 34969.1055, 17878.9004,  9377.9043,  4805.6055,  2201.0972,\n",
      "        10961.4395,  8727.1738,  5142.2842, 11475.5439, 40974.1641,  6653.7886,\n",
      "         8444.4736,  8310.8389,  3208.7871,  6526.5928, 21232.1816,  6313.7588,\n",
      "         7147.1050,  4922.9160, 34393.4492, 16115.3047, 29918.5391,  9046.3711,\n",
      "         5488.2622, 18129.7715, 45610.9219, 10763.5752,  7526.7065,  5204.4038,\n",
      "        18944.2148,  9800.8887,  3914.3958,  6593.5083])\n",
      "tensor([ 5400.9805,  2217.4692, 14643.6221,  7228.7710, 17550.7637, 11534.8730,\n",
      "         6429.9795,  6235.9092,  2137.6536,  5313.0581,  5210.3770,  9166.4688,\n",
      "        19658.3535,  1877.9294,  6787.5308,  4618.7397,  8971.3008, 23632.3750,\n",
      "        11168.2471,  1728.8970,  8017.0610, 19107.7793,  9230.8125,  2128.4312,\n",
      "        11145.7910, 12982.8750,  8635.2988,  6604.7139,  2055.3250,  8591.3857,\n",
      "         6751.0796,  3536.5894,  6607.4302,  8753.9961,  5002.7827, 11433.1113,\n",
      "         5244.6787, 11016.3340,  7150.3506,  9778.3477, 39245.1641,  1639.5631,\n",
      "        10355.6406, 10680.1963,  7727.2534,  9781.5039,  1744.4650, 44397.3906,\n",
      "         4931.6470, 19515.5410, 10008.9678, 17081.0801, 46247.9648, 10922.4590,\n",
      "         5257.6685, 18806.1445,  5119.3516,  8733.2295, 11281.5967,  8526.8809,\n",
      "        11842.6240, 11298.1533,  3537.7029, 10740.0205])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4768.7769, 17984.5977,  5028.1465, 30063.5801, 20335.8613,  2899.4893,\n",
      "        11008.7568, 42000.0352,  4708.9570,  9386.1611, 44400.4062,  1711.3367,\n",
      "         8697.2246, 12233.8281,  6750.3789,  6652.5288, 22493.6602,  2457.5020,\n",
      "         2198.1899, 22198.8750, 10976.2461, 10491.6934,  4667.6074, 48885.2422,\n",
      "        10806.4873,  9193.8389, 10537.9121,  8953.5049, 41097.1602,  6270.9673,\n",
      "         8047.2007,  9036.5869, 11117.1660, 10797.3359,  8988.1592,  6738.0117,\n",
      "        13073.8184, 12904.9053,  2750.1782,  5495.3623, 12635.6611, 14923.2422,\n",
      "         5525.4204,  7148.5503,  7634.4023, 46544.0820, 23945.7148, 14004.6660,\n",
      "        10959.6943,  3761.2920,  1674.6323,  6123.9985, 10621.7578, 12476.7812,\n",
      "         3526.3516, 19964.7461, 17663.1445,  6585.8774, 38169.4375, 14289.9834,\n",
      "         8583.9736, 54108.4180,  4892.4019,  9391.3457])\n",
      "tensor([15003.8145, 11922.0859,  3310.4224, 46718.1641,  1972.9500,  4883.8662,\n",
      "         2353.8313, 17560.3789,  1712.2271,  2527.8186,  3535.1780, 28039.5391,\n",
      "         5390.6162,  2207.6975,  9636.7324, 12557.6055,  4415.1587, 10594.5020,\n",
      "        19444.2656,  6677.5752,  4830.6299, 14043.4766,  8653.0039,  1621.8827,\n",
      "         6990.8960,  2150.4690,  4472.8804,  2261.5688,  8549.4805, 11837.1602,\n",
      "         5406.6313,  6403.4287,  5620.7578, 14051.1152,  7265.7026,  5379.2900,\n",
      "         8186.8477, 27218.4375,  1694.7964, 22218.1152,  7443.8477,  4801.4756,\n",
      "        26823.7402,  8766.9248,  9313.5957, 13457.9609,  3956.0715, 13936.8818,\n",
      "        47437.5430,  6658.3477, 16753.3984, 36397.5742,  6186.1270,  9820.2334,\n",
      "        17496.3066,  4274.2217, 18817.6211,  8712.5498,  9288.0264, 11272.3311,\n",
      "         4149.7358, 18415.4375,  6364.9355,  8606.1201])\n",
      "tensor([20277.8066,  4320.4106, 11228.0234,  7609.9360, 26993.5117, 18823.5957,\n",
      "        16796.4121,  5116.5005, 19129.8770,  5062.8535, 10986.4014, 12222.8984,\n",
      "        10782.4883, 19726.1484,  4921.7368, 12648.7031, 11314.8672, 23514.5645,\n",
      "         6071.1353,  4756.5337, 11661.3965,  2639.0430,  3757.8447,  9523.9785,\n",
      "        12644.5889,  7724.7695, 10715.7383, 14571.8906, 10334.8125, 10991.8525,\n",
      "        11372.7080, 14453.7627, 10313.1445, 42856.8398, 10796.3506, 10090.1299,\n",
      "        19431.9727,  9365.2812,  8124.4082, 38392.7461, 21491.9805, 13844.5059,\n",
      "         8699.7979,  5271.1968, 26926.5137, 13555.0049,  9597.2881, 21828.0293,\n",
      "        15181.1689,  8601.3291,  3281.9583, 14154.8193,  5836.5205,  9163.1084,\n",
      "        24058.4453,  7161.5146, 11353.2275, 38092.4102,  3393.5928, 11163.5684,\n",
      "         9813.9365,  4917.9258,  4611.5186,  8587.1279])\n",
      "tensor([ 8678.6367,  4669.0410,  2407.0042, 12890.0576, 42705.1836,  4889.9995,\n",
      "        11339.5977, 18223.4512, 35491.6406, 13770.0977,  1708.0013,  9086.5273,\n",
      "        18232.0332, 18955.2207,  3484.3311,  6748.7524, 14283.4590, 17593.3750,\n",
      "         7688.3145, 20167.3359, 29762.3105, 12323.9355,  9210.7949, 36036.2812,\n",
      "         5451.4233,  8989.9814, 10882.7529, 13342.3301,  7372.1353,  6684.9663,\n",
      "        26125.6738,  3736.4646,  4752.7188,  8302.5361, 10952.2207, 13041.9209,\n",
      "         8688.8584, 10446.0498, 19434.5449,  7441.0532,  9074.2441,  5702.2310,\n",
      "        13675.2305,  5871.2061,  9450.8184, 13204.2861, 12265.8857,  3369.4167,\n",
      "        45841.4688,  5398.8911,  3522.6023, 12142.8438, 12629.8965,  9095.0684,\n",
      "        23306.5469,  7153.5537, 24499.0840, 17578.2148, 46217.6641, 14799.0312,\n",
      "         2721.3208,  4853.6582, 11247.4453,  6628.2686])\n",
      "tensor([ 5889.1040,  6788.5342, 12401.7461, 10376.2949,  6658.2373,  7705.1616,\n",
      "         9141.2588,  6674.1318, 15019.7598, 26875.1562,  5227.2632,  9655.7354,\n",
      "         8734.1172,  9143.5732,  1632.5645,  9755.0039,  3404.4211, 10092.3564,\n",
      "         8107.7363, 10264.4424,  5312.5171,  7121.3081,  4869.0752,  3021.8091,\n",
      "        38310.2305,  4360.3711, 10451.9365, 38258.9219, 40941.2852,  8624.3711,\n",
      "         5214.5659, 16710.0039,  2523.1694,  2200.8308, 23807.2402,  5050.9492,\n",
      "         4708.5298,  4745.0073,  9931.6240, 37484.4492,  6286.6333, 16776.3047,\n",
      "         8116.2690,  9921.9004, 47403.8789, 27000.9844, 11176.2168, 18328.2383,\n",
      "        39722.7461,  5312.1699,  5757.4136,  6258.0811, 11038.9854, 14001.1338,\n",
      "        17844.1328, 11070.5352,  3238.4358,  9748.9102, 10929.7871,  8217.4004,\n",
      "        37465.3438, 14744.0938, 12030.7793,  5221.3887])\n",
      "tensor([11537.5742,  4297.9951, 43223.4648, 42661.0117, 34593.8672, 10015.4395,\n",
      "         8583.3223,  6716.7861,  4788.8638,  6417.5674, 40690.5430, 11891.9922,\n",
      "         6655.1758,  1748.7740, 10825.2539, 10751.9248, 10726.4121, 23552.6621,\n",
      "        14109.8145, 37607.5273,  9724.5303,  4343.5864, 46199.6367, 46889.2617,\n",
      "         3353.4702,  5465.4326, 19593.2031, 26730.3984,  5934.3799,  8070.9092,\n",
      "        10594.2256,  5412.7876, 28287.8984, 10584.0400, 10493.9453,  4791.5850,\n",
      "         4863.7627, 18866.6875,  6018.9741,  3443.0640, 10897.6035,  6753.0000,\n",
      "         2210.1880,  7731.8579,  7162.0122, 36837.4688, 20334.6172, 12797.2100,\n",
      "         2221.5645, 16450.8945, 15817.9854, 11193.2080, 19480.2383,  9630.3975,\n",
      "         4934.0996,  6203.9019, 17101.9160,  6014.0322,  6416.2925, 48675.5195,\n",
      "        17500.2051,  9104.3770,  6457.3037, 11540.8369])\n",
      "tensor([19350.3691,  5012.4712,  7196.8672,  9957.4824, 13140.4922, 12757.7354,\n",
      "         6940.9097,  6971.5854, 12656.7852, 28476.7344,  5548.1787, 12835.6465,\n",
      "         6718.9780, 42983.4570,  5377.4580,  5966.8872, 30284.6426, 17942.1055,\n",
      "         7400.1914, 11366.3506,  6726.0503, 38711.0000, 12032.3262,  5898.1665,\n",
      "        25382.2969, 13648.6016, 25147.0195,  9850.4316,  3554.2029,  9485.2559,\n",
      "         6755.6372, 38349.7500, 40861.9336,  9411.1689,  7441.2729,  3591.6248,\n",
      "         5272.6763, 47412.0312,  9610.9561,  9979.2393, 11200.0918, 24671.6641,\n",
      "         1839.4099, 25081.7676,  7077.1895,  8592.1816, 10761.4844,  8564.4863,\n",
      "        18650.8203, 15731.3271, 11073.1758, 14256.1924,  4307.1519, 43753.3359,\n",
      "         5983.9121,  2483.7361,  5824.1021,  9174.1357,  6211.9331,  8083.9199,\n",
      "         6688.9746, 12838.9932,  5809.5537,  9584.8838])\n",
      "tensor([16417.6914,  1981.5819,  6330.9521, 19085.0898,  6350.9326, 12609.8867,\n",
      "         8218.7500, 11170.6738,  6756.5708, 21308.4199,  2331.5190,  4986.4912,\n",
      "         5238.0103, 17434.5801,  7421.1943,  5599.7192, 37133.8984, 10278.5635,\n",
      "        12112.9785, 19892.5918,  6475.1812,  5243.5674,  4500.3394, 19069.4746,\n",
      "        10274.5830,  6761.2153, 24476.4785,  4428.8877, 19594.8105,  5282.9453,\n",
      "         5150.5874,  4738.2681,  5455.4463,  4853.2388, 17626.2402,  5602.9517,\n",
      "        16657.7168, 45863.2031, 29523.1660,  6356.2705,  5151.9502, 33900.6523,\n",
      "        11226.9932, 11830.6074,  5003.3179,  7512.2671, 27037.9141,  2211.1309,\n",
      "         1727.7850, 15439.3281,  3850.3679, 22478.5996,  8551.3467,  4793.2402,\n",
      "         9872.7012,  6184.2993, 20630.2832, 46475.2383, 43287.3750, 13725.4717,\n",
      "         8732.0859,  8792.4785,  9254.5244,  8662.5176])\n",
      "tensor([ 3766.8838,  7804.1606,  3972.9248,  9516.3633, 27117.9941,  6777.4302,\n",
      "        20149.3223,  6548.1948, 41279.2422, 14768.0205, 11568.4766, 11300.4541,\n",
      "        12265.5068,  5527.3135, 12387.4072, 34472.8398,  6664.6860, 45702.0234,\n",
      "         9393.3584, 22412.6484,  9948.9697, 10720.6953,  4708.3374,  4522.0054,\n",
      "        10736.8711, 26489.5762,  6093.4038,  5822.7246,  9964.0596,  5415.6611,\n",
      "         4433.7495,  8769.6445, 32905.8164, 15238.4053,  8965.7959,  5185.2397,\n",
      "        10065.4131,  6763.1831, 24919.3574,  4846.9849,  8517.6445, 12129.6143,\n",
      "        10807.3350, 11664.4541,  6604.6367, 12731.0000,  6657.5737, 22060.1719,\n",
      "         7085.8442,  6832.8516,  6677.3218,  7671.1797, 36023.1484, 46661.4414,\n",
      "         9642.2871,  7810.5654,  5266.3657,  4929.9517,  8559.9883, 30269.4277,\n",
      "        10390.9131, 11504.0811,  3398.1965,  9873.8760])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1242.8160, 27626.1797, 18608.2617,  9950.3486,  3980.1387, 11854.6738,\n",
      "         3704.3545, 13670.9150, 13393.7559, 22331.5664, 10693.0615, 36225.4023,\n",
      "        10600.5479,  6710.1919,  2395.1716, 19524.6582, 46145.6523, 17748.5059,\n",
      "         2404.7339,  6663.0903, 15746.4619, 28058.4707, 46654.7266, 21659.9297,\n",
      "         6543.0454, 13919.8232,  8823.9854,  7985.8149, 41919.0977, 13994.2930,\n",
      "         8491.9072,  3176.8159, 27941.2871, 23244.7910, 13065.5439,  7156.5244,\n",
      "        39532.2461,  9414.9199, 33907.5469,  7206.2949, 14133.0381,  2045.6853,\n",
      "         6389.8535, 11950.1514, 10106.1338,  4895.6797, 41661.6016,  4318.3760,\n",
      "         7034.8438,  3556.9224, 17174.6816, 10823.2266,  6211.3403, 14249.3955,\n",
      "        11674.8730,  6105.5820, 18310.7422,  4292.8584,  8277.5234,  4743.8374,\n",
      "         6112.3530,  6149.6147, 10751.5498,  9047.6494])\n",
      "tensor([21925.6836,  4357.0435,  8595.2559, 11884.0488, 12874.1396, 55135.4023,\n",
      "         3393.3564, 10746.0781,  3855.9429,  5859.6704, 10240.5527,  5929.4653,\n",
      "        24915.0469,  7222.7861,  4772.8760,  6775.9609, 46988.3516, 12950.0713,\n",
      "        16085.1279,  4698.2017,  4340.4409,  2741.9480,  1880.4871,  4934.7051,\n",
      "         7045.4990, 14049.8330,  3044.2134, 10563.2197, 23582.4688,  5253.5239,\n",
      "        37829.7227,  1964.7800,  5926.8462, 12491.8818, 44202.6523,  8553.9541,\n",
      "         8712.3389, 44061.4805,  6322.3701,  5008.2500, 16586.4980,  8827.5264,\n",
      "         4662.8926, 13228.8467,  9095.5312, 11396.9004,  4285.9956, 25154.1719,\n",
      "        39385.6328,  6198.7520, 10787.6904, 10832.7705, 11130.3535, 13557.7188,\n",
      "         3947.4131,  1526.3120, 33353.5000, 39836.5195,  5210.1001, 11111.9248,\n",
      "        19214.7051,  5257.9595, 23563.0156, 40932.4297])\n",
      "tensor([42284.9180, 28868.6641,  2026.9741, 18989.3652, 21806.1816, 11355.8174,\n",
      "        11185.9141, 39727.6133,  4894.7534,  4779.6025,  6216.3872,  8844.6348,\n",
      "        16776.3047,  5425.0234, 13846.9619, 17593.3750,  2689.4954, 10976.2461,\n",
      "         9884.6934, 41949.2422,  9138.3857,  8807.7295, 16796.4121,  8559.9883,\n",
      "        39586.8633,  3526.3516,  8716.2393, 12905.0811,  6664.6860, 19071.2871,\n",
      "        10431.9648,  6775.9609,  9948.9697, 41180.8711, 13393.7559,  8598.2676,\n",
      "         5327.4004,  4833.8750, 20277.8066,  1137.0110, 22218.1152, 13145.5684,\n",
      "        10250.4209,  6697.6084,  5257.5078, 48173.3594,  7625.1558, 11837.1602,\n",
      "         3234.1348, 11833.7822,  5427.3716,  2585.8506, 12838.9932, 11018.9854,\n",
      "        18817.6211, 36189.1016,  6674.1318,  8792.4785, 20630.2832,  4709.6787,\n",
      "         9046.3711,  6272.4771, 15554.3066, 11068.2744])\n",
      "tensor([43753.3359,  2480.9790,  6684.9663,  4909.7246, 12533.9229,  8709.8965,\n",
      "         5000.0771,  6728.2705, 12491.4844,  3268.8467, 28868.6641,  8591.2148,\n",
      "         9182.0586, 12235.8389,  6112.3530, 11048.3555,  6322.3701,  4297.9951,\n",
      "         8116.2690,  4234.9268,  2352.9685,  6648.9106, 15439.3281, 17844.1328,\n",
      "         4752.7188, 18232.0332,  4350.5142,  4449.2339,  5654.8184,  2362.2290,\n",
      "         5347.5425, 18804.7520,  5257.9595,  5050.9492, 10824.4844,  8588.3887,\n",
      "        10991.8525,  2731.8477, 16739.4160,  6837.3687, 17740.2930,  8595.2559,\n",
      "         4766.5093,  4267.7329,  8816.9209, 11353.2275,  4747.0527, 14410.9316,\n",
      "        25729.1855,  6961.4736, 40941.2852, 14064.4277,  4058.7124, 39109.7539,\n",
      "         2680.9492,  9946.9463, 21677.2832,  5345.6807, 13508.6172,  7633.7207,\n",
      "         4294.2544, 19539.2422,  2045.6853,  4766.0220])\n",
      "tensor([ 6196.4482,  3005.1008,  5605.0166,  9878.7461,  9462.1533,  4784.8198,\n",
      "         4266.1660,  9783.7812, 17352.6797, 62592.8750,  6009.9824, 34166.2734,\n",
      "         9172.7686,  6616.0239, 10090.1299, 17716.6367, 34393.4492, 29523.1660,\n",
      "         2775.1921,  8347.1641,  5240.7651,  7050.0215,  6571.0244, 12638.1953,\n",
      "        11013.7119,  9222.4023, 10373.2334, 11460.4990,  3847.6741, 17009.3359,\n",
      "        14249.3955,  5288.9102, 16455.7070,  9360.9238,  4977.2495, 15169.1836,\n",
      "        38239.4102,  4360.3711,  6764.7871,  6117.4946, 12485.3594, 11555.6875,\n",
      "        44246.3320,  9962.6123,  6093.4038,  6415.2651, 17434.5801, 20099.7363,\n",
      "         9748.9102,  7201.7007,  2302.3000, 12360.8281,  4827.9048, 18091.3730,\n",
      "         5093.8828, 24956.6738,  5415.6611,  5857.2939, 42983.4570, 10787.6904,\n",
      "        18823.5957,  7475.3062,  3985.0098, 12493.1104])\n",
      "tensor([ 6680.3022, 11335.8467, 13352.4766, 19663.3809, 10118.4238, 14235.0723,\n",
      "        18444.5488,  8553.9541, 10148.4678,  6361.5400,  6738.9458, 37747.6836,\n",
      "        12491.8818, 46151.1250,  4040.5583,  5400.9805, 18303.1406,  5080.0962,\n",
      "        14224.7549,  8938.4619, 11168.2471, 11170.6738,  9321.8613, 10942.1318,\n",
      "        11228.0234, 39611.7578, 12644.5889, 18317.6250,  8732.0859, 13483.6182,\n",
      "         7441.5010,  4805.6055, 13770.0977,  8798.5928, 18629.5156, 25333.3320,\n",
      "        13143.8652,  4884.5830,  9875.6807, 19533.8320,  3398.1965,  8825.0859,\n",
      "        10959.3301,  2967.0117, 10019.4902, 18861.9668,  9716.7441,  4433.2295,\n",
      "        19530.0527,  4133.6416, 30166.6191,  5253.5239,  9046.0400, 11316.2148,\n",
      "         6088.7759,  5927.6006, 18310.7422, 23162.3965, 14692.6689,  9174.1357,\n",
      "        10806.8389, 11386.5137, 10159.4766, 11615.2969])\n",
      "tensor([11100.1875, 40242.7969,  7730.8525, 11891.9922,  4435.0942,  9861.0254,\n",
      "         3227.1211,  8116.6802,  8162.7163,  3404.4211,  9290.3174,  8797.9082,\n",
      "        13844.7969,  9236.1387,  6360.9937, 12254.8135, 39245.1641,  6705.0220,\n",
      "        11554.2236, 23757.2539,  6999.2749, 18157.8770,  7742.2729, 38310.2305,\n",
      "        15413.5967,  5702.2310, 10812.6650,  2102.2646,  6356.2705,  5466.0967,\n",
      "        13140.4922, 29010.3184,  1137.4697,  2026.9741,  4888.4927,  7935.2910,\n",
      "         4832.9170,  5271.1968, 13880.9492,  7098.9106,  4809.7949,  3385.3992,\n",
      "         4243.5898,  9453.6953,  3471.4097,  7147.4727, 19107.7793, 13073.8184,\n",
      "         8869.2197,  6338.0757, 16297.8457,  9921.9004, 45863.2031,  3293.2917,\n",
      "         6088.1855, 10711.7891,  3261.9417,  4687.7969,  6933.2422, 12635.6611,\n",
      "         6128.7974,  8107.7363,  4510.5015,  9634.5381])\n",
      "tensor([ 1149.3959, 44213.4219,  4461.8037,  7345.0840, 44641.1992,  1837.2371,\n",
      "        17174.6816, 10422.9170,  6403.4287, 24321.3496,  5191.5757, 10658.5049,\n",
      "        10232.0596,  4921.8262,  6079.6714, 37464.0859, 23258.9980,  1880.4871,\n",
      "         7168.9038,  5214.5659,  8877.5127,  2137.6536,  4345.8423, 22494.5723,\n",
      "         8186.8477,  8520.2656, 14004.3096,  4518.2300, 48673.5586,  2899.4893,\n",
      "         4340.0557,  8902.8799, 19496.7188,  6406.4106,  9370.4639, 40920.5703,\n",
      "         5282.9453,  4826.3560, 11961.2793,  7209.4917, 10370.9121, 15230.3242,\n",
      "         1252.4070,  5028.1465, 16710.0039,  2198.1899, 38392.7461,  4441.2134,\n",
      "        11922.0859,  9095.0684,  8486.9014, 44397.3906, 10621.7578,  4846.9199,\n",
      "         6255.7744, 17748.5059,  6101.8604,  4032.2407, 14254.6084, 16862.8984,\n",
      "         8569.8613,  7620.8408,  1146.7966, 21880.8203])\n",
      "tensor([10796.3506, 11388.3428, 42124.5156, 11661.3965,  9898.4219, 16420.6777,\n",
      "         8410.0469,  7726.8540, 21223.6758, 10825.2539,  6149.6147,  5748.8198,\n",
      "         7281.5054, 14833.3945, 41501.6562, 11197.0967,  7046.7222,  9365.2812,\n",
      "        19023.2598, 10823.2266, 35907.0117,  1534.3044,  6849.0259, 40861.9336,\n",
      "        42969.8516, 32548.3398,  5452.0752,  8563.3232,  6014.0322, 23033.8848,\n",
      "         9874.5771, 17765.9785, 33907.5469, 18306.3945, 11048.9639,  6046.6641,\n",
      "        12265.5068, 47400.9414,  6551.7500, 12592.5342, 42760.5039, 47412.0312,\n",
      "         6426.7280,  7147.1050, 11276.6777, 14988.4316,  5028.1250,  9755.0039,\n",
      "         6405.0771, 41920.0781,  4158.2539, 10792.8076,  9301.8936,  5729.0054,\n",
      "         9166.4688,  8731.4805,  9432.9258,  5150.3169, 13256.1748,  5972.3779,\n",
      "         5301.3931,  6628.2686,  5811.5186, 35069.3750])\n",
      "tensor([ 1391.5287, 16420.4941,  4721.4297,  1632.0363,  4317.3154,  3375.3062,\n",
      "         9137.7803,  6986.6968,  9490.4531, 11218.9795,  9877.6074, 33475.8164,\n",
      "         5934.3799,  5312.1699,  9304.7021,  9235.0645,  4662.8926, 18972.4941,\n",
      "         9630.3975, 18061.0137, 47403.8789, 11512.4053,  1632.5645,  2264.7219,\n",
      "         4040.4421, 14349.8906, 10763.5752,  4669.0410,  8334.5898,  6746.7427,\n",
      "         2407.0042, 25154.1719, 28403.3711,  2155.6814, 10584.0400,  5241.3555,\n",
      "         6099.9766, 20773.6270,  8026.6665, 30063.5801,  4452.5493,  6948.7007,\n",
      "         9270.5479,  3502.8157, 11268.0127, 11155.0889,  4869.0752,  9535.4424,\n",
      "        19964.7461, 11443.9365, 17929.3027, 46182.8320,  2527.8186,  7729.6455,\n",
      "         9595.4395, 39802.3516,  3947.4131,  4559.5342,  4590.5381,  2020.1770,\n",
      "        41279.2422, 41332.5312, 14643.6221,  5548.1787])\n",
      "tensor([17914.1328,  6712.0068,  5482.2915, 19453.6855,  5920.1040, 23300.7910,\n",
      "         1635.7336, 12479.7090,  6061.5435,  4889.0366, 12646.2070,  4930.2988,\n",
      "         2457.2112, 44501.3984,  8794.9893,  5851.0708,  6770.1924,  4912.5459,\n",
      "         3484.3311,  3875.7341, 28101.3340, 19480.2383,  6891.6792,  9694.4453,\n",
      "         7378.3042, 23632.3750,  5397.6167,  6358.1851,  8891.1396, 20462.9980,\n",
      "        37484.4492, 23261.0215, 10436.0957,  5379.2900, 21774.3223, 16232.8467,\n",
      "         8549.4805, 11674.1299,  7235.6587, 13041.9209, 47273.1094,  6527.7969,\n",
      "         9850.4316, 12649.6045, 11030.4414, 40160.7891, 21982.4746, 11801.3779,\n",
      "         7730.8125, 18765.8750,  6425.8906,  4343.5864,  9553.4902,  6579.8521,\n",
      "         3980.1387, 39722.7461, 17270.3926, 12478.4727, 41999.5195, 12233.8281,\n",
      "        38511.6289, 19305.1934,  5822.7246, 17879.6328])\n",
      "tensor([ 5211.0073, 39871.7031,  9387.7686,  6182.4146,  7731.8579,  8886.0176,\n",
      "         7085.8442,  6186.1270, 14829.3516, 13887.9688, 10950.0391,  5377.0161,\n",
      "         5210.3770, 27107.3418, 13405.3906, 14133.0381,  5515.8096,  6971.5854,\n",
      "        23945.7148,  3213.6221, 11184.4941,  8936.3086, 39000.4609,  2156.7517,\n",
      "         4791.5850,  4559.3188, 11033.6621,  7727.2534, 12129.6143,  1631.8212,\n",
      "         4661.2861,  4012.1570,  5390.6113, 10715.7383,  6765.6182, 29964.4688,\n",
      "         8424.0781, 11015.1748,  4853.6143, 12815.4453,  1629.8335,  3479.0305,\n",
      "         4508.5103,  5620.7578, 12645.1660, 10920.3223,  8653.0039,  9669.7842,\n",
      "        10348.4600,  5003.8530,  2721.3208,  8218.7500, 36910.6094, 37270.1523,\n",
      "         5207.8936,  5966.8872,  5167.4790,  7448.4038, 40103.8906, 18137.9727,\n",
      "         5540.3325, 37829.7227,  7196.8672,  8920.6895])\n",
      "tensor([ 7337.7480, 10991.3584, 17942.1055, 11312.4746, 13224.6934, 12574.0488,\n",
      "         8023.1353,  8609.9229,  5560.0654, 10336.7979, 39983.4258,  9193.8389,\n",
      "        10008.9678, 46412.5352,  8342.9092,  9583.8936, 27941.2871,  6108.3462,\n",
      "        19121.4629,  5014.1821,  8027.9678, 10491.6934, 12146.9707, 27709.7969,\n",
      "         5550.9214,  3615.3975,  9724.5303, 21925.6836,  9283.5615,  2396.0959,\n",
      "         2639.0430,  6673.0005, 11093.9844, 19361.9980,  4260.7441,  6777.4302,\n",
      "         6653.7886,  9610.9561, 21806.1816,  2755.0210, 10381.4785, 10987.3252,\n",
      "        11735.8447, 10602.3848,  8976.1406,  7027.6987])\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(train_data)\n",
    "    model.train()\n",
    "    for batch, i in enumerate(dataloader):\n",
    "        X, y = i.values()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42368995-21c7-48d0-b3d6-689a5e78c644",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[60], line 27\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_relu_stack(x)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/torch/nn/modules/flatten.py:53\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([20,30,0,2,1,0])\n",
    "x = train_dataset[0][\"X\"]\n",
    "x = x.to(device)\n",
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf5f94-9a98-4bbf-bd8d-b05a396441a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
